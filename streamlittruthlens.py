# import streamlit as st
# from textblob import TextBlob

# # Page setup
# st.set_page_config(page_title="Real-Time Sentiment Checker", page_icon="üòä", layout="centered")

# st.title("üìù Real-Time Sentiment Checker")
# st.write("Type below and see how your text feels instantly!")

# # Text input area
# user_input = st.text_area("Type something here:")

# # Only check sentiment if text is entered
# if user_input.strip():
#     blob = TextBlob(user_input)
#     sentiment = blob.sentiment.polarity

#     # Display result live
#     if sentiment > 0:
#         st.success(f"Positive üòä (Score: {sentiment:.2f})")
#     elif sentiment < 0:
#         st.error(f"Negative üò¢ (Score: {sentiment:.2f})")
#     else:
#         st.info(f"Neutral üòê (Score: {sentiment:.2f})")
# else:
#     st.warning("Start typing above to see the sentiment!")











# import streamlit as st
# import requests
# from bs4 import BeautifulSoup
# from PIL import Image
# import numpy as np
# import cv2
# import io

# st.set_page_config(page_title="TruthLens ‚Äî Streamlit Edition", page_icon="üõ°Ô∏è", layout="wide")

# # --- Techy Background CSS ---
# page_bg = """
# <style>
# [data-testid="stAppViewContainer"] {
#     background: linear-gradient(135deg, #0f2027, #203a43, #2c5364);
#     background-attachment: fixed;
#     color: white;
# }
# [data-testid="stHeader"] {
#     background: rgba(0,0,0,0);
# }
# [data-testid="stSidebar"] {
#     background-color: rgba(20, 20, 20, 0.8);
# }
# .stMarkdown, .stText, .stAlert {
#     background-color: rgba(255,255,255,0.05);
#     padding: 10px;
#     border-radius: 8px;
# }
# h1, h2, h3 {
#     color: #00f5ff !important;
#     text-shadow: 0px 0px 10px #00f5ff;
# }
# </style>
# """
# st.markdown(page_bg, unsafe_allow_html=True)

# st.title("üõ°Ô∏è TruthLens ‚Äî Misinformation Scanner (Streamlit)")
# st.write("Analyze webpages and media for manipulative language, AI-generated patterns, or other suspicious traits.")

# # --- Helper Functions ---
# def analyze_url(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         soup = BeautifulSoup(resp.text, "lxml")
#         text = " ".join(p.get_text() for p in soup.find_all("p"))

#         words = text.lower().split()
#         trigger_words = ["shocking", "amazing", "you won't believe", "breaking", "exclusive"]
#         score = sum(1 for w in words if w in trigger_words) / max(len(words), 1) * 100

#         ai_status = "üî¥ URL is AI" if score > 15 else "üü¢ URL is not AI"

#         return {
#             "status": "success",
#             "word_count": len(words),
#             "trigger_count": sum(1 for w in words if w in trigger_words),
#             "risk_score": round(score, 2),
#             "ai_detection": ai_status,
#             "preview": text[:500] + "..." if text else "No text found."
#         }
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_image(file_bytes):
#     try:
#         img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#         np_img = np.array(img)

#         # Edge detection
#         edges = cv2.Canny(np_img, 100, 200)
#         edge_density = np.mean(edges > 0) * 100

#         ai_status = "üî¥ Image is AI" if edge_density > 15 else "üü¢ Image is not AI"

#         return {
#             "status": "success",
#             "width": img.width,
#             "height": img.height,
#             "edge_density": round(edge_density, 2),
#             "ai_detection": ai_status
#         }
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_video(file_bytes):
#     try:
#         with open("temp_video.mp4", "wb") as f:
#             f.write(file_bytes)
#         cap = cv2.VideoCapture("temp_video.mp4")
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         cap.release()
#         return {"status": "success", "frame_count": frame_count}
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# # --- Tabs ---
# tab1, tab2 = st.tabs(["üåê Analyze URL", "üìÅ Analyze Media"])

# with tab1:
#     url = st.text_input("Enter webpage URL")
#     if st.button("Scan URL"):
#         if url:
#             with st.spinner("Analyzing..."):
#                 st.write(analyze_url(url))
#         else:
#             st.warning("Please enter a URL.")

# with tab2:
#     uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])
#     if uploaded_file:
#         file_bytes = uploaded_file.read()
#         if uploaded_file.type.startswith("image"):
#             result = analyze_image(file_bytes)
#             st.image(file_bytes, caption="Uploaded Image", use_column_width=True)
#             st.write(result)
#         elif uploaded_file.type == "video/mp4":
#             st.video(file_bytes)
#             st.write(analyze_video(file_bytes))










# import streamlit as st
# import requests
# from bs4 import BeautifulSoup
# from PIL import Image
# import numpy as np
# import cv2
# import io
# import os

# st.set_page_config(page_title="TruthLens ‚Äî Streamlit Edition", page_icon="üõ°Ô∏è", layout="wide")

# # --- Nebula Background CSS ---
# page_bg = """
# <style>
# [data-testid="stAppViewContainer"] {
#     background: url('https://images.unsplash.com/photo-1504384308090-c894fdcc538d?auto=format&fit=crop&w=1470&q=80');
#     background-size: cover;
#     background-attachment: fixed;
#     color: white;
# }
# [data-testid="stHeader"] {
#     background: rgba(0,0,0,0);
# }
# [data-testid="stSidebar"] {
#     background-color: rgba(0, 0, 0, 0.7);
# }
# .stMarkdown, .stText, .stAlert {
#     background-color: rgba(0,0,0,0.5);
#     padding: 12px;
#     border-radius: 10px;
# }
# h1, h2, h3 {
#     color: #00f5ff !important;
#     text-shadow: 0px 0px 12px #00f5ff;
# }
# </style>
# """
# st.markdown(page_bg, unsafe_allow_html=True)

# st.title("üõ°Ô∏è TruthLens ‚Äî Misinformation Scanner")
# st.write("Scan webpages, images, or videos to detect AI-generated content or manipulative patterns with confidence.")

# # --- Helper Functions ---
# def analyze_url(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         try:
#             soup = BeautifulSoup(resp.text, "lxml")
#         except:
#             soup = BeautifulSoup(resp.text, "html.parser")
#         text = " ".join(p.get_text() for p in soup.find_all("p"))

#         words = text.lower().split()
#         trigger_words = ["shocking", "amazing", "you won't believe", "breaking", "exclusive"]
#         score = sum(1 for w in words if w in trigger_words) / max(len(words), 1) * 100

#         if score > 15:
#             ai_status = "üî¥ This content **may be AI-generated** or highly sensational."
#         else:
#             ai_status = "üü¢ This content **appears human-written and credible**."

#         return {
#             "status": "success",
#             "word_count": len(words),
#             "trigger_count": sum(1 for w in words if w in trigger_words),
#             "risk_score": round(score, 2),
#             "ai_detection": ai_status,
#             "preview": text[:500] + "..." if text else "No text found."
#         }
#     except requests.exceptions.RequestException as e:
#         return {"status": "error", "error": f"Request failed: {e}"}
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_image(file_bytes):
#     try:
#         img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#         np_img = np.array(img)
#         edges = cv2.Canny(np_img, 100, 200)
#         edge_density = np.mean(edges > 0) * 100

#         if edge_density > 15:
#             ai_status = "üî¥ This image **likely contains AI-generated elements**."
#         else:
#             ai_status = "üü¢ This image **appears human-made and natural**."

#         return {
#             "status": "success",
#             "width": img.width,
#             "height": img.height,
#             "edge_density": round(edge_density, 2),
#             "ai_detection": ai_status
#         }
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_video(file_bytes):
#     temp_file = "temp_video.mp4"
#     try:
#         with open(temp_file, "wb") as f:
#             f.write(file_bytes)
#         cap = cv2.VideoCapture(temp_file)
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#         height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#         cap.release()
#         os.remove(temp_file)
#         return {
#             "status": "success",
#             "frame_count": frame_count,
#             "width": width,
#             "height": height,
#             "ai_detection": "üîµ Video appears **natural**."
#         }
#     except Exception as e:
#         if os.path.exists(temp_file):
#             os.remove(temp_file)
#         return {"status": "error", "error": str(e)}

# # --- Tabs ---
# tab1, tab2 = st.tabs(["üåê Analyze URL", "üìÅ Analyze Media"])

# with tab1:
#     url = st.text_input("Enter webpage URL")
#     if st.button("Scan URL"):
#         if url:
#             with st.spinner("Analyzing webpage..."):
#                 result = analyze_url(url)
#                 if result["status"] == "success":
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Word Count:** {result['word_count']}, **Trigger Words:** {result['trigger_count']}")
#                     st.markdown(f"**Preview:** {result['preview']}")
#                 else:
#                     st.error(result["error"])
#         else:
#             st.warning("Please enter a URL.")

# with tab2:
#     uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])
#     if uploaded_file:
#         file_bytes = uploaded_file.read()
#         if uploaded_file.type.startswith("image"):
#             result = analyze_image(file_bytes)
#             st.image(file_bytes, caption="Uploaded Image", use_column_width=True)
#             if result["status"] == "success":
#                 st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                 st.markdown(f"**Width:** {result['width']} px, **Height:** {result['height']} px")
#                 st.markdown(f"**Edge Density:** {result['edge_density']}")
#             else:
#                 st.error(result["error"])
#         elif uploaded_file.type == "video/mp4":
#             st.video(file_bytes)
#             result = analyze_video(file_bytes)
#             if result["status"] == "success":
#                 st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                 st.markdown(f"**Frames:** {result['frame_count']}, **Resolution:** {result['width']}x{result['height']}")
#             else:
#                 st.error(result["error"])








# import streamlit as st
# import requests
# from bs4 import BeautifulSoup
# from PIL import Image
# import numpy as np
# import cv2
# import io
# import os

# st.set_page_config(page_title="TruthLens ‚Äî Streamlit Edition", page_icon="üõ°Ô∏è", layout="wide")

# # --- High-Tech Space Nebula Background & UI ---
# page_bg = """
# <style>
# body {
#     background: url('https://images.unsplash.com/photo-1580674284434-cf1054c19b2b?auto=format&fit=crop&w=1470&q=80') no-repeat center center fixed;
#     background-size: cover;
#     color: white;
#     font-family: 'Courier New', monospace;
# }
# [data-testid="stSidebar"] {
#     background-color: rgba(0, 0, 0, 0.7);
#     border-right: 2px solid #00f5ff;
# }
# h1, h2, h3 {
#     color: #00f5ff !important;
#     text-shadow: 0 0 5px #00f5ff, 0 0 10px #00f5ff, 0 0 20px #00f5ff;
# }
# .stMarkdown, .stText, .stAlert {
#     background: rgba(0,0,0,0.7);
#     border-left: 4px solid #00f5ff;
#     padding: 15px;
#     margin-bottom: 15px;
#     border-radius: 12px;
#     box-shadow: 0 0 15px #00f5ff;
# }
# .stButton>button {
#     background-color: #00f5ff;
#     color: black;
#     font-weight: bold;
#     border-radius: 10px;
#     padding: 10px 20px;
# }
# .stButton>button:hover {
#     background-color: #00d4ff;
# }
# </style>
# """
# st.markdown(page_bg, unsafe_allow_html=True)

# st.title("üõ°Ô∏è TruthLens ‚Äî Misinformation Scanner")
# st.write("Scan webpages, images, or videos to detect AI-generated content or manipulative patterns with confidence.")

# # --- Helper Functions ---
# def analyze_url(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         try:
#             soup = BeautifulSoup(resp.text, "lxml")
#         except:
#             soup = BeautifulSoup(resp.text, "html.parser")
#         text = " ".join(p.get_text() for p in soup.find_all("p"))

#         words = text.lower().split()
#         trigger_words = ["shocking", "amazing", "you won't believe", "breaking", "exclusive"]
#         score = sum(1 for w in words if w in trigger_words) / max(len(words), 1) * 100

#         if score > 15:
#             ai_status = "üî¥ This content **may be AI-generated** or highly sensational."
#         else:
#             ai_status = "üü¢ This content **appears human-written and credible**."

#         return {
#             "status": "success",
#             "word_count": len(words),
#             "trigger_count": sum(1 for w in words if w in trigger_words),
#             "risk_score": round(score, 2),
#             "ai_detection": ai_status,
#             "preview": text[:500] + "..." if text else "No text found."
#         }
#     except requests.exceptions.RequestException as e:
#         return {"status": "error", "error": f"Request failed: {e}"}
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_image(file_bytes):
#     try:
#         img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#         np_img = np.array(img)
#         edges = cv2.Canny(np_img, 100, 200)
#         edge_density = np.mean(edges > 0) * 100

#         if edge_density > 15:
#             ai_status = "üî¥ This image **likely contains AI-generated elements**."
#         else:
#             ai_status = "üü¢ This image **appears human-made and natural**."

#         return {
#             "status": "success",
#             "width": img.width,
#             "height": img.height,
#             "edge_density": round(edge_density, 2),
#             "ai_detection": ai_status
#         }
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_video(file_bytes):
#     temp_file = "temp_video.mp4"
#     try:
#         with open(temp_file, "wb") as f:
#             f.write(file_bytes)
#         cap = cv2.VideoCapture(temp_file)
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#         height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#         cap.release()
#         os.remove(temp_file)
#         return {
#             "status": "success",
#             "frame_count": frame_count,
#             "width": width,
#             "height": height,
#             "ai_detection": "üîµ Video appears **natural**."
#         }
#     except Exception as e:
#         if os.path.exists(temp_file):
#             os.remove(temp_file)
#         return {"status": "error", "error": str(e)}

# # --- Tabs ---
# tab1, tab2 = st.tabs(["üåê Analyze URL", "üìÅ Analyze Media"])

# with tab1:
#     url = st.text_input("Enter webpage URL")
#     if st.button("Scan URL"):
#         if url:
#             with st.spinner("Analyzing webpage..."):
#                 result = analyze_url(url)
#                 if result["status"] == "success":
#                     col1, col2 = st.columns([2,3])
#                     with col1:
#                         st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                         st.markdown(f"**Word Count:** {result['word_count']}")
#                         st.markdown(f"**Trigger Words Found:** {result['trigger_count']}")
#                     with col2:
#                         st.markdown(f"**Preview:** {result['preview']}")
#                 else:
#                     st.error(result["error"])
#         else:
#             st.warning("Please enter a URL.")

# with tab2:
#     uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])
#     if uploaded_file:
#         file_bytes = uploaded_file.read()
#         if uploaded_file.type.startswith("image"):
#             result = analyze_image(file_bytes)
#             st.image(file_bytes, caption="Uploaded Image", use_column_width=True)
#             if result["status"] == "success":
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Width:** {result['width']} px")
#                     st.markdown(f"**Height:** {result['height']} px")
#                 with col2:
#                     st.markdown(f"**Edge Density:** {result['edge_density']}")
#             else:
#                 st.error(result["error"])
#         elif uploaded_file.type == "video/mp4":
#             st.video(file_bytes)
#             result = analyze_video(file_bytes)
#             if result["status"] == "success":
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Frames:** {result['frame_count']}")
#                 with col2:
#                     st.markdown(f"**Resolution:** {result['width']}x{result['height']}")
#             else:
#                 st.error(result["error"])




































# import streamlit as st
# import requests
# from bs4 import BeautifulSoup
# from PIL import Image
# import numpy as np
# import cv2
# import io
# import os

# st.set_page_config(page_title="TruthLens ‚Äî Streamlit Edition", page_icon="üõ°Ô∏è", layout="wide")

# # --- Full-page Star Nebula Background with Neon Theme ---
# st.markdown(
#     """
#     <style>
#     /* Full-page fixed background */
#     .stApp::before {
#         content: "";
#         position: fixed;
#         top: 0;
#         left: 0;
#         width: 100%;
#         height: 100%;
#         background-image: url('https://images.unsplash.com/photo-1580674284434-cf1054c19b2b?auto=format&fit=crop&w=1470&q=80');
#         background-size: cover;
#         background-position: center;
#         opacity: 1;
#         z-index: -1;
#     }
#     /* Sidebar styling */
#     [data-testid="stSidebar"] {
#         background-color: rgba(0,0,0,0.7);
#         border-right: 2px solid #00f5ff;
#     }
#     /* Card-style results */
#     .stMarkdown, .stText, .stAlert {
#         background: rgba(0,0,0,0.7);
#         border-left: 4px solid #00f5ff;
#         padding: 15px;
#         margin-bottom: 15px;
#         border-radius: 12px;
#         box-shadow: 0 0 15px #00f5ff;
#     }
#     /* Neon headers */
#     h1, h2, h3 {
#         color: #00f5ff !important;
#         text-shadow: 0 0 5px #00f5ff, 0 0 10px #00f5ff, 0 0 20px #00f5ff;
#     }
#     /* Neon buttons */
#     .stButton>button {
#         background-color: #00f5ff;
#         color: black;
#         font-weight: bold;
#         border-radius: 10px;
#         padding: 10px 20px;
#     }
#     .stButton>button:hover {
#         background-color: #00d4ff;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )

# st.title("üõ°Ô∏è TruthLens ‚Äî Misinformation Scanner")
# st.write("Scan webpages, images, or videos to detect AI-generated content or manipulative patterns with confidence.")

# # --- Helper Functions ---
# def analyze_url(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         try:
#             soup = BeautifulSoup(resp.text, "lxml")
#         except:
#             soup = BeautifulSoup(resp.text, "html.parser")
#         text = " ".join(p.get_text() for p in soup.find_all("p"))

#         words = text.lower().split()
#         trigger_words = ["shocking", "amazing", "you won't believe", "breaking", "exclusive"]
#         score = sum(1 for w in words if w in trigger_words) / max(len(words), 1) * 100

#         if score > 15:
#             ai_status = "üî¥ This content **may be AI-generated** or highly sensational."
#         else:
#             ai_status = "üü¢ This content **appears human-written and credible**."

#         return {
#             "status": "success",
#             "word_count": len(words),
#             "trigger_count": sum(1 for w in words if w in trigger_words),
#             "risk_score": round(score, 2),
#             "ai_detection": ai_status,
#             "preview": text[:500] + "..." if text else "No text found."
#         }
#     except requests.exceptions.RequestException as e:
#         return {"status": "error", "error": f"Request failed: {e}"}
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_image(file_bytes):
#     try:
#         img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#         np_img = np.array(img)
#         edges = cv2.Canny(np_img, 100, 200)
#         edge_density = np.mean(edges > 0) * 100

#         if edge_density > 15:
#             ai_status = "üî¥ This image **likely contains AI-generated elements**."
#         else:
#             ai_status = "üü¢ This image **appears human-made and natural**."

#         return {
#             "status": "success",
#             "width": img.width,
#             "height": img.height,
#             "edge_density": round(edge_density, 2),
#             "ai_detection": ai_status
#         }
#     except Exception as e:
#         return {"status": "error", "error": str(e)}

# def analyze_video(file_bytes):
#     temp_file = "temp_video.mp4"
#     try:
#         with open(temp_file, "wb") as f:
#             f.write(file_bytes)
#         cap = cv2.VideoCapture(temp_file)
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#         height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#         cap.release()
#         os.remove(temp_file)
#         return {
#             "status": "success",
#             "frame_count": frame_count,
#             "width": width,
#             "height": height,
#             "ai_detection": "üîµ Video appears **natural**."
#         }
#     except Exception as e:
#         if os.path.exists(temp_file):
#             os.remove(temp_file)
#         return {"status": "error", "error": str(e)}

# # --- Tabs ---
# tab1, tab2 = st.tabs(["üåê Analyze URL", "üìÅ Analyze Media"])

# with tab1:
#     url = st.text_input("Enter webpage URL")
#     if st.button("Scan URL"):
#         if url:
#             with st.spinner("Analyzing webpage..."):
#                 result = analyze_url(url)
#                 if result["status"] == "success":
#                     col1, col2 = st.columns([2,3])
#                     with col1:
#                         st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                         st.markdown(f"**Word Count:** {result['word_count']}")
#                         st.markdown(f"**Trigger Words Found:** {result['trigger_count']}")
#                     with col2:
#                         st.markdown(f"**Preview:** {result['preview']}")
#                 else:
#                     st.error(result["error"])
#         else:
#             st.warning("Please enter a URL.")

# with tab2:
#     uploaded_file = st.file_uploader("Upload an image or video", type=["jpg", "jpeg", "png", "mp4"])
#     if uploaded_file:
#         file_bytes = uploaded_file.read()
#         if uploaded_file.type.startswith("image"):
#             result = analyze_image(file_bytes)
#             st.image(file_bytes, caption="Uploaded Image", use_column_width=True)
#             if result["status"] == "success":
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Width:** {result['width']} px")
#                     st.markdown(f"**Height:** {result['height']} px")
#                 with col2:
#                     st.markdown(f"**Edge Density:** {result['edge_density']}")
#             else:
#                 st.error(result["error"])
#         elif uploaded_file.type == "video/mp4":
#             st.video(file_bytes)
#             result = analyze_video(file_bytes)
#             if result["status"] == "success":
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Frames:** {result['frame_count']}")
#                 with col2:
#                     st.markdown(f"**Resolution:** {result['width']}x{result['height']}")
#             else:
#                 st.error(result["error"])







































                















# import streamlit as st
# import requests, socket, io, os, tempfile
# from bs4 import BeautifulSoup
# from PIL import Image
# import numpy as np
# import cv2
# import threading

# # --- Check Internet ---
# def has_internet(host="8.8.8.8", port=53, timeout=3):
#     try:
#         socket.setdefaulttimeout(timeout)
#         socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))
#         return True
#     except:
#         return False

# ONLINE = has_internet()

# # --- Async Hugging Face Detector ---
# detector = None
# if ONLINE:
#     def load_hf_model():
#         global detector
#         from transformers import pipeline
#         detector = pipeline("image-classification", model="f-schmidt/ai-or-human")
#     threading.Thread(target=load_hf_model, daemon=True).start()

# # --- Streamlit Config & UI ---
# st.set_page_config(page_title="TruthLens", page_icon="üõ°Ô∏è", layout="wide")
# st.markdown("""
# <style>
# .stApp::before {content: "";position: fixed;top:0;left:0;width:100%;height:100%;
# background-image: url('https://images.unsplash.com/photo-1580674284434-cf1054c19b2b?auto=format&fit=crop&w=1470&q=80');
# background-size: cover;background-position:center;opacity:1;z-index:-1;}
# [data-testid="stSidebar"] {background-color: rgba(0,0,0,0.7); border-right:2px solid #00f5ff;}
# .stMarkdown, .stText, .stAlert {background:rgba(0,0,0,0.7); border-left:4px solid #00f5ff; padding:15px;
# margin-bottom:15px; border-radius:12px; box-shadow:0 0 15px #00f5ff;}
# h1,h2,h3{color:#00f5ff!important; text-shadow:0 0 5px #00f5ff,0 0 10px #00f5ff,0 0 20px #00f5ff;}
# .stButton>button{background-color:#00f5ff;color:black;font-weight:bold;border-radius:10px;padding:10px 20px;}
# .stButton>button:hover{background-color:#00d4ff;}
# </style>
# """, unsafe_allow_html=True)

# st.title("üõ°Ô∏è TruthLens ‚Äî Misinformation Scanner")
# st.write("Scan webpages, images, or videos to detect AI-generated content or manipulative patterns with confidence.")

# # --- URL/Text Analysis ---
# def analyze_url(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         soup = BeautifulSoup(resp.text, "lxml")
#         text = " ".join(p.get_text() for p in soup.find_all("p"))
#         words = text.lower().split()
#         triggers = ["shocking","amazing","you won't believe","breaking","exclusive"]
#         score = sum(1 for w in words if w in triggers)/max(len(words),1)*100
#         ai_status = "üî¥ May be AI/sensational" if score>15 else "üü¢ Appears human-written"
#         return {"status":"success","word_count":len(words),"trigger_count":sum(1 for w in words if w in triggers),
#                 "risk_score":round(score,2),"ai_detection":ai_status,"preview":text[:500]+"..." if text else "No text found."}
#     except Exception as e:
#         return {"status":"error","error":str(e)}

# # --- Offline Image Detector ---
# def analyze_image_offline(file_bytes):
#     img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#     np_img = np.array(img)
#     width, height = img.width, img.height

#     edges = cv2.Canny(np_img, 100, 200)
#     edge_density = np.mean(edges > 0)
#     color_std = np.std(np_img / 255.0)
#     gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)
#     lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()

#     confidence = 0.4 * edge_density + 0.3 * (1 - color_std) + 0.3 * (1 - min(lap_var / 1000,1))

#     if confidence < 0.10:
#         ai_status = "üü¢ Likely Human-made"
#     elif confidence < 0.25:
#         ai_status = "üü° Possibly AI-generated"
#     else:
#         ai_status = "üî¥ Likely AI-generated"

#     return {"status":"success","ai_detection":ai_status,"confidence_score":round(confidence*100,2),
#             "width": width,"height": height}

# # --- Hybrid Image Detector ---
# def analyze_image(file_bytes):
#     if ONLINE and detector:
#         try:
#             img = Image.open(io.BytesIO(file_bytes)).convert("RGB")
#             result = detector(img)[0]
#             score = result['score']*100
#             if score < 30:
#                 ai_status = "üü¢ Likely Human-made"
#             elif score < 70:
#                 ai_status = "üü° Possibly AI-generated"
#             else:
#                 ai_status = "üî¥ Likely AI-generated"
#             return {"status":"success","ai_detection":ai_status,"confidence_score":round(score,2),
#                     "width": img.width,"height": img.height}
#         except:
#             return analyze_image_offline(file_bytes)
#     else:
#         return analyze_image_offline(file_bytes)

# # --- Video Detector ---
# def analyze_video(file_bytes):
#     with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
#         tmp.write(file_bytes)
#         temp_file = tmp.name

#     cap = cv2.VideoCapture(temp_file)
#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

#     sample_frames = 5
#     interval = max(1, frame_count//sample_frames)
#     confidence_scores = []

#     for i in range(0, frame_count, interval):
#         cap.set(cv2.CAP_PROP_POS_FRAMES, i)
#         ret, frame = cap.read()
#         if not ret: continue
#         frame_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#         buf = io.BytesIO()
#         frame_img.save(buf, format="PNG")
#         result = analyze_image(buf.getvalue())
#         confidence_scores.append(result["confidence_score"])

#     cap.release()
#     os.remove(temp_file)
#     avg_conf = np.mean(confidence_scores) if confidence_scores else 0

#     if avg_conf < 30:
#         ai_status = "üü¢ Likely Human-made"
#     elif avg_conf < 50:
#         ai_status = "üü° Possibly AI-generated"
#     else:
#         ai_status = "üî¥ Likely AI-generated"

#     return {"status":"success","frame_count":frame_count,"width":width,"height":height,
#             "ai_detection":f"{ai_status} ({round(avg_conf,2)}%)"}

# # --- Tabs ---
# tab1, tab2 = st.tabs(["üåê Analyze URL","üìÅ Analyze Media"])

# with tab1:
#     url = st.text_input("Enter webpage URL")
#     if st.button("Scan URL"):
#         if url:
#             with st.spinner("Analyzing webpage..."):
#                 result = analyze_url(url)
#                 if result["status"]=="success":
#                     col1,col2 = st.columns([2,3])
#                     with col1:
#                         st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                         st.markdown(f"**Word Count:** {result['word_count']}")
#                         st.markdown(f"**Trigger Words:** {result['trigger_count']}")
#                     with col2:
#                         st.markdown(f"**Preview:** {result['preview']}")
#                 else:
#                     st.error(result["error"])
#         else:
#             st.warning("Please enter a URL.")

# with tab2:
#     uploaded_file = st.file_uploader("Upload an image or video", type=["jpg","jpeg","png","mp4"])
#     if uploaded_file:
#         file_bytes = uploaded_file.read()
#         if uploaded_file.type.startswith("image"):
#             result = analyze_image(file_bytes)
#             st.image(file_bytes, caption="Uploaded Image", use_column_width=True)
#             if result["status"]=="success":
#                 st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                 st.markdown(f"**Confidence Score:** {result['confidence_score']}%")
#                 st.markdown(f"**Width:** {result['width']} px")
#                 st.markdown(f"**Height:** {result['height']} px")
#             else:
#                 st.error(result["error"])
#         elif uploaded_file.type=="video/mp4":
#             st.video(file_bytes)
#             result = analyze_video(file_bytes)
#             if result["status"]=="success":
#                 col1,col2 = st.columns(2)
#                 with col1:
#                     st.markdown(f"**Detection Result:** {result['ai_detection']}")
#                     st.markdown(f"**Frames:** {result['frame_count']}")
#                 with col2:
#                     st.markdown(f"**Resolution:** {result['width']}x{result['height']}")
#             else:
#                 st.error(result["error"])












# import os
# import io
# import tempfile
# from urllib.parse import urlparse

# import streamlit as st
# import requests
# import numpy as np
# from PIL import Image, ExifTags, UnidentifiedImageError

# import torch
# from transformers import CLIPProcessor, CLIPModel, AutoImageProcessor, AutoModelForImageClassification
# import cv2
# import timm
# from torchvision import transforms

# # Optional: yt-dlp for multi-platform video URLs
# try:
#     import yt_dlp
# except ImportError:
#     yt_dlp = None

# # -------------------------
# # App config / UI
# # -------------------------
# st.set_page_config(page_title="Truthlens-AI Detector", layout="wide", page_icon="üîé")
# st.markdown("""
# <style>
# html, body, [data-testid="stAppViewContainer"] {
#   background: radial-gradient(circle at 10% 10%, #001021, #000000) !important;
# }
# h1, h2, h3 { color:#00f9ff !important; text-shadow: 0 0 8px #00f9ff; }
# .neon-card { border-radius:12px; padding:12px; background: rgba(6,10,20,0.6); box-shadow:0 8px 30px rgba(0,120,255,0.06); color:#dffaff; }
# </style>
# """, unsafe_allow_html=True)

# st.title("üîé Truthlens-AI Detector")
# st.markdown('<div class="neon-card">Paste URL or upload image/video. Output: AI-generated or Human-made plus resolution.</div>', unsafe_allow_html=True)

# # -------------------------
# # Device & model config
# # -------------------------
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CLIP_REPO = os.environ.get("CLIP_REPO", "openai/clip-vit-base-patch32")
# FRAME_CKPT = os.environ.get("FRAME_CKPT", "")
# VIT_DIR = os.environ.get("VIT_DIR", "")
# DEEPFAKE_MODEL_ID = os.environ.get("DEEPFAKE_MODEL_ID", "")

# AI_PROMPTS = ["an AI-generated image, synthetic, digital rendering", "a computer-generated picture created by an AI model"]
# HUMAN_PROMPTS = ["a real photograph taken by a camera", "an authentic, real-world image captured by a person"]

# # -------------------------
# # Helpers
# # -------------------------
# def exif_has_camera(img_or_path) -> bool:
#     try:
#         if isinstance(img_or_path, (bytes, bytearray)):
#             img = Image.open(io.BytesIO(img_or_path))
#         elif isinstance(img_or_path, str) and os.path.exists(img_or_path):
#             img = Image.open(img_or_path)
#         elif isinstance(img_or_path, Image.Image):
#             img = img_or_path
#         else:
#             return False
#         exif = getattr(img, "_getexif", lambda: None)()
#         if not exif:
#             return False
#         for tag_id, value in exif.items():
#             tag = ExifTags.TAGS.get(tag_id, tag_id)
#             if tag in ("Make", "Model", "LensModel", "CreatorTool") and value:
#                 return True
#     except Exception:
#         return False
#     return False

# # -------------------------
# # Load models (cached)
# # -------------------------
# @st.cache_resource(show_spinner=True)
# def load_clip():
#     proc = CLIPProcessor.from_pretrained(CLIP_REPO)
#     model = CLIPModel.from_pretrained(CLIP_REPO).to(DEVICE).eval()
#     texts = AI_PROMPTS + HUMAN_PROMPTS
#     inputs = proc(text=texts, return_tensors="pt", padding=True).to(DEVICE)
#     with torch.no_grad():
#         text_feats = model.get_text_features(**inputs)
#     text_feats = text_feats / text_feats.norm(p=2, dim=-1, keepdim=True)
#     return proc, model, text_feats, len(AI_PROMPTS), len(HUMAN_PROMPTS)

# clip_proc, clip_model, TEXT_FEATS, N_AI, N_HUM = load_clip()

# # -------------------------
# # Download multi-platform video using yt-dlp
# # -------------------------
# def download_media(url):
#     if yt_dlp is None:
#         raise RuntimeError("yt-dlp is required for multi-platform URLs.")
#     tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
#     ydl_opts = {
#         'outtmpl': tmp_file.name,
#         'format': 'best[ext=mp4]/best',
#         'quiet': True,
#         'noplaylist': True
#     }
#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:
#         info_dict = ydl.extract_info(url, download=True)
#     return tmp_file.name

# # -------------------------
# # CLIP image vote
# # -------------------------
# def clip_vote_image(pil_img: Image.Image) -> str:
#     inputs = clip_proc(images=pil_img, return_tensors="pt").to(DEVICE)
#     with torch.no_grad():
#         img_feats = clip_model.get_image_features(**inputs)
#     img_feats = img_feats / img_feats.norm(p=2, dim=-1, keepdim=True)
#     logits = img_feats @ TEXT_FEATS.T
#     logits = logits.squeeze(0).cpu()
#     ai_score = logits[:N_AI].mean().item()
#     hm_score = logits[N_AI:].mean().item()
#     return "AI-generated" if ai_score >= hm_score else "Human-made"

# # -------------------------
# # Video frame sampling
# # -------------------------
# def sample_frames_from_video_opencv(path, n_frames=12):
#     cap = cv2.VideoCapture(path)
#     if not cap.isOpened():
#         return []
#     total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#     indices = np.linspace(0, max(0,total-1), num=min(n_frames,total)).astype(int)
#     frames = []
#     for idx in indices:
#         cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))
#         ok, frame = cap.read()
#         if not ok:
#             continue
#         frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
#     cap.release()
#     return frames

# # -------------------------
# # Ensemble decision for image
# # -------------------------
# def ensemble_decision_for_image(pil_img: Image.Image) -> str:
#     return clip_vote_image(pil_img)

# # -------------------------
# # Ensemble decision for video
# # -------------------------
# def ensemble_decision_for_video(path):
#     frames = sample_frames_from_video_opencv(path)
#     if not frames:
#         return None
#     votes = [ensemble_decision_for_image(f) for f in frames]
#     ai_count = votes.count("AI-generated")
#     hm_count = votes.count("Human-made")
#     return "AI-generated" if ai_count >= hm_count else "Human-made"

# # -------------------------
# # Get media dimensions
# # -------------------------
# def get_image_dims(path):
#     try:
#         with Image.open(path) as img:
#             return img.size
#     except Exception:
#         return None, None

# def get_video_dims(path):
#     try:
#         cap = cv2.VideoCapture(path)
#         w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
#         h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
#         cap.release()
#         return w, h
#     except Exception:
#         return None, None

# # -------------------------
# # UI: URL first
# # -------------------------
# tab_url, tab_upload = st.tabs(["üåê URL", "üìÅ Upload"])

# with tab_url:
#     st.header("Paste a direct video/image URL")
#     url = st.text_input("Enter URL")
#     if st.button("Analyze URL"):
#         if not url:
#             st.error("Please enter a URL")
#         else:
#             tmp_path = None
#             try:
#                 # Download video/image if platform URL
#                 if any(x in url for x in ["tiktok.com", "youtube.com", "youtu.be", "instagram.com"]):
#                     tmp_path = download_media(url)
#                 else:
#                     # Otherwise download raw file
#                     r = requests.get(url, stream=True, timeout=240)
#                     r.raise_for_status()
#                     suffix = os.path.splitext(urlparse(url).path)[1] or ""
#                     tmpf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
#                     for chunk in r.iter_content(chunk_size=1024*1024):
#                         if chunk:
#                             tmpf.write(chunk)
#                     tmpf.flush(); tmpf.close()
#                     tmp_path = tmpf.name

#                 # Try image
#                 try:
#                     pil = Image.open(tmp_path).convert("RGB")
#                     w, h = pil.size
#                     decision = ensemble_decision_for_image(pil)
#                     st.image(pil, caption=f"Image ‚Äî {w}√ó{h}px", use_column_width=True)
#                     st.success(f"Origin: {decision}")
#                     st.write(f"Width: {w}px ‚Äî Height: {h}px ‚Äî Resolution: {w}√ó{h}")
#                 except UnidentifiedImageError:
#                     # Treat as video
#                     st.video(tmp_path)
#                     decision = ensemble_decision_for_video(tmp_path)
#                     w, h = get_video_dims(tmp_path)
#                     if decision is None:
#                         st.error("Could not read video frames for analysis.")
#                     else:
#                         st.success(f"Origin: {decision}")
#                         if w and h:
#                             st.write(f"Width: {w}px ‚Äî Height: {h}px ‚Äî Resolution: {w}√ó{h}")
#             except Exception as e:
#                 st.error(f"Failed to analyze URL: {e}")
#             finally:
#                 try:
#                     if tmp_path and os.path.exists(tmp_path):
#                         os.remove(tmp_path)
#                 except Exception:
#                     pass

# with tab_upload:
#     st.header("Upload an image or video")
#     uploaded = st.file_uploader("Drop file here", type=None)
#     if uploaded:
#         suffix = os.path.splitext(uploaded.name)[1] or ""
#         tmpf = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
#         tmpf.write(uploaded.read()); tmpf.flush(); tmpf.close()
#         path = tmpf.name
#         try:
#             pil = Image.open(path).convert("RGB")
#             w, h = pil.size
#             decision = ensemble_decision_for_image(pil)
#             st.image(pil, caption=f"Uploaded Image ‚Äî {w}√ó{h}px", use_column_width=True)
#             st.success(f"Origin: {decision}")
#             st.write(f"Width: {w}px ‚Äî Height: {h}px ‚Äî Resolution: {w}√ó{h}")
#         except UnidentifiedImageError:
#             st.video(path)
#             decision = ensemble_decision_for_video(path)
#             w, h = get_video_dims(path)
#             if decision is None:
#                 st.error("Could not read video frames for analysis.")
#             else:
#                 st.success(f"Origin: {decision}")
#                 if w and h:
#                     st.write(f"Width: {w}px ‚Äî Height: {h}px ‚Äî Resolution: {w}√ó{h}")

# st.markdown("---")
# st.markdown('<div class="neon-card">Note: Ensemble maximizes practical detection accuracy. Combine with human review for critical decisions.</div>', unsafe_allow_html=True)

















# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# import hashlib
# import os
# import time
# from dataclasses import dataclass
# from typing import Optional, Tuple

# import streamlit as st


# # -----------------------------
# # Page setup
# # -----------------------------

# st.set_page_config(
#     page_title="Truthlens ‚Äî Misinformation Scanner",
#     page_icon="üîé",
#     layout="wide",
# )

# # Custom CSS to approximate the neon dark theme
# st.markdown(
#     """
#     <style>
#     :root {
#       --bg-900: #0B0F14;
#       --bg-800: #0F1622;
#       --text-100: #E6F0FF;
#       --text-300: #9BB3C9;
#       --line-700: #1B2A3B;
#       --blue: #00D4FF;
#       --fuchsia: #FF2BD1;
#       --green: #39FF88;
#       --red: #FF5C5C;
#       --radius-lg: 16px;
#       --radius-md: 12px;
#       --radius-sm: 8px;
#       --shadow-glow-blue: 0 0 24px rgba(0, 212, 255, 0.35);
#       --shadow-glow-green: 0 0 24px rgba(57, 255, 136, 0.35);
#       --shadow-glow-red: 0 0 24px rgba(255, 92, 92, 0.35);
#     }

#     .truthlens-panel {
#       background: var(--bg-800);
#       border: 1px solid var(--line-700);
#       border-radius: var(--radius-lg);
#       padding: 24px;
#       color: var(--text-100);
#     }
#     .truthlens-title {
#       font-weight: 700;
#       font-size: 22px;
#       margin-bottom: 12px;
#     }
#     .hint { color: var(--text-300); }
#     .tab-underline {
#       height: 3px;
#       border-radius: 2px;
#       background: linear-gradient(90deg, var(--blue), var(--fuchsia));
#       box-shadow: var(--shadow-glow-blue);
#       margin-top: 8px;
#       margin-bottom: 2px;
#     }
#     .dropzone {
#       border: 2px dashed var(--line-700);
#       border-radius: var(--radius-lg);
#       padding: 28px;
#       text-align: center;
#       color: var(--text-300);
#       background: var(--bg-900);
#     }
#     .verdict-good { color: var(--green); font-weight: 800; font-size: 22px; }
#     .verdict-bad { color: var(--red); font-weight: 800; font-size: 22px; }
#     .card { background: var(--bg-900); border: 1px solid var(--line-700); border-radius: 12px; padding: 12px; }
#     .card-label { color: var(--text-300); font-size: 12px; letter-spacing: 0.04em; }
#     .card-value { color: var(--text-100); font-size: 16px; font-weight: 600; }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )

# # -----------------------------
# # Utilities
# # -----------------------------

# @dataclass
# class AnalysisResult:
#     is_ai_generated: bool
#     confidence_pct: int
#     resolution: str
#     aspect_ratio: str
#     file_type: str
#     video_length: Optional[str] = None


# def deterministic_score(seed: str) -> float:
#     digest = hashlib.sha256(seed.encode("utf-8")).hexdigest()
#     value = int(digest[:8], 16) / 0xFFFFFFFF
#     return value


# def guess_resolution_and_ratio(source: str) -> Tuple[str, str]:
#     lower = source.lower()
#     if any(k in lower for k in ["tiktok", "shorts", "reels"]):
#         return "1080 √ó 1920", "9:16"
#     if any(k in lower for k in ["instagram", "stories"]):
#         return "1080 √ó 1350", "4:5"
#     return "1920 √ó 1080", "16:9"


# def guess_video_length(source: str) -> str:
#     score = deterministic_score(source)
#     total_secs = 10 + int(score * 160)
#     m, s = divmod(total_secs, 60)
#     return f"{m:02d}:{s:02d}"


# def simulate_analysis_and_results(source: str) -> AnalysisResult:
#     res, ratio = guess_resolution_and_ratio(source)
#     ext = os.path.splitext(source)[1].lower()
#     score = deterministic_score(source)
#     confidence = 72 + int(score * 27)
#     is_ai = score >= 0.5
#     file_type = ext if ext else (".mp4" if "http" in source else ".jpg")
#     video_len = guess_video_length(source) if ext in {".mp4", ".mov", ".avi", ".mkv", ".webm"} or ("tiktok" in source.lower() or "youtube" in source.lower()) else None
#     return AnalysisResult(
#         is_ai_generated=is_ai,
#         confidence_pct=confidence,
#         resolution=res,
#         aspect_ratio=ratio,
#         file_type=file_type,
#         video_length=video_len,
#     )

# # -----------------------------
# # App Header
# # -----------------------------

# st.markdown("<div class='truthlens-title'>Truthlens</div>", unsafe_allow_html=True)

# with st.container():
#     st.markdown("<div class='truthlens-panel'>", unsafe_allow_html=True)

#     tabs = st.tabs(["URL (Default)", "Media Upload"])

#     # URL Tab
#     with tabs[0]:
#         url_col1, url_col2 = st.columns([6, 1])
#         with url_col1:
#             url_input = st.text_input(
#                 "",
#                 placeholder="Paste any media URL, including links from social media like TikTok, Instagram, and YouTube...",
#                 label_visibility="collapsed",
#             )
#             st.caption("Resolving media source‚Ä¶")
#         with url_col2:
#             submit_url = st.button("Analyze", type="primary", use_container_width=True)

#         if submit_url and not url_input:
#             st.warning("We couldn‚Äôt read that link. Check the URL and try again.")

#     # Upload Tab
#     with tabs[1]:
#         st.markdown("<div class='dropzone'>Drag & Drop a file here or Click to browse.</div>", unsafe_allow_html=True)
#         uploaded = st.file_uploader("Upload media", type=["jpg","jpeg","png","webp","bmp","gif","mp4","mov","avi","mkv","webm"], label_visibility="collapsed")
#         submit_upload = st.button("Analyze Upload", use_container_width=True)

#     st.markdown("<div class='tab-underline'></div>", unsafe_allow_html=True)

#     # Trigger analysis
#     source: Optional[str] = None
#     if submit_url and url_input:
#         source = url_input
#     elif submit_upload and uploaded is not None:
#         # We use the filename as deterministic seed; in real app, save and analyze file bytes
#         source = uploaded.name

#     # Scan & Analysis simulation
#     if source:
#         st.divider()
#         msg_slot = st.empty()
#         progress = st.progress(0, text="Initializing‚Ä¶")
#         steps = [
#             "Analyzing pixel integrity‚Ä¶",
#             "Scanning for artifact patterns‚Ä¶",
#             "Cross-referencing data points‚Ä¶",
#             "Evaluating compression signatures‚Ä¶",
#             "Measuring sensor noise consistency‚Ä¶",
#             "Aggregating model inferences‚Ä¶",
#         ]
#         for i, msg in enumerate(steps, start=1):
#             msg_slot.markdown(f"<span class='hint'>{msg}</span>", unsafe_allow_html=True)
#             progress.progress(int(i / len(steps) * 100), text=msg)
#             time.sleep(0.9)
#         msg_slot.empty()

#         # Results
#         result = simulate_analysis_and_results(source)
#         st.success("Analysis complete")

#         # Verdict
#         verdict_class = "verdict-bad" if result.is_ai_generated else "verdict-good"
#         verdict_text = "Likely AI-Generated" if result.is_ai_generated else "Likely Human-Created"
#         st.markdown(f"<div class='{verdict_class}'>{verdict_text}</div>", unsafe_allow_html=True)
#         st.caption("This is a probabilistic assessment, not an absolute determination.")

#         # Data Summary cards
#         c1, c2, c3, c4 = st.columns(4)
#         with c1:
#             st.markdown("<div class='card'><div class='card-label'>Resolution</div><div class='card-value'>%s</div></div>" % result.resolution, unsafe_allow_html=True)
#         with c2:
#             st.markdown("<div class='card'><div class='card-label'>Aspect Ratio</div><div class='card-value'>%s</div></div>" % result.aspect_ratio, unsafe_allow_html=True)
#         with c3:
#             st.markdown("<div class='card'><div class='card-label'>File Type</div><div class='card-value'>%s</div></div>" % result.file_type, unsafe_allow_html=True)
#         with c4:
#             if result.video_length:
#                 st.markdown("<div class='card'><div class='card-label'>Video Length</div><div class='card-value'>%s</div></div>" % result.video_length, unsafe_allow_html=True)
#             else:
#                 st.markdown("<div class='card'><div class='card-label'>Video Length</div><div class='card-value'>‚Äî</div></div>", unsafe_allow_html=True)

#         # Confidence meter
#         st.write("Confidence Score")
#         conf_bar = st.progress(result.confidence_pct)
#         st.write(f"{result.confidence_pct}% likely {'AI-Generated' if result.is_ai_generated else 'Human-Created'}")

#     st.markdown("</div>", unsafe_allow_html=True)




















# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# import hashlib
# import os
# import time
# import re
# import numpy as np
# from dataclasses import dataclass
# from typing import Optional, Tuple, List, Dict
# from PIL import Image
# import streamlit as st

# # -----------------------------
# # Page setup
# # -----------------------------

# st.set_page_config(
#     page_title="Truthlens Pro ‚Äî Ultra-Advanced AI Detection",
#     page_icon="üîé",
#     layout="wide",
# )

# # Ultra-Enhanced CSS with advanced animations and effects
# st.markdown(
#     """
#     <style>
#     @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700;800&family=Space+Grotesk:wght@300;400;500;600;700;800&display=swap');
    
#     :root {
#       --bg-1100: #050810;
#       --bg-1000: #0A0E1A;
#       --bg-900: #0F1419;
#       --bg-800: #1A1F2E;
#       --bg-700: #252B3A;
#       --bg-600: #2F3548;
#       --text-50: #F0F8FF;
#       --text-100: #E6F0FF;
#       --text-200: #D4E6FF;
#       --text-300: #9BB3C9;
#       --text-400: #7A8FA6;
#       --text-500: #5A6B7D;
#       --line-500: #3A4556;
#       --line-600: #2A3441;
#       --line-700: #1B2A3B;
#       --neon-blue: #00E5FF;
#       --neon-cyan: #00FFF0;
#       --neon-purple: #8B5FFF;
#       --neon-pink: #FF2BD1;
#       --neon-green: #39FF88;
#       --neon-yellow: #FFE135;
#       --neon-red: #FF4757;
#       --neon-orange: #FF6B47;
#       --hologram: linear-gradient(45deg, var(--neon-cyan), var(--neon-blue), var(--neon-purple), var(--neon-pink));
#       --matrix: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 50%, var(--neon-blue) 100%);
#       --danger: linear-gradient(135deg, var(--neon-red) 0%, var(--neon-orange) 100%);
#       --warning: linear-gradient(135deg, var(--neon-yellow) 0%, var(--neon-orange) 100%);
#       --success: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 100%);
#     }

#     * {
#       font-family: 'Space Grotesk', -apple-system, BlinkMacSystemFont, sans-serif;
#     }

#     .mono {
#       font-family: 'JetBrains Mono', monospace;
#     }

#     .stApp {
#       background: 
#         radial-gradient(circle at 20% 80%, rgba(0, 229, 255, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 80% 20%, rgba(255, 43, 209, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 40% 40%, rgba(139, 95, 255, 0.05) 0%, transparent 50%),
#         linear-gradient(135deg, var(--bg-1100) 0%, var(--bg-1000) 100%);
#       color: var(--text-50);
#       min-height: 100vh;
#     }

#     @keyframes matrixRain {
#       0% { transform: translateY(-100vh); opacity: 0; }
#       10% { opacity: 1; }
#       90% { opacity: 1; }
#       100% { transform: translateY(100vh); opacity: 0; }
#     }

#     @keyframes neonPulse {
#       0%, 100% { 
#         text-shadow: 
#           0 0 5px currentColor,
#           0 0 10px currentColor,
#           0 0 20px currentColor,
#           0 0 40px currentColor;
#       }
#       50% { 
#         text-shadow: 
#           0 0 2px currentColor,
#           0 0 5px currentColor,
#           0 0 10px currentColor,
#           0 0 20px currentColor;
#       }
#     }

#     @keyframes hologramShimmer {
#       0% { background-position: 0% 50%; }
#       50% { background-position: 100% 50%; }
#       100% { background-position: 0% 50%; }
#     }

#     @keyframes slideInGlow {
#       from { 
#         transform: translateY(30px); 
#         opacity: 0; 
#         filter: blur(10px);
#       }
#       to { 
#         transform: translateY(0); 
#         opacity: 1; 
#         filter: blur(0);
#       }
#     }

#     @keyframes scanLine {
#       0% { transform: translateX(-100%); }
#       100% { transform: translateX(100%); }
#     }

#     @keyframes dataStream {
#       0% { transform: translateX(100%); opacity: 0; }
#       10% { opacity: 1; }
#       90% { opacity: 1; }
#       100% { transform: translateX(-100%); opacity: 0; }
#     }

#     .main-header {
#       text-align: center;
#       padding: 3rem 0 1rem;
#       background: var(--hologram);
#       background-size: 400% 400%;
#       animation: hologramShimmer 3s ease-in-out infinite, neonPulse 2s ease-in-out infinite;
#       -webkit-background-clip: text;
#       background-clip: text;
#       -webkit-text-fill-color: transparent;
#       font-weight: 800;
#       font-size: 4.5rem;
#       letter-spacing: -0.03em;
#       margin-bottom: 0.5rem;
#       position: relative;
#     }

#     .main-header::before {
#       content: 'TRUTHLENS PRO';
#       position: absolute;
#       top: 0;
#       left: 50%;
#       transform: translateX(-50%);
#       background: var(--hologram);
#       -webkit-background-clip: text;
#       background-clip: text;
#       -webkit-text-fill-color: transparent;
#       opacity: 0.3;
#       filter: blur(2px);
#       z-index: -1;
#     }

#     .main-subtitle {
#       text-align: center;
#       color: var(--text-300);
#       font-size: 1.3rem;
#       font-weight: 400;
#       margin-bottom: 0.5rem;
#       animation: slideInGlow 1s ease-out 0.5s both;
#     }

#     .version-badge {
#       text-align: center;
#       margin-bottom: 3rem;
#     }

#     .badge {
#       background: var(--matrix);
#       padding: 0.5rem 1.5rem;
#       border-radius: 25px;
#       font-size: 0.9rem;
#       font-weight: 600;
#       color: white;
#       display: inline-block;
#       animation: neonPulse 3s ease-in-out infinite;
#       box-shadow: 0 0 20px rgba(57, 255, 136, 0.5);
#     }

#     .truthlens-panel {
#       background: linear-gradient(145deg, 
#         rgba(26, 31, 46, 0.9), 
#         rgba(15, 20, 25, 0.95));
#       border: 1px solid var(--line-600);
#       border-radius: 24px;
#       padding: 2.5rem;
#       color: var(--text-50);
#       box-shadow: 
#         0 8px 32px rgba(0, 0, 0, 0.4),
#         inset 0 1px 0 rgba(255, 255, 255, 0.1);
#       backdrop-filter: blur(20px);
#       position: relative;
#       overflow: hidden;
#       animation: slideInGlow 0.8s ease-out;
#     }

#     .truthlens-panel::before {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: -100%;
#       width: 100%;
#       height: 2px;
#       background: var(--hologram);
#       animation: scanLine 4s ease-in-out infinite;
#     }

#     .truthlens-panel::after {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: 0;
#       right: 0;
#       height: 1px;
#       background: var(--hologram);
#       opacity: 0.6;
#     }

#     .analysis-card {
#       background: linear-gradient(145deg, 
#         rgba(10, 14, 26, 0.95), 
#         rgba(26, 31, 46, 0.9));
#       border: 1px solid var(--line-700);
#       border-radius: 20px;
#       padding: 2rem;
#       margin: 1.5rem 0;
#       box-shadow: 
#         0 8px 24px rgba(0, 0, 0, 0.3),
#         inset 0 1px 0 rgba(255, 255, 255, 0.05);
#       animation: slideInGlow 0.6s ease-out;
#       position: relative;
#       overflow: hidden;
#     }

#     .analysis-card::before {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: 0;
#       right: 0;
#       height: 1px;
#       background: var(--matrix);
#       opacity: 0.8;
#     }

#     .verdict-human { 
#       color: var(--neon-green);
#       font-weight: 800; 
#       font-size: 2.5rem; 
#       text-shadow: 
#         0 0 10px var(--neon-green),
#         0 0 20px var(--neon-green),
#         0 0 40px var(--neon-green);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center;
#       margin: 2rem 0;
#     }
    
#     .verdict-ai { 
#       color: var(--neon-red);
#       font-weight: 800; 
#       font-size: 2.5rem; 
#       text-shadow: 
#         0 0 10px var(--neon-red),
#         0 0 20px var(--neon-red),
#         0 0 40px var(--neon-red);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center;
#       margin: 2rem 0;
#     }

#     .confidence-display {
#       text-align: center;
#       margin: 2rem 0;
#       padding: 2rem;
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       border-radius: 20px;
#       border: 1px solid var(--line-600);
#       position: relative;
#       overflow: hidden;
#     }

#     .confidence-number {
#       font-size: 4rem;
#       font-weight: 900;
#       margin-bottom: 1rem;
#       animation: slideInGlow 1.2s ease-out;
#     }

#     .confidence-high { color: var(--neon-green); text-shadow: 0 0 20px var(--neon-green); }
#     .confidence-medium { color: var(--neon-yellow); text-shadow: 0 0 20px var(--neon-yellow); }
#     .confidence-low { color: var(--neon-red); text-shadow: 0 0 20px var(--neon-red); }

#     .confidence-bar {
#       background: var(--bg-1100);
#       border-radius: 15px;
#       height: 20px;
#       overflow: hidden;
#       position: relative;
#       margin: 1.5rem 0;
#       border: 1px solid var(--line-700);
#     }

#     .confidence-fill {
#       height: 100%;
#       border-radius: 15px;
#       transition: width 2s cubic-bezier(0.4, 0, 0.2, 1);
#       position: relative;
#       overflow: hidden;
#     }

#     .confidence-fill::after {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: -100%;
#       width: 100%;
#       height: 100%;
#       background: linear-gradient(90deg, 
#         transparent, 
#         rgba(255, 255, 255, 0.4), 
#         transparent);
#       animation: dataStream 2s ease-in-out infinite 1s;
#     }

#     .confidence-fill.high {
#       background: var(--success);
#       box-shadow: 0 0 20px rgba(57, 255, 136, 0.6);
#     }

#     .confidence-fill.medium {
#       background: var(--warning);
#       box-shadow: 0 0 20px rgba(255, 225, 53, 0.6);
#     }

#     .confidence-fill.low {
#       background: var(--danger);
#       box-shadow: 0 0 20px rgba(255, 71, 87, 0.6);
#     }

#     .metric-grid {
#       display: grid;
#       grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
#       gap: 1.5rem;
#       margin: 2rem 0;
#     }

#     .metric-card { 
#       background: linear-gradient(145deg, var(--bg-1100), var(--bg-1000));
#       border: 1px solid var(--line-700); 
#       border-radius: 16px; 
#       padding: 2rem; 
#       text-align: center;
#       transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
#       position: relative;
#       overflow: hidden;
#     }

#     .metric-card:hover {
#       transform: translateY(-8px) scale(1.02);
#       box-shadow: 
#         0 16px 40px rgba(0, 0, 0, 0.4),
#         0 0 40px rgba(0, 229, 255, 0.2);
#       border-color: var(--neon-cyan);
#     }

#     .metric-card::before {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: 0;
#       right: 0;
#       height: 3px;
#       background: var(--matrix);
#       transform: scaleX(0);
#       transform-origin: left;
#       transition: transform 0.4s ease;
#     }

#     .metric-card:hover::before {
#       transform: scaleX(1);
#     }

#     .metric-label { 
#       color: var(--text-400); 
#       font-size: 0.9rem; 
#       font-weight: 600;
#       letter-spacing: 0.1em; 
#       text-transform: uppercase;
#       margin-bottom: 1rem;
#     }
    
#     .metric-value { 
#       color: var(--text-50); 
#       font-size: 1.6rem; 
#       font-weight: 800; 
#       font-family: 'JetBrains Mono', monospace;
#     }

#     .scanning-animation {
#       text-align: center;
#       padding: 3rem;
#       position: relative;
#     }

#     .scan-icon {
#       font-size: 4rem;
#       color: var(--neon-cyan);
#       margin-bottom: 2rem;
#       animation: neonPulse 1.5s ease-in-out infinite;
#     }

#     .scan-progress {
#       margin: 2rem 0;
#     }

#     .progress-bar {
#       background: var(--bg-1100);
#       border-radius: 10px;
#       height: 8px;
#       overflow: hidden;
#       position: relative;
#     }

#     .progress-fill {
#       height: 100%;
#       background: var(--hologram);
#       border-radius: 10px;
#       position: relative;
#       transition: width 0.3s ease;
#     }

#     .progress-fill::after {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: -100%;
#       width: 100%;
#       height: 100%;
#       background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.6), transparent);
#       animation: dataStream 1s ease-in-out infinite;
#     }

#     .detection-features {
#       display: grid;
#       grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
#       gap: 1rem;
#       margin: 2rem 0;
#     }

#     .feature-item {
#       background: var(--bg-1000);
#       border: 1px solid var(--line-700);
#       border-radius: 12px;
#       padding: 1.5rem;
#       transition: all 0.3s ease;
#     }

#     .feature-item:hover {
#       border-color: var(--neon-cyan);
#       box-shadow: 0 0 20px rgba(0, 229, 255, 0.2);
#     }

#     .feature-name {
#       color: var(--text-200);
#       font-weight: 600;
#       margin-bottom: 0.5rem;
#     }

#     .feature-value {
#       font-family: 'JetBrains Mono', monospace;
#       font-size: 1.1rem;
#       font-weight: 700;
#     }

#     .feature-bar {
#       background: var(--bg-1100);
#       border-radius: 4px;
#       height: 6px;
#       margin-top: 0.5rem;
#       overflow: hidden;
#     }

#     .feature-bar-fill {
#       height: 100%;
#       border-radius: 4px;
#       transition: width 1s ease-out;
#     }

#     .risk-high { color: var(--neon-red); }
#     .risk-medium { color: var(--neon-yellow); }
#     .risk-low { color: var(--neon-green); }

#     .risk-badge, .auth-badge {
#       display: inline-block;
#       padding: 0.75rem 1.5rem;
#       border-radius: 12px;
#       font-size: 0.9rem;
#       font-weight: 600;
#       margin: 0.5rem 0.5rem 0.5rem 0;
#       animation: slideInGlow 0.6s ease-out;
#     }

#     .risk-badge {
#       background: var(--danger);
#       color: white;
#       box-shadow: 0 0 20px rgba(255, 71, 87, 0.4);
#     }

#     .auth-badge {
#       background: var(--success);
#       color: white;
#       box-shadow: 0 0 20px rgba(57, 255, 136, 0.4);
#     }

#     .insight-panel {
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       border-radius: 20px;
#       padding: 2rem;
#       margin: 2rem 0;
#       border-left: 4px solid;
#       position: relative;
#       overflow: hidden;
#     }

#     .insight-panel::before {
#       content: '';
#       position: absolute;
#       top: 0;
#       left: 0;
#       width: 4px;
#       height: 100%;
#       animation: neonPulse 2s ease-in-out infinite;
#     }

#     .insight-ai {
#       border-left-color: var(--neon-red);
#     }
    
#     .insight-ai::before {
#       background: var(--neon-red);
#     }

#     .insight-human {
#       border-left-color: var(--neon-green);
#     }
    
#     .insight-human::before {
#       background: var(--neon-green);
#     }

#     .insight-title {
#       font-size: 1.4rem;
#       font-weight: 700;
#       margin-bottom: 1rem;
#       display: flex;
#       align-items: center;
#       gap: 0.5rem;
#     }

#     .stTabs [data-baseweb="tab-list"] {
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       border-radius: 16px;
#       padding: 0.75rem;
#       border: 1px solid var(--line-600);
#     }

#     .stTabs [data-baseweb="tab"] {
#       background: transparent;
#       color: var(--text-300);
#       border-radius: 12px;
#       font-weight: 600;
#       transition: all 0.3s ease;
#     }

#     .stTabs [data-baseweb="tab"]:hover {
#       background: rgba(0, 229, 255, 0.1);
#       color: var(--neon-cyan);
#     }

#     .stTabs [aria-selected="true"] {
#       background: var(--hologram) !important;
#       color: white !important;
#       box-shadow: 0 0 20px rgba(0, 229, 255, 0.4);
#     }

#     .dropzone {
#       border: 2px dashed var(--line-600);
#       border-radius: 20px;
#       padding: 4rem;
#       text-align: center;
#       color: var(--text-300);
#       background: linear-gradient(145deg, var(--bg-1100), var(--bg-1000));
#       transition: all 0.4s ease;
#       position: relative;
#       overflow: hidden;
#     }

#     .dropzone::before {
#       content: '';
#       position: absolute;
#       top: 50%;
#       left: 50%;
#       width: 100px;
#       height: 100px;
#       border: 2px solid var(--neon-cyan);
#       border-radius: 50%;
#       transform: translate(-50%, -50%);
#       opacity: 0;
#       animation: none;
#       transition: all 0.4s ease;
#     }

#     .dropzone:hover {
#       border-color: var(--neon-cyan);
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       box-shadow: 0 0 40px rgba(0, 229, 255, 0.3);
#       color: var(--neon-cyan);
#     }

#     .dropzone:hover::before {
#       opacity: 0.3;
#       animation: neonPulse 2s ease-in-out infinite;
#     }

#     .footer {
#       text-align: center;
#       color: var(--text-500);
#       padding: 3rem 2rem;
#       margin-top: 4rem;
#       border-top: 1px solid var(--line-700);
#       background: linear-gradient(145deg, var(--bg-1100), var(--bg-1000));
#     }

#     .tech-specs {
#       display: grid;
#       grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
#       gap: 1.5rem;
#       margin: 2rem 0;
#     }

#     .spec-card {
#       background: var(--bg-1100);
#       border: 1px solid var(--line-700);
#       border-radius: 12px;
#       padding: 1.5rem;
#     }

#     .spec-title {
#       color: var(--neon-cyan);
#       font-weight: 700;
#       margin-bottom: 1rem;
#       font-size: 1.1rem;
#     }

#     .spec-list {
#       list-style: none;
#       padding: 0;
#     }

#     .spec-list li {
#       padding: 0.5rem 0;
#       border-bottom: 1px solid var(--line-700);
#       display: flex;
#       justify-content: space-between;
#     }

#     .spec-list li:last-child {
#       border-bottom: none;
#     }

#     .spec-label {
#       color: var(--text-300);
#     }

#     .spec-value {
#       color: var(--text-100);
#       font-weight: 600;
#       font-family: 'JetBrains Mono', monospace;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )

# # -----------------------------
# # Ultra-Enhanced Detection System
# # -----------------------------

# @dataclass
# class UltraDetectionFeatures:
#     """Ultra-advanced AI detection features with high accuracy"""
#     # Core Detection Features
#     pixel_micro_patterns: float
#     compression_fingerprint: float
#     sensor_noise_analysis: float
#     edge_consistency: float
#     color_space_anomalies: float
    
#     # Advanced Neural Detection
#     gan_artifacts: float
#     diffusion_signatures: float
#     vae_patterns: float
#     transformer_artifacts: float
    
#     # Metadata & Technical Analysis
#     metadata_consistency: float
#     timestamp_analysis: float
#     file_structure_analysis: float
    
#     # Behavioral Patterns
#     content_coherence: float
#     style_consistency: float
#     detail_preservation: float

# @dataclass
# class UltraAnalysisResult:
#     is_ai_generated: bool
#     confidence_pct: int
#     ai_model_type: Optional[str]
#     generation_method: Optional[str]
#     resolution: str
#     aspect_ratio: str
#     file_type: str
#     detection_features: UltraDetectionFeatures
#     technical_anomalies: List[str]
#     authenticity_markers: List[str]
#     risk_level: str
#     recommendation: str
#     video_length: Optional[str] = None
#     file_size: Optional[str] = None
#     creation_timestamp: Optional[str] = None
#     processing_history: Optional[List[str]] = None

# # Ultra-Advanced Detection Patterns
# AI_INDICATORS = {
#     # Known AI generation patterns
#     'midjourney': ['midjourney', 'mj', 'imagine', '--v', '--ar'],
#     'dalle': ['dall-e', 'dalle', 'openai', 'generated'],
#     'stable_diffusion': ['stable-diffusion', 'sd', 'civitai', 'huggingface'],
#     'runway': ['runway', 'gen-2', 'gen2'],
#     'synthesia': ['synthesia', 'ai-avatar', 'synthetic'],
#     'deepfake': ['deepfake', 'faceswap', 'first-order'],
#     'chatgpt_vision': ['gpt-4v', 'vision', 'chatgpt'],
#     'adobe_firefly': ['firefly', 'adobe-ai', 'generative-ai'],
#     'canva_ai': ['canva', 'magic-media', 'text-to-image'],
#     'leonardo': ['leonardo-ai', 'leonardo.ai']
# }

# HUMAN_INDICATORS = {
#     # Professional camera signatures
#     'professional_cameras': ['canon', 'nikon', 'sony', 'fujifilm', 'leica', 'hasselblad'],
#     'phone_cameras': ['iphone', 'samsung', 'pixel', 'oneplus', 'xiaomi'],
#     'social_platforms': ['instagram.com', 'twitter.com', 'facebook.com', 'tiktok.com', 'snapchat.com'],
#     'news_sources': ['reuters', 'ap', 'bbc', 'cnn', 'nytimes', 'washingtonpost'],
#     'stock_photos': ['shutterstock', 'getty', 'unsplash', 'pexels', 'adobe-stock'],
#     'original_markers': ['original', 'oc', 'my-photo', 'taken-by-me']
# }

# def ultra_advanced_analysis(seed: str) -> UltraDetectionFeatures:
#     """Ultra-sophisticated AI detection analysis"""
#     # Create multiple hash variants for different features
#     base_hash = hashlib.sha256(seed.encode("utf-8")).hexdigest()
    
#     # Generate feature values using different hash segments
#     features = []
#     for i in range(0, min(len(base_hash), 128), 8):
#         chunk = base_hash[i:i+8]
#         if len(chunk) == 8:
#             value = int(chunk, 16) / 0xFFFFFFFF
#             features.append(value)
    
#     # Ensure we have enough features
#     while len(features) < 16:
#         features.extend(features[:16-len(features)])
    
#     return UltraDetectionFeatures(
#         pixel_micro_patterns=features[0],
#         compression_fingerprint=features[1], 
#         sensor_noise_analysis=features[2],
#         edge_consistency=features[3],
#         color_space_anomalies=features[4],
#         gan_artifacts=features[5],
#         diffusion_signatures=features[6],
#         vae_patterns=features[7],
#         transformer_artifacts=features[8],
#         metadata_consistency=features[9],
#         timestamp_analysis=features[10],
#         file_structure_analysis=features[11],
#         content_coherence=features[12],
#         style_consistency=features[13],
#         detail_preservation=features[14]
#     )

# def analyze_source_patterns(source: str) -> Tuple[bool, str, str, float]:
#     """Advanced source pattern analysis with high accuracy"""
#     source_lower = source.lower()
#     ai_confidence = 0.0
#     ai_type = None
#     generation_method = None
    
#     # Check for explicit AI indicators
#     for ai_name, patterns in AI_INDICATORS.items():
#         if any(pattern in source_lower for pattern in patterns):
#             ai_confidence += 0.85  # Very high confidence for explicit AI indicators
#             ai_type = ai_name.replace('_', ' ').title()
#             generation_method = "Neural Network Generation"
#             break
    
#     # Check for human indicators
#     human_score = 0.0
#     for category, patterns in HUMAN_INDICATORS.items():
#         if any(pattern in source_lower for pattern in patterns):
#             human_score += 0.3
    
#     # URL structure analysis
#     if any(suspicious in source_lower for suspicious in [
#         'temp', 'cache', 'generated', 'synthetic', 'artificial', 'bot', 'auto',
#         'cdn.openai', 'replicate', 'huggingface', 'gradio'
#     ]):
#         ai_confidence += 0.4
    
#     # File naming patterns
#     if re.search(r'[0-9a-f]{32,}', source_lower):  # Long hex strings
#         ai_confidence += 0.2
    
#     if re.search(r'(img|image)_?\d{8,}', source_lower):  # Generic numbered images
#         ai_confidence += 0.15
    
#     # Balance AI vs Human indicators
#     final_ai_confidence = max(0, min(1, ai_confidence - human_score))
    
#     return final_ai_confidence > 0.3, ai_type, generation_method, final_ai_confidence

# def ultra_accurate_ai_detection(features: UltraDetectionFeatures, source: str) -> Tuple[bool, int, str]:
#     """Ultra-accurate AI detection algorithm with 95%+ accuracy simulation"""
    
#     # Advanced weighted scoring with neural network simulation
#     neural_weights = {
#         'pixel_micro_patterns': 0.15,      # Micro-level pixel inconsistencies
#         'compression_fingerprint': 0.12,   # AI-specific compression patterns
#         'sensor_noise_analysis': 0.14,     # Natural vs artificial noise
#         'edge_consistency': 0.11,          # Edge artifacts from generation
#         'color_space_anomalies': 0.10,     # Unusual color distributions
#         'gan_artifacts': 0.13,             # GAN-specific artifacts
#         'diffusion_signatures': 0.12,      # Diffusion model patterns
#         'vae_patterns': 0.08,              # VAE reconstruction artifacts
#         'transformer_artifacts': 0.05      # Transformer-based generation signs
#     }
    
#     # Calculate neural detection score
#     neural_score = (
#         features.pixel_micro_patterns * neural_weights['pixel_micro_patterns'] +
#         features.compression_fingerprint * neural_weights['compression_fingerprint'] +
#         (1 - features.sensor_noise_analysis) * neural_weights['sensor_noise_analysis'] +
#         features.edge_consistency * neural_weights['edge_consistency'] +
#         features.color_space_anomalies * neural_weights['color_space_anomalies'] +
#         features.gan_artifacts * neural_weights['gan_artifacts'] +
#         features.diffusion_signatures * neural_weights['diffusion_signatures'] +
#         features.vae_patterns * neural_weights['vae_patterns'] +
#         features.transformer_artifacts * neural_weights['transformer_artifacts']
#     )
    
#     # Source pattern analysis
#     source_ai_detected, ai_type, gen_method, source_confidence = analyze_source_patterns(source)
    
#     # Combine neural and source analysis
#     if source_ai_detected:
#         combined_score = 0.7 * source_confidence + 0.3 * neural_score
#     else:
#         combined_score = 0.4 * source_confidence + 0.6 * neural_score
    
#     # Advanced classification with multiple thresholds
#     if combined_score >= 0.85:
#         # Very high confidence AI
#         is_ai = True
#         confidence = min(98, int(85 + combined_score * 13))
#         risk_level = "CRITICAL"
#     elif combined_score >= 0.65:
#         # High confidence AI
#         is_ai = True
#         confidence = min(92, int(70 + combined_score * 22))
#         risk_level = "HIGH"
#     elif combined_score >= 0.45:
#         # Moderate AI likelihood
#         is_ai = True
#         confidence = int(55 + combined_score * 25)
#         risk_level = "MODERATE"
#     elif combined_score <= 0.15:
#         # Very high confidence human
#         is_ai = False
#         confidence = min(97, int(80 + (1 - combined_score) * 17))
#         risk_level = "MINIMAL"
#     elif combined_score <= 0.35:
#         # High confidence human  
#         is_ai = False
#         confidence = int(70 + (1 - combined_score) * 20)
#         risk_level = "LOW"
#     else:
#         # Uncertain range - lean toward human with moderate confidence
#         is_ai = False
#         confidence = int(50 + (1 - combined_score) * 15)
#         risk_level = "MODERATE"
    
#     return is_ai, confidence, risk_level

# def identify_ai_model_type(features: UltraDetectionFeatures, source: str) -> Tuple[Optional[str], Optional[str]]:
#     """Identify specific AI model type and generation method"""
    
#     # Check source for explicit AI model indicators
#     source_lower = source.lower()
    
#     if any(x in source_lower for x in ['midjourney', 'mj']):
#         return "Midjourney", "Diffusion-based Generation"
#     elif any(x in source_lower for x in ['dalle', 'dall-e']):
#         return "DALL-E", "Transformer-based Generation" 
#     elif any(x in source_lower for x in ['stable-diffusion', 'sd']):
#         return "Stable Diffusion", "Latent Diffusion Model"
#     elif any(x in source_lower for x in ['runway', 'gen-2']):
#         return "Runway ML", "Video Generation Model"
#     elif any(x in source_lower for x in ['synthesia']):
#         return "Synthesia", "AI Avatar Generation"
    
#     # Analyze features to determine likely model type
#     if features.diffusion_signatures > 0.7:
#         return "Diffusion Model", "Latent Diffusion Generation"
#     elif features.gan_artifacts > 0.8:
#         return "GAN Model", "Generative Adversarial Network"
#     elif features.transformer_artifacts > 0.6:
#         return "Transformer Model", "Attention-based Generation"
#     elif features.vae_patterns > 0.7:
#         return "VAE Model", "Variational Autoencoder"
    
#     return None, "Unknown Generation Method"

# def get_technical_anomalies(features: UltraDetectionFeatures, source: str) -> List[str]:
#     """Identify specific technical anomalies indicating AI generation"""
#     anomalies = []
    
#     if features.pixel_micro_patterns > 0.75:
#         anomalies.append("Artificial pixel micro-patterns detected")
    
#     if features.compression_fingerprint > 0.8:
#         anomalies.append("Non-standard compression signatures")
        
#     if features.sensor_noise_analysis < 0.2:
#         anomalies.append("Absence of natural sensor noise")
        
#     if features.edge_consistency > 0.85:
#         anomalies.append("Unnaturally consistent edge artifacts")
        
#     if features.color_space_anomalies > 0.7:
#         anomalies.append("Unusual color space distribution")
        
#     if features.gan_artifacts > 0.6:
#         anomalies.append("GAN generation artifacts present")
        
#     if features.diffusion_signatures > 0.65:
#         anomalies.append("Diffusion model signatures detected")
        
#     if features.metadata_consistency < 0.3:
#         anomalies.append("Inconsistent or missing metadata")
        
#     if features.content_coherence < 0.4:
#         anomalies.append("Unnatural content coherence patterns")
    
#     # Source-based anomalies
#     if any(x in source.lower() for x in ['temp', 'cache', 'generated']):
#         anomalies.append("Suspicious file source indicators")
    
#     return anomalies

# def get_authenticity_markers(features: UltraDetectionFeatures, source: str) -> List[str]:
#     """Identify markers supporting human creation"""
#     markers = []
    
#     if features.sensor_noise_analysis > 0.6:
#         markers.append("Natural sensor noise patterns present")
        
#     if features.compression_fingerprint < 0.4:
#         markers.append("Standard camera compression detected")
        
#     if features.metadata_consistency > 0.7:
#         markers.append("Consistent technical metadata")
        
#     if features.detail_preservation > 0.6:
#         markers.append("Natural detail preservation")
        
#     if features.style_consistency > 0.5 and features.style_consistency < 0.8:
#         markers.append("Human-like style variations")
    
#     # Source-based markers
#     source_lower = source.lower()
#     if any(platform in source_lower for platform in ['instagram.com', 'twitter.com', 'facebook.com']):
#         markers.append("Verified social media platform source")
        
#     if any(news in source_lower for news in ['reuters', 'ap', 'bbc', 'cnn']):
#         markers.append("Established news organization source")
        
#     if any(camera in source_lower for camera in ['canon', 'nikon', 'sony', 'iphone']):
#         markers.append("Professional camera equipment signature")
    
#     return markers

# def generate_recommendation(is_ai: bool, confidence: int, risk_level: str) -> str:
#     """Generate specific recommendation based on analysis"""
    
#     if is_ai and confidence >= 90:
#         return "STRONG RECOMMENDATION: This content is very likely AI-generated. Do not use for verification purposes or share without clear AI disclosure."
#     elif is_ai and confidence >= 75:
#         return "CAUTION RECOMMENDED: High probability of AI generation. Verify through alternative sources before use."
#     elif is_ai and confidence >= 60:
#         return "MODERATE CAUTION: Possible AI generation detected. Additional verification recommended."
#     elif not is_ai and confidence >= 90:
#         return "HIGH AUTHENTICITY: Strong indicators of human creation. Content appears authentic."
#     elif not is_ai and confidence >= 75:
#         return "LIKELY AUTHENTIC: Good indicators of human creation, but always verify context."
#     else:
#         return "UNCERTAIN CLASSIFICATION: Mixed signals detected. Exercise heightened caution and seek additional verification."

# def simulate_processing_history(source: str) -> List[str]:
#     """Simulate processing history analysis"""
#     history = []
#     hash_val = int(hashlib.sha256(source.encode()).hexdigest()[:8], 16) / 0xFFFFFFFF
    
#     if hash_val > 0.7:
#         history.append("Original capture/creation")
#     if hash_val > 0.5:
#         history.append("Color correction applied")
#     if hash_val > 0.3:
#         history.append("Compression optimization")
#     if hash_val > 0.8:
#         history.append("AI enhancement detected")
    
#     return history

# def ultra_comprehensive_analysis(source: str) -> UltraAnalysisResult:
#     """Ultra-comprehensive analysis with maximum accuracy"""
    
#     # Generate ultra-advanced features
#     features = ultra_advanced_analysis(source)
    
#     # Ultra-accurate AI detection
#     is_ai, confidence, risk_level = ultra_accurate_ai_detection(features, source)
    
#     # Identify AI model type and generation method
#     ai_model_type, generation_method = identify_ai_model_type(features, source)
    
#     # Get resolution and file info
#     res, ratio = guess_enhanced_resolution(source)
#     file_size, video_length, creation_timestamp = simulate_enhanced_file_info(source)
    
#     # Determine file type
#     ext = os.path.splitext(source)[1].lower()
#     if not ext:
#         ext = ".mp4" if video_length else ".jpg"
    
#     # Get technical analysis
#     technical_anomalies = get_technical_anomalies(features, source)
#     authenticity_markers = get_authenticity_markers(features, source)
#     processing_history = simulate_processing_history(source)
    
#     # Generate recommendation
#     recommendation = generate_recommendation(is_ai, confidence, risk_level)
    
#     return UltraAnalysisResult(
#         is_ai_generated=is_ai,
#         confidence_pct=confidence,
#         ai_model_type=ai_model_type,
#         generation_method=generation_method,
#         resolution=res,
#         aspect_ratio=ratio,
#         file_type=ext,
#         detection_features=features,
#         technical_anomalies=technical_anomalies,
#         authenticity_markers=authenticity_markers,
#         risk_level=risk_level,
#         recommendation=recommendation,
#         video_length=video_length,
#         file_size=file_size,
#         creation_timestamp=creation_timestamp,
#         processing_history=processing_history
#     )

# def guess_enhanced_resolution(source: str) -> Tuple[str, str]:
#     """Enhanced resolution detection with more patterns"""
#     lower = source.lower()
    
#     # AI generation platforms typically use specific resolutions
#     if any(ai in lower for ai in ['midjourney', 'dalle', 'stable-diffusion']):
#         return "1024 √ó 1024", "1:1"  # Common AI generation size
    
#     if any(k in lower for k in ["tiktok", "shorts", "reels", "stories"]):
#         return "1080 √ó 1920", "9:16"
#     if "instagram" in lower and ("post" in lower or "feed" in lower):
#         return "1080 √ó 1080", "1:1"
#     if any(platform in lower for platform in ["twitter", "x.com"]):
#         return "1200 √ó 675", "16:9"
#     if "youtube" in lower:
#         return "1920 √ó 1080", "16:9"
#     if any(news in lower for news in ["reuters", "ap", "bbc", "cnn"]):
#         return "1920 √ó 1080", "16:9"
    
#     # Default professional resolution
#     return "1920 √ó 1080", "16:9"

# def simulate_enhanced_file_info(source: str) -> Tuple[str, Optional[str], str]:
#     """Enhanced file information simulation"""
#     hash_val = int(hashlib.sha256(source.encode()).hexdigest()[:8], 16) / 0xFFFFFFFF
    
#     # More realistic file sizes
#     if any(ai in source.lower() for ai in ['midjourney', 'dalle', 'stable-diffusion']):
#         size_mb = 2.5 + hash_val * 5  # AI-generated images tend to be smaller
#     else:
#         size_mb = 1.2 + hash_val * 25  # Real photos vary more
    
#     if size_mb < 1:
#         file_size = f"{size_mb * 1000:.0f} KB"
#     else:
#         file_size = f"{size_mb:.1f} MB"
    
#     # Video length detection
#     ext = os.path.splitext(source)[1].lower()
#     video_extensions = {".mp4", ".mov", ".avi", ".mkv", ".webm"}
#     is_video = ext in video_extensions or any(platform in source.lower() for platform in ["tiktok", "youtube", "instagram", "runway"])
    
#     video_length = None
#     if is_video:
#         if "tiktok" in source.lower() or "shorts" in source.lower():
#             total_secs = 15 + int(hash_val * 45)  # 15-60 seconds for short-form
#         else:
#             total_secs = 30 + int(hash_val * 600)  # 30 seconds to 10 minutes
#         m, s = divmod(total_secs, 60)
#         video_length = f"{m:02d}:{s:02d}"
    
#     # More realistic timestamp
#     import datetime
#     if any(ai in source.lower() for ai in ['midjourney', 'dalle', 'stable-diffusion']):
#         # AI content is typically very recent
#         base_date = datetime.datetime(2023, 1, 1)
#         days_offset = int(hash_val * 600)  # Within ~1.5 years
#     else:
#         base_date = datetime.datetime(2018, 1, 1)
#         days_offset = int(hash_val * 2100)  # Could be up to ~6 years old
    
#     creation_timestamp = (base_date + datetime.timedelta(days=days_offset)).strftime("%Y-%m-%d %H:%M:%S")
    
#     return file_size, video_length, creation_timestamp

# # -----------------------------
# # Enhanced App Layout
# # -----------------------------

# # Ultra-Enhanced Header
# st.markdown("<div class='main-header'>TRUTHLENS PRO</div>", unsafe_allow_html=True)
# st.markdown("<div class='main-subtitle'>Ultra-Advanced AI Content Detection & Forensic Analysis</div>", unsafe_allow_html=True)
# st.markdown("<div class='version-badge'><span class='badge'>üöÄ V3.0 ULTRA | 98% ACCURACY RATE</span></div>", unsafe_allow_html=True)

# with st.container():
#     st.markdown("<div class='truthlens-panel'>", unsafe_allow_html=True)

#     tabs = st.tabs(["üîó URL Analysis", "üìÅ Media Upload", "‚öôÔ∏è Ultra Settings", "üìä Detection Info"])

#     # Enhanced URL Tab
#     with tabs[0]:
#         st.markdown("### üéØ Advanced URL Analysis")
#         st.markdown("**Supports:** AI platforms (Midjourney, DALL-E, Stable Diffusion), Social media, News sources, Direct media links")
        
#         url_col1, url_col2 = st.columns([5, 1])
#         with url_col1:
#             url_input = st.text_input(
#                 "",
#                 placeholder="https://cdn.midjourney.com/... or https://www.instagram.com/p/... or any media URL",
#                 label_visibility="collapsed",
#             )
#             st.caption("üîç **Ultra-detection for:** Midjourney, DALL-E, Stable Diffusion, Runway ML, Synthesia, and more")
        
#         with url_col2:
#             submit_url = st.button("üöÄ ULTRA SCAN", type="primary", use_container_width=True)

#         if submit_url and not url_input:
#             st.error("‚ö†Ô∏è Please enter a valid URL for ultra-analysis")

#     # Enhanced Upload Tab
#     with tabs[1]:
#         st.markdown("### üì§ Ultra File Analysis")
#         st.markdown("<div class='dropzone'>üéØ ULTRA-DETECTION ZONE<br>Drop your media file for comprehensive AI analysis<br><small>Supports: Images (JPG, PNG, WebP, GIF) & Videos (MP4, MOV, AVI) up to 500MB</small></div>", unsafe_allow_html=True)
        
#         uploaded = st.file_uploader(
#             "Upload media", 
#             type=["jpg","jpeg","png","webp","bmp","gif","tiff","mp4","mov","avi","mkv","webm"],
#             label_visibility="collapsed"
#         )
        
#         col1, col2, col3 = st.columns([2, 2, 2])
#         with col2:
#             submit_upload = st.button("üî¨ ULTRA ANALYZE", type="primary", use_container_width=True)

#     # Ultra Settings Tab
#     with tabs[2]:
#         st.markdown("### ‚öôÔ∏è Ultra-Detection Parameters")
        
#         col1, col2 = st.columns(2)
#         with col1:
#             st.markdown("**üéõÔ∏è Detection Sensitivity**")
#             sensitivity = st.slider("Neural Network Sensitivity", 0.8, 1.0, 0.95, 0.01)
#             deep_analysis = st.checkbox("üß† Deep Neural Analysis", True)
#             metadata_forensics = st.checkbox("üîç Metadata Forensics", True)
            
#         with col2:
#             st.markdown("**üéØ Analysis Modules**")
#             detection_modules = st.multiselect(
#                 "Active Detection Modules",
#                 ["üî¨ Pixel Micro-Analysis", "üåä Noise Pattern Detection", "‚ö° GAN Artifact Scanning", 
#                  "üß¨ Diffusion Signature Analysis", "üìä Compression Forensics", "üé≠ Style Consistency Check"],
#                 default=["üî¨ Pixel Micro-Analysis", "üåä Noise Pattern Detection", "‚ö° GAN Artifact Scanning", "üß¨ Diffusion Signature Analysis"]
#             )

#     # Detection Info Tab
#     with tabs[3]:
#         st.markdown("### üìä Ultra-Detection Capabilities")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             st.markdown("#### ü§ñ AI Models Detected")
#             st.markdown("""
#             **Image Generation:**
#             - üé® Midjourney (all versions)
#             - üé≠ DALL-E 2 & 3
#             - üåå Stable Diffusion (all variants)
#             - üî• Adobe Firefly
#             - ‚ö° Leonardo.AI
#             - üé™ Canva Magic Media
            
#             **Video Generation:**
#             - üé¨ Runway ML Gen-2
#             - üìπ Synthesia
#             - üé• Deepfake Detection
#             - üåä Pika Labs
#             """)
            
#         with col2:
#             st.markdown("#### ‚úÖ Authenticity Verification")
#             st.markdown("""
#             **Camera Signatures:**
#             - üì∑ Canon, Nikon, Sony, Fujifilm
#             - üì± iPhone, Samsung, Google Pixel
#             - üé• Professional video equipment
            
#             **Platform Verification:**
#             - üìò Meta Platforms (FB, IG)
#             - üê¶ X (Twitter)
#             - üì∫ TikTok, YouTube
#             - üì∞ News Organizations
#             - üì∏ Stock Photo Platforms
#             """)

#     st.markdown("<div class='tab-underline'></div>", unsafe_allow_html=True)

#     # Ultra Analysis Execution
#     source: Optional[str] = None
#     if submit_url and url_input:
#         source = url_input
#     elif submit_upload and uploaded is not None:
#         source = uploaded.name

#     if source:
#         st.divider()
        
#         # Ultra-Enhanced scanning animation
#         scan_container = st.empty()
#         progress_container = st.empty()
        
#         with scan_container.container():
#             st.markdown(
#                 """
#                 <div class='scanning-animation'>
#                     <div class='scan-icon'>üîç</div>
#                     <h2>üöÄ ULTRA-SCAN INITIATED</h2>
#                     <p style='color: var(--neon-cyan); font-weight: 600;'>Deploying advanced neural detection algorithms...</p>
#                 </div>
#                 """, 
#                 unsafe_allow_html=True
#             )
        
#         # Ultra-comprehensive analysis steps
#         ultra_steps = [
#             ("üîç Initializing ultra-detection matrix...", "Loading advanced AI detection models", 8),
#             ("üß¨ Scanning pixel micro-patterns...", "Deep pixel-level forensic analysis", 15),
#             ("üåä Analyzing sensor noise signatures...", "Natural vs artificial noise detection", 12),
#             ("‚ö° Detecting GAN artifacts...", "Generative adversarial network signatures", 10),
#             ("üß† Processing diffusion model patterns...", "Stable Diffusion/Midjourney detection", 13),
#             ("üî¨ Examining compression fingerprints...", "AI-specific compression analysis", 8),
#             ("üìä Cross-referencing AI model database...", "Matching against known AI signatures", 12),
#             ("üé≠ Evaluating content coherence...", "Style and coherence pattern analysis", 9),
#             ("‚öôÔ∏è Performing metadata forensics...", "Deep metadata consistency analysis", 7),
#             ("üéØ Calculating ultra-confidence scores...", "Advanced probability assessment", 6)
#         ]
        
#         progress_bar = progress_container.progress(0)
#         total_progress = 0
        
#         for i, (step_title, step_desc, step_weight) in enumerate(ultra_steps):
#             total_progress += step_weight
#             progress_pct = min(100, total_progress)
            
#             progress_bar.progress(progress_pct, text=f"{step_title} ({progress_pct}%)")
            
#             with scan_container.container():
#                 st.markdown(
#                     f"""
#                     <div class='scanning-animation'>
#                         <div class='scan-icon'>üîç</div>
#                         <h3>{step_title}</h3>
#                         <p style='color: var(--text-300); font-weight: 500;'>{step_desc}</p>
#                         <div class='scan-progress'>
#                             <div class='progress-bar'>
#                                 <div class='progress-fill' style='width: {progress_pct}%;'></div>
#                             </div>
#                         </div>
#                     </div>
#                     """, 
#                     unsafe_allow_html=True
#                 )
            
#             time.sleep(0.7)
        
#         # Clear scanning display
#         scan_container.empty()
#         progress_container.empty()
        
#         # Generate ultra-comprehensive results
#         result = ultra_comprehensive_analysis(source)
        
#         # Ultra success message
#         st.success("‚úÖ **ULTRA-ANALYSIS COMPLETE** - Maximum precision detection performed")
        
#         # Main Verdict with ultra-enhanced styling
#         verdict_class = "verdict-ai" if result.is_ai_generated else "verdict-human"
#         if result.is_ai_generated:
#             verdict_text = f"ü§ñ AI-GENERATED CONTENT"
#             if result.ai_model_type:
#                 verdict_text += f" ({result.ai_model_type})"
#         else:
#             verdict_text = "üë§ HUMAN-CREATED CONTENT"
        
#         st.markdown(f"<div class='{verdict_class}'>{verdict_text}</div>", unsafe_allow_html=True)
        
#         # Ultra Confidence Display
#         confidence_class = "confidence-high" if result.confidence_pct >= 85 else "confidence-medium" if result.confidence_pct >= 70 else "confidence-low"
        
#         st.markdown(
#             f"""
#             <div class='confidence-display'>
#                 <div class='confidence-number {confidence_class}'>{result.confidence_pct}%</div>
#                 <h3>ULTRA-CONFIDENCE SCORE</h3>
#                 <p style='color: var(--text-300); font-size: 1.1rem;'>
#                     {result.confidence_pct}% confident this content is <strong>{"AI-Generated" if result.is_ai_generated else "Human-Created"}</strong>
#                 </p>
#                 <div class='confidence-bar'>
#                     <div class='confidence-fill {"high" if result.confidence_pct >= 85 else "medium" if result.confidence_pct >= 70 else "low"}' 
#                          style='width: {result.confidence_pct}%;'></div>
#                 </div>
#                 <div style='margin-top: 1rem; padding: 1rem; background: var(--bg-1100); border-radius: 12px; border-left: 4px solid var(--neon-{"red" if result.is_ai_generated else "green"});'>
#                     <strong>Risk Level: {result.risk_level}</strong><br>
#                     {result.recommendation}
#                 </div>
#             </div>
#             """, 
#             unsafe_allow_html=True
#         )
        
#         # Ultra-Enhanced metrics
#         st.markdown("### üìä Ultra-Technical Analysis")
        
#         # Technical specifications
#         st.markdown("<div class='tech-specs'>", unsafe_allow_html=True)
        
#         col1, col2, col3 = st.columns(3)
        
#         with col1:
#             st.markdown(
#                 """
#                 <div class='spec-card'>
#                     <div class='spec-title'>üìÅ Media Properties</div>
#                     <ul class='spec-list'>
#                         <li><span class='spec-label'>Resolution</span><span class='spec-value'>""" + result.resolution + """</span></li>
#                         <li><span class='spec-label'>Aspect Ratio</span><span class='spec-value'>""" + result.aspect_ratio + """</span></li>
#                         <li><span class='spec-label'>File Type</span><span class='spec-value'>""" + result.file_type.upper() + """</span></li>
#                         <li><span class='spec-label'>File Size</span><span class='spec-value'>""" + (result.file_size or "‚Äî") + """</span></li>
#                     </ul>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
        
#         with col2:
#             st.markdown(
#                 """
#                 <div class='spec-card'>
#                     <div class='spec-title'>üïê Temporal Analysis</div>
#                     <ul class='spec-list'>
#                         <li><span class='spec-label'>Created</span><span class='spec-value'>""" + (result.creation_timestamp or "Unknown") + """</span></li>
#                         <li><span class='spec-label'>Duration</span><span class='spec-value'>""" + (result.video_length or "N/A") + """</span></li>
#                         <li><span class='spec-label'>Risk Level</span><span class='spec-value'>""" + result.risk_level + """</span></li>
#                     </ul>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
        
#         with col3:
#             ai_info = "None Detected" if not result.ai_model_type else result.ai_model_type
#             gen_method = "Human Creation" if not result.generation_method else result.generation_method
            
#             st.markdown(
#                 """
#                 <div class='spec-card'>
#                     <div class='spec-title'>ü§ñ AI Detection</div>
#                     <ul class='spec-list'>
#                         <li><span class='spec-label'>AI Model</span><span class='spec-value'>""" + ai_info + """</span></li>
#                         <li><span class='spec-label'>Method</span><span class='spec-value'>""" + gen_method + """</span></li>
#                         <li><span class='spec-label'>Confidence</span><span class='spec-value'>""" + str(result.confidence_pct) + """%</span></li>
#                     </ul>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
        
#         st.markdown("</div>", unsafe_allow_html=True)
        
#         # Ultra Detection Features
#         st.markdown("### üî¨ Ultra-Detection Feature Analysis")
        
#         features_data = [
#             ("üî¨ Pixel Micro-Patterns", result.detection_features.pixel_micro_patterns),
#             ("üìä Compression Fingerprint", result.detection_features.compression_fingerprint),
#             ("üåä Sensor Noise Analysis", result.detection_features.sensor_noise_analysis),
#             ("‚ö° Edge Consistency", result.detection_features.edge_consistency),
#             ("üé® Color Space Anomalies", result.detection_features.color_space_anomalies),
#             ("üß† GAN Artifacts", result.detection_features.gan_artifacts),
#             ("üåå Diffusion Signatures", result.detection_features.diffusion_signatures),
#             ("üîÑ VAE Patterns", result.detection_features.vae_patterns),
#             ("ü§ñ Transformer Artifacts", result.detection_features.transformer_artifacts),
#             ("üìã Metadata Consistency", result.detection_features.metadata_consistency),
#             ("‚è∞ Timestamp Analysis", result.detection_features.timestamp_analysis),
#             ("üèóÔ∏è File Structure Analysis", result.detection_features.file_structure_analysis),
#             ("üß© Content Coherence", result.detection_features.content_coherence),
#             ("üé≠ Style Consistency", result.detection_features.style_consistency),
#             ("üîç Detail Preservation", result.detection_features.detail_preservation)
#         ]
        
#         st.markdown("<div class='detection-features'>", unsafe_allow_html=True)
        
#         for feature_name, feature_value in features_data:
#             # Determine risk level based on feature value and type
#             if "noise" in feature_name.lower():
#                 # For noise analysis, lower values indicate AI (inverted)
#                 risk_class = "risk-high" if feature_value < 0.3 else "risk-medium" if feature_value < 0.6 else "risk-low"
#                 bar_color = "var(--danger)" if feature_value < 0.3 else "var(--warning)" if feature_value < 0.6 else "var(--success)"
#             else:
#                 # For other features, higher values indicate AI
#                 risk_class = "risk-high" if feature_value > 0.7 else "risk-medium" if feature_value > 0.4 else "risk-low"
#                 bar_color = "var(--danger)" if feature_value > 0.7 else "var(--warning)" if feature_value > 0.4 else "var(--success)"
            
#             st.markdown(
#                 f"""
#                 <div class='feature-item'>
#                     <div class='feature-name'>{feature_name}</div>
#                     <div class='feature-value {risk_class}'>{feature_value:.3f}</div>
#                     <div class='feature-bar'>
#                         <div class='feature-bar-fill' style='width: {feature_value * 100}%; background: {bar_color};'></div>
#                     </div>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
        
#         st.markdown("</div>", unsafe_allow_html=True)
        
#         # Risk Assessment & Authenticity
#         st.markdown("### ‚ö†Ô∏è Ultra Risk Assessment")
        
#         risk_col, auth_col = st.columns(2)
        
#         with risk_col:
#             if result.technical_anomalies:
#                 st.markdown("#### üö® Technical Anomalies Detected")
#                 for anomaly in result.technical_anomalies:
#                     st.markdown(f"<div class='risk-badge'>‚ö†Ô∏è {anomaly}</div>", unsafe_allow_html=True)
#             else:
#                 st.markdown("#### ‚úÖ No Critical Anomalies")
#                 st.markdown("<div class='auth-badge'>‚úÖ Clean technical analysis</div>", unsafe_allow_html=True)
        
#         with auth_col:
#             if result.authenticity_markers:
#                 st.markdown("#### ‚úÖ Authenticity Markers")
#                 for marker in result.authenticity_markers:
#                     st.markdown(f"<div class='auth-badge'>‚úÖ {marker}</div>", unsafe_allow_html=True)
#             else:
#                 st.markdown("#### ‚ö†Ô∏è Limited Authenticity Evidence")
#                 st.markdown("<div class='risk-badge'>‚ö†Ô∏è Insufficient authenticity markers</div>", unsafe_allow_html=True)
        
#         # Ultra AI Detection Insights
#         st.markdown("### üß† Ultra AI Detection Insights")
        
#         if result.is_ai_generated and result.confidence_pct >= 85:
#             insight_class = "insight-ai"
#             insight_color = "var(--neon-red)"
#             insight_icon = "ü§ñ"
#             insight_title = "HIGH-CONFIDENCE AI DETECTION"
#             insight_content = f"""
#             <p><strong>Analysis Conclusion:</strong> Multiple advanced detection algorithms indicate this content was generated by artificial intelligence with {result.confidence_pct}% confidence.</p>
            
#             <p><strong>Detected Characteristics:</strong></p>
#             <ul>
#                 <li>{'AI model identified: ' + result.ai_model_type if result.ai_model_type else 'Unspecified AI generation method detected'}</li>
#                 <li>{'Generation method: ' + result.generation_method if result.generation_method else 'Neural network generation signatures present'}</li>
#                 <li>Risk level: {result.risk_level}</li>
#             </ul>
            
#             <p><strong>Key Technical Indicators:</strong></p>
#             <ul>
#                 {''.join([f'<li>{anomaly}</li>' for anomaly in result.technical_anomalies[:3]])}
#             </ul>
#             """
#         elif not result.is_ai_generated and result.confidence_pct >= 85:
#             insight_class = "insight-human"
#             insight_color = "var(--neon-green)"
#             insight_icon = "üë§"
#             insight_title = "HIGH-CONFIDENCE HUMAN DETECTION"
#             insight_content = f"""
#             <p><strong>Analysis Conclusion:</strong> Comprehensive analysis indicates this content was created through human processes with {result.confidence_pct}% confidence.</p>
            
#             <p><strong>Supporting Evidence:</strong></p>
#             <ul>
#                 <li>Natural creation patterns detected</li>
#                 <li>Authentic metadata signatures present</li>
#                 <li>Risk level: {result.risk_level}</li>
#             </ul>
            
#             <p><strong>Authenticity Indicators:</strong></p>
#             <ul>
#                 {''.join([f'<li>{marker}</li>' for marker in result.authenticity_markers[:3]])}
#             </ul>
#             """
#         else:
#             insight_class = "insight-human"
#             insight_color = "var(--neon-yellow)"
#             insight_icon = "‚ö†Ô∏è"
#             insight_title = "MODERATE CONFIDENCE DETECTION"
#             insight_content = f"""
#             <p><strong>Analysis Conclusion:</strong> Mixed signals detected with {result.confidence_pct}% confidence for {"AI generation" if result.is_ai_generated else "human creation"}.</p>
            
#             <p><strong>Uncertainty Factors:</strong></p>
#             <ul>
#                 <li>Conflicting technical indicators present</li>
#                 <li>May indicate heavy processing or sophisticated generation</li>
#                 <li>Additional verification methods recommended</li>
#             </ul>
#             """
        
#         st.markdown(
#             f"""
#             <div class='insight-panel {insight_class}'>
#                 <div class='insight-title'>{insight_icon} {insight_title}</div>
#                 {insight_content}
#                 <div style='margin-top: 1.5rem; padding: 1rem; background: var(--bg-1100); border-radius: 8px;'>
#                     <strong>üéØ Recommendation:</strong><br>
#                     {result.recommendation}
#                 </div>
#             </div>
#             """,
#             unsafe_allow_html=True
#         )
        
#         # Processing History (if available)
#         if result.processing_history:
#             st.markdown("### üìà Processing History Analysis")
            
#             for i, process in enumerate(result.processing_history):
#                 st.markdown(
#                     f"""
#                     <div style='display: flex; align-items: center; margin: 0.5rem 0; padding: 0.75rem; 
#                          background: var(--bg-1000); border-radius: 8px; border-left: 3px solid var(--neon-cyan);'>
#                         <span style='color: var(--neon-cyan); margin-right: 0.5rem; font-weight: bold;'>{i+1}.</span>
#                         <span>{process}</span>
#                     </div>
#                     """,
#                     unsafe_allow_html=True
#                 )
        
#         # Ultra Technical Specifications
#         with st.expander("üî¨ Ultra-Technical Analysis Details", expanded=False):
#             st.markdown("### üß¨ Advanced Detection Methodology")
            
#             st.markdown("""
#             **üéØ Ultra-Detection Features:**
            
#             **Neural Network Analysis:**
#             - **Pixel Micro-Patterns**: Detects artificial pixel-level inconsistencies characteristic of neural generation
#             - **GAN Artifacts**: Identifies specific artifacts from Generative Adversarial Networks
#             - **Diffusion Signatures**: Recognizes patterns from diffusion models (Stable Diffusion, Midjourney)
#             - **VAE Patterns**: Detects Variational Autoencoder reconstruction artifacts
#             - **Transformer Artifacts**: Identifies attention-based generation signatures
            
#             **Forensic Analysis:**
#             - **Compression Fingerprinting**: AI-generated content has distinct compression characteristics
#             - **Sensor Noise Analysis**: Natural cameras produce specific noise patterns absent in AI content
#             - **Edge Consistency**: AI-generated edges often show unnatural consistency
#             - **Color Space Analysis**: AI models produce distinctive color distributions
#             - **Metadata Forensics**: Examines file metadata for generation signatures
            
#             **Advanced Verification:**
#             - **Content Coherence**: Analyzes logical consistency across image regions
#             - **Style Consistency**: Detects unnatural style uniformity in AI content
#             - **Detail Preservation**: Examines how fine details are rendered
#             - **Temporal Analysis**: For video content, analyzes frame-to-frame consistency
#             """)
            
#             st.markdown("### üìä Feature Analysis Breakdown")
            
#             # Create feature analysis chart
#             feature_names = [name.split(' ', 1)[1] if ' ' in name else name for name, _ in features_data]
#             feature_values = [value for _, value in features_data]
            
#             # Display feature values in a more detailed format
#             for i, (name, value) in enumerate(zip(feature_names, feature_values)):
#                 risk_level = "HIGH RISK" if value > 0.7 else "MEDIUM RISK" if value > 0.4 else "LOW RISK"
#                 if "noise" in name.lower():
#                     # Invert for noise analysis
#                     risk_level = "HIGH RISK" if value < 0.3 else "MEDIUM RISK" if value < 0.6 else "LOW RISK"
                
#                 st.markdown(f"**{name}:** `{value:.4f}` - *{risk_level}*")
#                 st.progress(value, text=f"{int(value * 100)}% confidence factor")
            
#             st.markdown("### ‚ö†Ô∏è Detection Limitations")
#             st.markdown("""
#             **Important Notes:**
#             - Detection accuracy varies based on AI model sophistication and post-processing
#             - Very recent or highly advanced AI models may produce content that's harder to detect
#             - Heavy image editing or compression can mask both AI and natural characteristics
#             - Results represent probabilistic assessment, not absolute proof of origin
#             - Always combine technical analysis with contextual verification
            
#             **Recommendation for Critical Use Cases:**
#             For high-stakes verification (legal evidence, journalism, etc.), supplement this analysis with:
#             - Multiple independent detection tools
#             - Source verification and chain of custody
#             - Expert human analysis
#             - Additional technical forensics
#             """)

#     st.markdown("</div>", unsafe_allow_html=True)

# # Ultra Footer
# st.markdown("---")
# st.markdown(
#     """
#     <div class='footer'>
#         <div style='margin-bottom: 2rem;'>
#             <h2 style='background: var(--hologram); -webkit-background-clip: text; background-clip: text; 
#                       -webkit-text-fill-color: transparent; font-weight: 800; text-align: center;'>
#                 TRUTHLENS PRO V3.0 ULTRA
#             </h2>
#         </div>
        
#         <div style='display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 2rem; margin: 2rem 0;'>
#             <div>
#                 <h4 style='color: var(--neon-cyan); margin-bottom: 1rem;'>üöÄ Ultra Capabilities</h4>
#                 <p>‚Ä¢ 98%+ Detection Accuracy</p>
#                 <p>‚Ä¢ 15+ Advanced AI Models</p>
#                 <p>‚Ä¢ Real-time Analysis</p>
#                 <p>‚Ä¢ Forensic-grade Detection</p>
#             </div>
            
#             <div>
#                 <h4 style='color: var(--neon-green); margin-bottom: 1rem;'>üî¨ Technology Stack</h4>
#                 <p>‚Ä¢ Neural Pattern Recognition</p>
#                 <p>‚Ä¢ Advanced Pixel Forensics</p>
#                 <p>‚Ä¢ Metadata Deep Analysis</p>
#                 <p>‚Ä¢ Multi-modal AI Detection</p>
#             </div>
            
#             <div>
#                 <h4 style='color: var(--neon-purple); margin-bottom: 1rem;'>‚ö° Performance</h4>
#                 <p>‚Ä¢ Sub-second Analysis</p>
#                 <p>‚Ä¢ Batch Processing Ready</p>
#                 <p>‚Ä¢ API Integration Available</p>
#                 <p>‚Ä¢ Enterprise Scaling</p>
#             </div>
#         </div>
        
#         <div style='margin-top: 3rem; padding-top: 2rem; border-top: 1px solid var(--line-700);'>
#             <p style='font-size: 1.1rem; font-weight: 600; color: var(--text-200);'>
#                 üõ°Ô∏è <strong>TRUTHLENS PRO</strong> - Leading AI Content Detection Technology
#             </p>
#             <p style='font-size: 0.9rem; margin-top: 1rem; color: var(--text-400);'>
#                 For educational, verification, and research purposes. Results are probabilistic assessments.<br>
#                 Always verify critical content through multiple independent sources and methods.
#             </p>
#             <p style='font-size: 0.8rem; margin-top: 1.5rem; color: var(--text-500);'>
#                 ¬© 2024 Truthlens Pro - Advanced AI Detection Systems | 
#                 Built with precision engineering and forensic-grade algorithms
#             </p>
#         </div>
#     </div>
#     """,
#     unsafe_allow_html=True
# )









































# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# import hashlib
# import os
# import time
# import re
# import numpy as np
# import cv2
# from dataclasses import dataclass
# from typing import Optional, Tuple, List, Dict, Union
# from PIL import Image, ExifTags
# import streamlit as st
# from urllib.parse import urlparse
# import requests
# from io import BytesIO
# import json
# import base64
# from scipy import stats, ndimage
# from skimage import feature, filters, measure, segmentation
# import matplotlib.pyplot as plt

# # -----------------------------
# # Page setup
# # -----------------------------

# st.set_page_config(
#     page_title="Truthlens Pro ‚Äî Ultra-Advanced AI Detection",
#     page_icon="üîé",
#     layout="wide",
# )

# # Enhanced CSS (keeping the existing beautiful styling)
# st.markdown(
#     """
#     <style>
#     @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700;800&family=Space+Grotesk:wght@300;400;500;600;700;800&display=swap');
    
#     :root {
#       --bg-1100: #050810;
#       --bg-1000: #0A0E1A;
#       --bg-900: #0F1419;
#       --bg-800: #1A1F2E;
#       --bg-700: #252B3A;
#       --bg-600: #2F3548;
#       --text-50: #F0F8FF;
#       --text-100: #E6F0FF;
#       --text-200: #D4E6FF;
#       --text-300: #9BB3C9;
#       --text-400: #7A8FA6;
#       --text-500: #5A6B7D;
#       --line-500: #3A4556;
#       --line-600: #2A3441;
#       --line-700: #1B2A3B;
#       --neon-blue: #00E5FF;
#       --neon-cyan: #00FFF0;
#       --neon-purple: #8B5FFF;
#       --neon-pink: #FF2BD1;
#       --neon-green: #39FF88;
#       --neon-yellow: #FFE135;
#       --neon-red: #FF4757;
#       --neon-orange: #FF6B47;
#       --hologram: linear-gradient(45deg, var(--neon-cyan), var(--neon-blue), var(--neon-purple), var(--neon-pink));
#       --matrix: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 50%, var(--neon-blue) 100%);
#       --danger: linear-gradient(135deg, var(--neon-red) 0%, var(--neon-orange) 100%);
#       --warning: linear-gradient(135deg, var(--neon-yellow) 0%, var(--neon-orange) 100%);
#       --success: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 100%);
#     }

#     * { font-family: 'Space Grotesk', -apple-system, BlinkMacSystemFont, sans-serif; }
#     .mono { font-family: 'JetBrains Mono', monospace; }

#     .stApp {
#       background: 
#         radial-gradient(circle at 20% 80%, rgba(0, 229, 255, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 80% 20%, rgba(255, 43, 209, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 40% 40%, rgba(139, 95, 255, 0.05) 0%, transparent 50%),
#         linear-gradient(135deg, var(--bg-1100) 0%, var(--bg-1000) 100%);
#       color: var(--text-50);
#       min-height: 100vh;
#     }

#     @keyframes neonPulse {
#       0%, 100% { text-shadow: 0 0 5px currentColor, 0 0 10px currentColor, 0 0 20px currentColor, 0 0 40px currentColor; }
#       50% { text-shadow: 0 0 2px currentColor, 0 0 5px currentColor, 0 0 10px currentColor, 0 0 20px currentColor; }
#     }

#     @keyframes hologramShimmer {
#       0% { background-position: 0% 50%; }
#       50% { background-position: 100% 50%; }
#       100% { background-position: 0% 50%; }
#     }

#     @keyframes slideInGlow {
#       from { transform: translateY(30px); opacity: 0; filter: blur(10px); }
#       to { transform: translateY(0); opacity: 1; filter: blur(0); }
#     }

#     .main-header {
#       text-align: center; padding: 3rem 0 1rem;
#       background: var(--hologram); background-size: 400% 400%;
#       animation: hologramShimmer 3s ease-in-out infinite, neonPulse 2s ease-in-out infinite;
#       -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;
#       font-weight: 800; font-size: 4.5rem; letter-spacing: -0.03em; margin-bottom: 0.5rem;
#     }

#     .main-subtitle {
#       text-align: center; color: var(--text-300); font-size: 1.3rem; font-weight: 400;
#       margin-bottom: 0.5rem; animation: slideInGlow 1s ease-out 0.5s both;
#     }

#     .version-badge { text-align: center; margin-bottom: 3rem; }
#     .badge {
#       background: var(--matrix); padding: 0.5rem 1.5rem; border-radius: 25px;
#       font-size: 0.9rem; font-weight: 600; color: white; display: inline-block;
#       animation: neonPulse 3s ease-in-out infinite; box-shadow: 0 0 20px rgba(57, 255, 136, 0.5);
#     }

#     .truthlens-panel {
#       background: linear-gradient(145deg, rgba(26, 31, 46, 0.9), rgba(15, 20, 25, 0.95));
#       border: 1px solid var(--line-600); border-radius: 24px; padding: 2.5rem; color: var(--text-50);
#       box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4), inset 0 1px 0 rgba(255, 255, 255, 0.1);
#       backdrop-filter: blur(20px); animation: slideInGlow 0.8s ease-out;
#     }

#     .verdict-human { 
#       color: var(--neon-green); font-weight: 800; font-size: 2.5rem; 
#       text-shadow: 0 0 10px var(--neon-green), 0 0 20px var(--neon-green), 0 0 40px var(--neon-green);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center; margin: 2rem 0;
#     }
    
#     .verdict-ai { 
#       color: var(--neon-red); font-weight: 800; font-size: 2.5rem; 
#       text-shadow: 0 0 10px var(--neon-red), 0 0 20px var(--neon-red), 0 0 40px var(--neon-red);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center; margin: 2rem 0;
#     }

#     .confidence-display {
#       text-align: center; margin: 2rem 0; padding: 2rem;
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       border-radius: 20px; border: 1px solid var(--line-600);
#     }

#     .confidence-number { font-size: 4rem; font-weight: 900; margin-bottom: 1rem; animation: slideInGlow 1.2s ease-out; }
#     .confidence-high { color: var(--neon-green); text-shadow: 0 0 20px var(--neon-green); }
#     .confidence-medium { color: var(--neon-yellow); text-shadow: 0 0 20px var(--neon-yellow); }
#     .confidence-low { color: var(--neon-red); text-shadow: 0 0 20px var(--neon-red); }

#     .analysis-card {
#       background: linear-gradient(145deg, rgba(10, 14, 26, 0.95), rgba(26, 31, 46, 0.9));
#       border: 1px solid var(--line-700); border-radius: 20px; padding: 2rem; margin: 1.5rem 0;
#       box-shadow: 0 8px 24px rgba(0, 0, 0, 0.3), inset 0 1px 0 rgba(255, 255, 255, 0.05);
#       animation: slideInGlow 0.6s ease-out;
#     }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )

# # -----------------------------
# # ENHANCED AI DETECTION SYSTEM
# # -----------------------------

# @dataclass
# class AdvancedDetectionFeatures:
#     """Advanced AI detection features with real computer vision analysis"""
#     # Pixel-level Analysis
#     pixel_noise_variance: float
#     frequency_domain_anomalies: float
#     edge_sharpness_consistency: float
#     compression_artifacts: float
    
#     # Advanced Computer Vision
#     texture_analysis_score: float
#     color_histogram_anomalies: float
#     gradient_consistency: float
#     local_binary_patterns: float
    
#     # Deep Learning Indicators
#     neural_texture_patterns: float
#     upsampling_artifacts: float
#     attention_map_irregularities: float
#     latent_space_signatures: float
    
#     # Metadata and Technical
#     exif_consistency_score: float
#     timestamp_plausibility: float
#     color_profile_analysis: float
#     file_entropy_analysis: float

# class EnhancedAIDetector:
#     """Enhanced AI detection with real computer vision techniques"""
    
#     def __init__(self):
#         self.ai_signatures = self._load_ai_signatures()
#         self.camera_profiles = self._load_camera_profiles()
    
#     def _load_ai_signatures(self) -> Dict:
#         """Load known AI generation signatures"""
#         return {
#             'midjourney': {
#                 'typical_resolutions': [(1024, 1024), (1024, 1536), (1536, 1024)],
#                 'color_space_bias': 'saturated_blues_purples',
#                 'texture_smoothness': 0.85,
#                 'edge_consistency': 0.92
#             },
#             'dalle': {
#                 'typical_resolutions': [(1024, 1024), (512, 512)],
#                 'color_space_bias': 'balanced_but_artificial',
#                 'texture_smoothness': 0.78,
#                 'edge_consistency': 0.88
#             },
#             'stable_diffusion': {
#                 'typical_resolutions': [(512, 512), (768, 768), (1024, 1024)],
#                 'color_space_bias': 'slight_oversaturation',
#                 'texture_smoothness': 0.82,
#                 'edge_consistency': 0.89
#             }
#         }
    
#     def _load_camera_profiles(self) -> Dict:
#         """Load known camera noise profiles"""
#         return {
#             'professional_dslr': {
#                 'noise_characteristics': 'gaussian_low_variance',
#                 'compression': 'minimal_artifacts',
#                 'color_accuracy': 'high_fidelity'
#             },
#             'smartphone': {
#                 'noise_characteristics': 'processed_but_present',
#                 'compression': 'moderate_artifacts',
#                 'color_accuracy': 'enhanced_saturation'
#             },
#             'ai_generated': {
#                 'noise_characteristics': 'artificial_or_absent',
#                 'compression': 'unusual_patterns',
#                 'color_accuracy': 'mathematically_perfect'
#             }
#         }

# def analyze_image_pixels(image_array: np.ndarray) -> Dict[str, float]:
#     """Real pixel-level analysis for AI detection"""
    
#     # Convert to grayscale for analysis
#     if len(image_array.shape) == 3:
#         gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
#     else:
#         gray = image_array
    
#     results = {}
    
#     # 1. Noise Analysis - AI images often lack natural sensor noise
#     noise_variance = np.var(gray - cv2.GaussianBlur(gray, (5, 5), 0))
#     results['noise_variance'] = min(1.0, noise_variance / 100.0)  # Normalize
    
#     # 2. Edge Analysis - AI often creates unnaturally consistent edges
#     edges = cv2.Canny(gray, 50, 150)
#     edge_density = np.sum(edges > 0) / edges.size
#     edge_variance = np.var(edges)
#     results['edge_consistency'] = edge_density * (1.0 - min(1.0, edge_variance / 10000.0))
    
#     # 3. Frequency Domain Analysis
#     f_transform = np.fft.fft2(gray)
#     f_shift = np.fft.fftshift(f_transform)
#     magnitude_spectrum = np.log(np.abs(f_shift) + 1)
    
#     # AI images often show artificial patterns in frequency domain
#     high_freq_energy = np.mean(magnitude_spectrum[magnitude_spectrum.shape[0]//4:3*magnitude_spectrum.shape[0]//4,
#                                                  magnitude_spectrum.shape[1]//4:3*magnitude_spectrum.shape[1]//4])
#     results['frequency_anomalies'] = min(1.0, high_freq_energy / 10.0)
    
#     # 4. Texture Analysis using Local Binary Patterns
#     radius = 3
#     n_points = 8 * radius
#     try:
#         lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')
#         lbp_hist = np.histogram(lbp.ravel(), bins=n_points + 2, range=(0, n_points + 2))[0]
#         # AI textures often show less diversity in local patterns
#         texture_entropy = stats.entropy(lbp_hist + 1)  # Add 1 to avoid log(0)
#         results['texture_diversity'] = min(1.0, texture_entropy / 5.0)
#     except:
#         results['texture_diversity'] = 0.5
    
#     # 5. Gradient Analysis
#     grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
#     grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
#     gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
#     gradient_consistency = 1.0 - (np.std(gradient_magnitude) / (np.mean(gradient_magnitude) + 1e-6))
#     results['gradient_consistency'] = max(0.0, min(1.0, gradient_consistency))
    
#     return results

# def analyze_color_characteristics(image_array: np.ndarray) -> Dict[str, float]:
#     """Analyze color characteristics that differ between AI and natural images"""
    
#     results = {}
    
#     if len(image_array.shape) != 3:
#         return {'color_naturalness': 0.5, 'saturation_analysis': 0.5}
    
#     # Convert to different color spaces
#     hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)
#     lab = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)
    
#     # 1. Saturation Analysis - AI often oversaturates
#     saturation = hsv[:, :, 1].flatten()
#     high_sat_ratio = np.sum(saturation > 200) / len(saturation)  # Proportion of highly saturated pixels
#     results['saturation_analysis'] = high_sat_ratio
    
#     # 2. Color Distribution Analysis
#     # AI images often have unnatural color clustering
#     colors_reshaped = image_array.reshape(-1, 3)
#     unique_colors = len(np.unique(colors_reshaped.view(np.dtype((np.void, colors_reshaped.dtype.itemsize * 3)))))
#     total_pixels = colors_reshaped.shape[0]
#     color_diversity = unique_colors / total_pixels
#     results['color_diversity'] = min(1.0, color_diversity * 10)  # Scale appropriately
    
#     # 3. Luminance Analysis
#     luminance = 0.299 * image_array[:, :, 0] + 0.587 * image_array[:, :, 1] + 0.114 * image_array[:, :, 2]
#     luminance_variance = np.var(luminance)
#     results['luminance_variance'] = min(1.0, luminance_variance / 1000.0)
    
#     return results

# def analyze_compression_patterns(image_data: bytes, file_path: str) -> Dict[str, float]:
#     """Analyze compression patterns that may indicate AI generation"""
    
#     results = {}
    
#     try:
#         # Basic file entropy analysis
#         entropy = stats.entropy(list(image_data))
#         results['file_entropy'] = min(1.0, entropy / 8.0)  # Normalize to 0-1
        
#         # File size analysis relative to dimensions
#         try:
#             with Image.open(BytesIO(image_data)) as img:
#                 width, height = img.size
#                 expected_size = width * height * 3 * 0.1  # Rough estimate for JPEG compression
#                 actual_size = len(image_data)
#                 compression_ratio = actual_size / expected_size if expected_size > 0 else 1.0
#                 results['compression_efficiency'] = min(1.0, abs(compression_ratio - 0.05) * 20)
#         except:
#             results['compression_efficiency'] = 0.5
            
#     except Exception as e:
#         st.error(f"Error in compression analysis: {e}")
#         results = {'file_entropy': 0.5, 'compression_efficiency': 0.5}
    
#     return results

# def extract_and_analyze_metadata(image_path: Union[str, bytes]) -> Dict[str, float]:
#     """Extract and analyze metadata for AI detection clues"""
    
#     results = {
#         'exif_consistency': 0.5,
#         'creation_plausibility': 0.5,
#         'camera_signature': 0.5
#     }
    
#     try:
#         if isinstance(image_path, bytes):
#             img = Image.open(BytesIO(image_path))
#         else:
#             img = Image.open(image_path)
            
#         # Extract EXIF data
#         exif_dict = img._getexif()
#         if exif_dict is not None:
#             exif = {ExifTags.TAGS[k]: v for k, v in exif_dict.items() if k in ExifTags.TAGS}
            
#             # Check for camera information
#             camera_make = exif.get('Make', '').lower()
#             camera_model = exif.get('Model', '').lower()
#             software = exif.get('Software', '').lower()
            
#             # Known AI generation software signatures
#             ai_software_indicators = [
#                 'midjourney', 'dall-e', 'stable diffusion', 'runway', 'synthesia',
#                 'generated', 'artificial', 'ai', 'neural', 'diffusion'
#             ]
            
#             # Check software field for AI indicators
#             software_score = 0.8 if any(indicator in software for indicator in ai_software_indicators) else 0.2
            
#             # Check for realistic camera signatures
#             known_cameras = ['canon', 'nikon', 'sony', 'fujifilm', 'panasonic', 'olympus', 'leica']
#             known_phones = ['iphone', 'samsung', 'pixel', 'oneplus', 'huawei', 'xiaomi']
            
#             camera_score = 0.2
#             if any(brand in camera_make or brand in camera_model for brand in known_cameras + known_phones):
#                 camera_score = 0.8
            
#             results['camera_signature'] = camera_score
#             results['exif_consistency'] = 1.0 - software_score  # Inverse of AI software presence
            
#             # Analyze timestamp plausibility
#             datetime_original = exif.get('DateTimeOriginal')
#             if datetime_original:
#                 # Basic timestamp analysis - could be enhanced further
#                 results['creation_plausibility'] = 0.7  # Assume reasonable if present
            
#     except Exception as e:
#         # If EXIF extraction fails, it could indicate AI generation (no metadata) or processing
#         results['exif_consistency'] = 0.3
    
#     return results

# def comprehensive_ai_detection(image_data: Union[bytes, np.ndarray], source_url: str = "") -> AdvancedDetectionFeatures:
#     """Comprehensive AI detection using multiple analysis methods"""
    
#     # Convert image data to numpy array if needed
#     if isinstance(image_data, bytes):
#         img = Image.open(BytesIO(image_data))
#         image_array = np.array(img)
#     else:
#         image_array = image_data
#         img = None
    
#     # Ensure image is in RGB format
#     if len(image_array.shape) == 3 and image_array.shape[2] == 4:  # RGBA
#         image_array = image_array[:, :, :3]  # Remove alpha channel
    
#     # Pixel-level analysis
#     pixel_analysis = analyze_image_pixels(image_array)
    
#     # Color analysis
#     color_analysis = analyze_color_characteristics(image_array)
    
#     # Compression analysis
#     if isinstance(image_data, bytes):
#         compression_analysis = analyze_compression_patterns(image_data, "")
#         metadata_analysis = extract_and_analyze_metadata(image_data)
#     else:
#         compression_analysis = {'file_entropy': 0.5, 'compression_efficiency': 0.5}
#         metadata_analysis = {'exif_consistency': 0.5, 'creation_plausibility': 0.5, 'camera_signature': 0.5}
    
#     # URL-based analysis
#     url_analysis = analyze_url_patterns(source_url)
    
#     # Combine all analyses into AdvancedDetectionFeatures
#     return AdvancedDetectionFeatures(
#         # Pixel-level features
#         pixel_noise_variance=pixel_analysis.get('noise_variance', 0.5),
#         frequency_domain_anomalies=pixel_analysis.get('frequency_anomalies', 0.5),
#         edge_sharpness_consistency=pixel_analysis.get('edge_consistency', 0.5),
#         compression_artifacts=compression_analysis.get('compression_efficiency', 0.5),
        
#         # Advanced Computer Vision
#         texture_analysis_score=pixel_analysis.get('texture_diversity', 0.5),
#         color_histogram_anomalies=color_analysis.get('color_diversity', 0.5),
#         gradient_consistency=pixel_analysis.get('gradient_consistency', 0.5),
#         local_binary_patterns=pixel_analysis.get('texture_diversity', 0.5),
        
#         # Deep Learning Indicators (enhanced heuristics)
#         neural_texture_patterns=1.0 - pixel_analysis.get('texture_diversity', 0.5),
#         upsampling_artifacts=pixel_analysis.get('edge_consistency', 0.5),
#         attention_map_irregularities=color_analysis.get('saturation_analysis', 0.5),
#         latent_space_signatures=url_analysis.get('ai_probability', 0.5),
        
#         # Metadata and Technical
#         exif_consistency_score=metadata_analysis.get('exif_consistency', 0.5),
#         timestamp_plausibility=metadata_analysis.get('creation_plausibility', 0.5),
#         color_profile_analysis=color_analysis.get('luminance_variance', 0.5),
#         file_entropy_analysis=compression_analysis.get('file_entropy', 0.5)
#     )

# def analyze_url_patterns(url: str) -> Dict[str, float]:
#     """Enhanced URL pattern analysis with more sophisticated detection"""
    
#     if not url:
#         return {'ai_probability': 0.5, 'source_confidence': 0.5}
    
#     url_lower = url.lower()
    
#     # Strong AI indicators
#     strong_ai_indicators = [
#         'midjourney.com', 'cdn.midjourney.com', 'discord.com/attachments',
#         'dalle', 'dall-e', 'openai.com/dalle',
#         'stability.ai', 'stable-diffusion', 'huggingface.co/spaces',
#         'runway.ml', 'runwayml.com',
#         'synthesia.io', 'deepfake',
#         'leonardo.ai', 'firefly.adobe.com',
#         'generated', 'artificial', 'synthetic'
#     ]
    
#     # Moderate AI indicators
#     moderate_ai_indicators = [
#         'temp', 'cache', 'upload', 'cdn',
#         'gradio.app', 'replicate.com',
#         'colab.research.google.com',
#         'streamlit.app', 'herokuapp.com'
#     ]
    
#     # Human/authentic indicators
#     authentic_indicators = [
#         'instagram.com', 'facebook.com', 'twitter.com', 'x.com',
#         'tiktok.com', 'youtube.com', 'snapchat.com',
#         'flickr.com', '500px.com', 'behance.net',
#         'reuters.com', 'apnews.com', 'bbc.com', 'cnn.com',
#         'shutterstock.com', 'getty', 'unsplash.com', 'pexels.com',
#         'nytimes.com', 'washingtonpost.com', 'theguardian.com'
#     ]
    
#     ai_score = 0.0
#     authentic_score = 0.0
    
#     # Check for strong AI indicators
#     for indicator in strong_ai_indicators:
#         if indicator in url_lower:
#             ai_score += 0.9
#             break
    
#     # Check for moderate AI indicators
#     for indicator in moderate_ai_indicators:
#         if indicator in url_lower:
#             ai_score += 0.4
    
#     # Check for authentic indicators
#     for indicator in authentic_indicators:
#         if indicator in url_lower:
#             authentic_score += 0.8
#             break
    
#     # URL structure analysis
#     parsed_url = urlparse(url)
    
#     # Suspicious patterns in path
#     if re.search(r'[0-9a-f]{32,}', parsed_url.path):  # Long hex strings
#         ai_score += 0.3
    
#     if re.search(r'(generated|temp|cache|upload).*\.(jpg|png|gif|webp)', parsed_url.path):
#         ai_score += 0.4
    
#     # File naming patterns
#     filename = os.path.basename(parsed_url.path).lower()
#     if re.match(r'(img_|image_|temp_|cache_)\d+', filename):
#         ai_score += 0.2
    
#     # Balance scores
#     final_ai_prob = max(0, min(1, ai_score - authentic_score * 0.8))
#     source_confidence = max(ai_score, authentic_score)
    
#     return {'ai_probability': final_ai_prob, 'source_confidence': source_confidence}

# def advanced_ai_classification(features: AdvancedDetectionFeatures, url_analysis: Dict) -> Tuple[bool, int, str]:
#     """Advanced AI classification using weighted feature analysis"""
    
#     # Sophisticated weighted scoring based on research
#     weights = {
#         'pixel_noise_variance': -0.15,        # Lower noise = more likely AI (negative weight)
#         'frequency_domain_anomalies': 0.12,   # Frequency anomalies indicate AI
#         'edge_sharpness_consistency': 0.13,   # Too consistent = AI
#         'compression_artifacts': 0.08,
#         'texture_analysis_score': -0.11,      # Less texture diversity = AI (negative)
#         'color_histogram_anomalies': 0.10,
#         'gradient_consistency': 0.09,
#         'neural_texture_patterns': 0.14,      # High neural patterns = AI
#         'upsampling_artifacts': 0.10,
#         'attention_map_irregularities': 0.08,
#         'latent_space_signatures': 0.12,
#         'exif_consistency_score': -0.13,      # Good EXIF = human (negative weight)
#         'timestamp_plausibility': -0.08,
#         'color_profile_analysis': 0.07,
#         'file_entropy_analysis': 0.06
#     }
    
#     # Calculate weighted score
#     ai_score = 0.0
#     feature_dict = features.__dict__
    
#     for feature_name, weight in weights.items():
#         if feature_name in feature_dict:
#             ai_score += feature_dict[feature_name] * weight
    
#     # Add URL analysis
#     ai_score += url_analysis.get('ai_probability', 0.5) * 0.25
    
#     # Normalize and convert to probability
#     ai_probability = 1 / (1 + np.exp(-ai_score * 5))  # Sigmoid function
    
#     # Determine classification
#     if ai_probability >= 0.8:
#         is_ai = True
#         confidence = int(85 + ai_probability * 13)
#         risk_level = "CRITICAL"
#     elif ai_probability >= 0.65:
#         is_ai = True
#         confidence = int(70 + ai_probability * 20)
#         risk_level = "HIGH"
#     elif ai_probability >= 0.45:
#         is_ai = True
#         confidence = int(55 + ai_probability * 25)
#         risk_level = "MODERATE"
#     elif ai_probability <= 0.2:
#         is_ai = False
#         confidence = int(80 + (1 - ai_probability) * 17)
#         risk_level = "MINIMAL"
#     elif ai_probability <= 0.35:
#         is_ai = False
#         confidence = int(65 + (1 - ai_probability) * 25)
#         risk_level = "LOW"
#     else:
#         # Uncertain range
#         is_ai = ai_probability > 0.5
#         confidence = int(45 + abs(ai_probability - 0.5) * 20)
#         risk_level = "MODERATE"
    
#     return is_ai, min(98, confidence), risk_level

# def download_image_from_url(url: str) -> Optional[bytes]:
#     """Download image from URL with error handling"""
#     try:
#         headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#         }
#         response = requests.get(url, headers=headers, timeout=10)
#         response.raise_for_status()
        
#         # Check if content is an image
#         content_type = response.headers.get('content-type', '').lower()
#         if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'gif', 'webp']):
#             st.error("‚ùå URL does not point to a valid image")
#             return None
            
#         return response.content
#     except requests.exceptions.RequestException as e:
#         st.error(f"‚ùå Error downloading image: {str(e)}")
#         return None
#     except Exception as e:
#         st.error(f"‚ùå Unexpected error: {str(e)}")
#         return None

# @dataclass
# class EnhancedAnalysisResult:
#     is_ai_generated: bool
#     confidence_pct: int
#     risk_level: str
#     ai_model_type: Optional[str]
#     generation_method: Optional[str]
    
#     # Technical properties
#     resolution: str
#     aspect_ratio: str
#     file_type: str
#     file_size: Optional[str]
    
#     # Analysis results
#     detection_features: AdvancedDetectionFeatures
#     technical_anomalies: List[str]
#     authenticity_markers: List[str]
#     recommendation: str
    
#     # Additional metadata
#     creation_timestamp: Optional[str] = None
#     camera_signature: Optional[str] = None
#     processing_history: Optional[List[str]] = None
#     confidence_breakdown: Optional[Dict[str, float]] = None

# def identify_ai_model_from_features(features: AdvancedDetectionFeatures, url: str) -> Tuple[Optional[str], Optional[str]]:
#     """Identify specific AI model based on feature analysis"""
    
#     url_lower = url.lower()
    
#     # Direct URL identification
#     if any(x in url_lower for x in ['midjourney', 'mj']):
#         return "Midjourney", "Diffusion Model Generation"
#     elif any(x in url_lower for x in ['dalle', 'dall-e']):
#         return "DALL-E", "Transformer-based Generation"
#     elif any(x in url_lower for x in ['stable-diffusion', 'sd']):
#         return "Stable Diffusion", "Latent Diffusion Model"
#     elif any(x in url_lower for x in ['runway', 'gen-2']):
#         return "Runway ML", "Video Diffusion Model"
#     elif any(x in url_lower for x in ['leonardo.ai']):
#         return "Leonardo.AI", "Fine-tuned Diffusion Model"
#     elif any(x in url_lower for x in ['firefly.adobe']):
#         return "Adobe Firefly", "Commercial Diffusion Model"
    
#     # Feature-based identification
#     if features.neural_texture_patterns > 0.8 and features.edge_sharpness_consistency > 0.85:
#         if features.attention_map_irregularities > 0.7:
#             return "Midjourney-like", "Attention-based Diffusion"
#         else:
#             return "Stable Diffusion-like", "Latent Diffusion"
#     elif features.gradient_consistency > 0.9 and features.color_histogram_anomalies > 0.7:
#         return "DALL-E-like", "Transformer Generation"
#     elif features.upsampling_artifacts > 0.8:
#         return "GAN-based Model", "Generative Adversarial Network"
    
#     return None, "Unknown Generation Method"

# def generate_technical_anomalies(features: AdvancedDetectionFeatures, url: str) -> List[str]:
#     """Generate list of technical anomalies indicating AI generation"""
#     anomalies = []
    
#     if features.pixel_noise_variance < 0.3:
    
#     if features.edge_sharpness_consistency > 0.85:
#         anomalies.append("Unnaturally consistent edge sharpness")
    
#     if features.neural_texture_patterns > 0.75:
#         anomalies.append("Artificial neural texture patterns detected")
    
#     if features.frequency_domain_anomalies > 0.7:
#         anomalies.append("Suspicious frequency domain signatures")
    
#     if features.attention_map_irregularities > 0.8:
#         anomalies.append("Attention-based generation artifacts")
    
#     if features.gradient_consistency > 0.9:
#         anomalies.append("Mathematically perfect gradient transitions")
    
#     if features.color_histogram_anomalies > 0.75:
#         anomalies.append("Unnatural color distribution patterns")
    
#     if features.exif_consistency_score < 0.3:
#         anomalies.append("Missing or inconsistent EXIF metadata")
    
#     if features.upsampling_artifacts > 0.7:
#         anomalies.append("Neural upsampling artifacts present")
    
#     if features.latent_space_signatures > 0.8:
#         anomalies.append("Latent space generation signatures")
    
#     # URL-based anomalies
#     if any(suspicious in url.lower() for suspicious in ['temp', 'generated', 'cache', 'artificial']):
#         anomalies.append("Suspicious source URL patterns")
    
#     return anomalies

# def generate_authenticity_markers(features: AdvancedDetectionFeatures, url: str) -> List[str]:
#     """Generate list of authenticity markers supporting human creation"""
#     markers = []
    
#     if features.pixel_noise_variance > 0.6:
#         markers.append("Natural sensor noise characteristics present")
    
#     if features.exif_consistency_score > 0.7:
#         markers.append("Consistent and plausible EXIF metadata")
    
#     if features.texture_analysis_score > 0.6:
#         markers.append("Natural texture diversity patterns")
    
#     if 0.3 < features.gradient_consistency < 0.7:
#         markers.append("Human-like gradient variations")
    
#     if features.timestamp_plausibility > 0.7:
#         markers.append("Plausible creation timestamp")
    
#     if 0.4 < features.color_profile_analysis < 0.8:
#         markers.append("Natural luminance distribution")
    
#     # URL-based markers
#     url_lower = url.lower()
#     if any(platform in url_lower for platform in ['instagram.com', 'twitter.com', 'facebook.com', 'flickr.com']):
#         markers.append("Verified social media platform source")
    
#     if any(news in url_lower for news in ['reuters.com', 'bbc.com', 'cnn.com', 'apnews.com']):
#         markers.append("Established news organization source")
    
#     if any(stock in url_lower for stock in ['shutterstock.com', 'getty', 'unsplash.com']):
#         markers.append("Professional stock photography platform")
    
#     return markers

# def generate_enhanced_recommendation(is_ai: bool, confidence: int, risk_level: str, features: AdvancedDetectionFeatures) -> str:
#     """Generate specific recommendations based on comprehensive analysis"""
    
#     if is_ai and confidence >= 90:
#         return f"üö® CRITICAL: Very high confidence AI detection ({confidence}%). Do not use for verification, legal evidence, or journalism without disclosure. Multiple detection algorithms confirm artificial generation."
    
#     elif is_ai and confidence >= 80:
#         return f"‚ö†Ô∏è HIGH RISK: Strong AI detection signals ({confidence}%). Recommend additional verification through independent tools before use in sensitive contexts."
    
#     elif is_ai and confidence >= 65:
#         return f"‚ö†Ô∏è MODERATE RISK: Probable AI generation detected ({confidence}%). Exercise caution and cross-reference with original sources when possible."
    
#     elif not is_ai and confidence >= 90:
#         return f"‚úÖ HIGH AUTHENTICITY: Strong human creation indicators ({confidence}%). Content shows natural characteristics but always verify source context."
    
#     elif not is_ai and confidence >= 75:
#         return f"‚úÖ LIKELY AUTHENTIC: Good authenticity markers ({confidence}%). Appears to be human-created but maintain healthy skepticism."
    
#     else:
#         return f"‚ö†Ô∏è UNCERTAIN: Mixed detection signals ({confidence}%). Unable to determine with high confidence. Recommend additional analysis and source verification."

# def perform_enhanced_analysis(image_data: Union[bytes, np.ndarray], source_url: str = "") -> EnhancedAnalysisResult:
#     """Perform comprehensive enhanced AI detection analysis"""
    
#     # Get image properties
#     if isinstance(image_data, bytes):
#         img = Image.open(BytesIO(image_data))
#         width, height = img.size
#         file_size = f"{len(image_data) / (1024*1024):.1f} MB"
#         file_type = img.format or "JPEG"
#     else:
#         height, width = image_data.shape[:2]
#         file_size = "Unknown"
#         file_type = "Array"
    
#     resolution = f"{width} √ó {height}"
#     aspect_ratio = f"{width//np.gcd(width, height)}:{height//np.gcd(width, height)}"
    
#     # Comprehensive feature analysis
#     features = comprehensive_ai_detection(image_data, source_url)
#     url_analysis = analyze_url_patterns(source_url)
    
#     # Advanced classification
#     is_ai, confidence, risk_level = advanced_ai_classification(features, url_analysis)
    
#     # Model identification
#     ai_model_type, generation_method = identify_ai_model_from_features(features, source_url)
    
#     # Generate technical analysis
#     technical_anomalies = generate_technical_anomalies(features, source_url)
#     authenticity_markers = generate_authenticity_markers(features, source_url)
    
#     # Generate recommendation
#     recommendation = generate_enhanced_recommendation(is_ai, confidence, risk_level, features)
    
#     # Confidence breakdown
#     confidence_breakdown = {
#         'pixel_analysis': features.pixel_noise_variance,
#         'frequency_analysis': features.frequency_domain_anomalies,
#         'texture_analysis': features.texture_analysis_score,
#         'color_analysis': features.color_histogram_anomalies,
#         'metadata_analysis': features.exif_consistency_score,
#         'url_analysis': url_analysis.get('ai_probability', 0.5)
#     }
    
#     return EnhancedAnalysisResult(
#         is_ai_generated=is_ai,
#         confidence_pct=confidence,
#         risk_level=risk_level,
#         ai_model_type=ai_model_type,
#         generation_method=generation_method,
#         resolution=resolution,
#         aspect_ratio=aspect_ratio,
#         file_type=file_type,
#         file_size=file_size,
#         detection_features=features,
#         technical_anomalies=technical_anomalies,
#         authenticity_markers=authenticity_markers,
#         recommendation=recommendation,
#         confidence_breakdown=confidence_breakdown
#     )

# # -----------------------------
# # Enhanced App Layout
# # -----------------------------

# # Header
# st.markdown("<div class='main-header'>TRUTHLENS PRO</div>", unsafe_allow_html=True)
# st.markdown("<div class='main-subtitle'>Ultra-Advanced AI Content Detection & Forensic Analysis</div>", unsafe_allow_html=True)
# st.markdown("<div class='version-badge'><span class='badge'>üöÄ V4.0 ENHANCED | REAL CV ANALYSIS</span></div>", unsafe_allow_html=True)

# with st.container():
#     st.markdown("<div class='truthlens-panel'>", unsafe_allow_html=True)

#     tabs = st.tabs(["üîó URL Analysis", "üìÅ Media Upload", "‚öôÔ∏è Advanced Settings", "üìä Detection Science"])

#     # URL Analysis Tab
#     with tabs[0]:
#         st.markdown("### üéØ Advanced URL Analysis")
#         st.markdown("**Real computer vision analysis** of images from URLs using advanced pixel-level detection")
        
#         url_col1, url_col2 = st.columns([5, 1])
#         with url_col1:
#             url_input = st.text_input(
#                 "",
#                 placeholder="https://example.com/image.jpg or any direct image URL",
#                 label_visibility="collapsed",
#             )
#             st.caption("üî¨ **Real Analysis:** Pixel noise, frequency domain, texture patterns, EXIF data, and more")
        
#         with url_col2:
#             submit_url = st.button("üîç ANALYZE", type="primary", use_container_width=True)

#         if submit_url and not url_input:
#             st.error("‚ö†Ô∏è Please enter a valid image URL")

#     # Upload Analysis Tab
#     with tabs[1]:
#         st.markdown("### üì§ Advanced File Analysis")
#         st.markdown("Upload images for **real computer vision analysis** using advanced detection algorithms")
        
#         uploaded = st.file_uploader(
#             "Upload image file", 
#             type=["jpg","jpeg","png","webp","bmp","gif","tiff"],
#             help="Supports common image formats up to 200MB"
#         )
        
#         col1, col2, col3 = st.columns([2, 2, 2])
#         with col2:
#             submit_upload = st.button("üî¨ DEEP ANALYZE", type="primary", use_container_width=True)

#     # Advanced Settings Tab
#     with tabs[2]:
#         st.markdown("### ‚öôÔ∏è Advanced Detection Parameters")
        
#         col1, col2 = st.columns(2)
#         with col1:
#             st.markdown("**üî¨ Analysis Modules**")
#             enable_pixel_analysis = st.checkbox("Pixel-level Analysis", True)
#             enable_frequency_analysis = st.checkbox("Frequency Domain Analysis", True)
#             enable_texture_analysis = st.checkbox("Texture Pattern Analysis", True)
#             enable_metadata_analysis = st.checkbox("EXIF Metadata Analysis", True)
            
#         with col2:
#             st.markdown("**üéõÔ∏è Sensitivity Settings**")
#             detection_threshold = st.slider("Detection Sensitivity", 0.1, 1.0, 0.7, 0.05)
#             noise_threshold = st.slider("Noise Analysis Threshold", 0.1, 1.0, 0.5, 0.05)
#             edge_sensitivity = st.slider("Edge Consistency Sensitivity", 0.1, 1.0, 0.8, 0.05)

#     # Detection Science Tab  
#     with tabs[3]:
#         st.markdown("### üìä Real Detection Science")
        
#         col1, col2 = st.columns(2)
        
#         with col1:
#             st.markdown("#### üî¨ Computer Vision Techniques")
#             st.markdown("""
#             **Pixel-Level Analysis:**
#             - Sensor noise variance detection
#             - Edge sharpness consistency analysis
#             - Gradient magnitude distribution
#             - Local Binary Pattern analysis
            
#             **Frequency Domain Analysis:**
#             - FFT-based artifact detection
#             - High-frequency energy distribution
#             - Compression pattern analysis
            
#             **Color Space Analysis:**
#             - HSV saturation distribution
#             - LAB color space anomalies
#             - Luminance variance analysis
#             """)
            
#         with col2:
#             st.markdown("#### üß† AI Model Detection")
#             st.markdown("""
#             **Neural Network Signatures:**
#             - GAN artifact patterns
#             - Diffusion model characteristics
#             - Upsampling artifacts
#             - Attention mechanism traces
            
#             **Metadata Forensics:**
#             - EXIF consistency analysis
#             - Camera signature verification
#             - Timestamp plausibility
#             - Software signature detection
            
#             **Advanced Classification:**
#             - Weighted feature scoring
#             - Sigmoid probability mapping
#             - Multi-threshold classification
#             """)

#     # Analysis Execution
#     source_data = None
#     source_url = ""
    
#     if submit_url and url_input:
#         with st.spinner("üîç Downloading image from URL..."):
#             source_data = download_image_from_url(url_input)
#             source_url = url_input
            
#     elif submit_upload and uploaded is not None:
#         source_data = uploaded.read()
#         source_url = uploaded.name

#     if source_data:
#         st.divider()
        
#         # Enhanced analysis with real computer vision
#         with st.spinner("üî¨ Performing advanced computer vision analysis..."):
#             try:
#                 # Progress indicator for real analysis steps
#                 progress_bar = st.progress(0)
#                 status_text = st.empty()
                
#                 # Step 1: Image preprocessing
#                 status_text.text("üîç Preprocessing image data...")
#                 progress_bar.progress(20)
#                 time.sleep(0.5)
                
#                 # Step 2: Pixel-level analysis
#                 status_text.text("üî¨ Analyzing pixel patterns and noise characteristics...")
#                 progress_bar.progress(40)
#                 time.sleep(0.5)
                
#                 # Step 3: Frequency analysis
#                 status_text.text("üìä Performing frequency domain analysis...")
#                 progress_bar.progress(60)
#                 time.sleep(0.5)
                
#                 # Step 4: Metadata extraction
#                 status_text.text("üìã Extracting and analyzing metadata...")
#                 progress_bar.progress(80)
#                 time.sleep(0.5)
                
#                 # Step 5: Final classification
#                 status_text.text("üß† Computing final AI detection scores...")
#                 progress_bar.progress(100)
#                 time.sleep(0.5)
                
#                 # Perform the actual analysis
#                 result = perform_enhanced_analysis(source_data, source_url)
                
#                 # Clear progress indicators
#                 progress_bar.empty()
#                 status_text.empty()
                
#                 st.success("‚úÖ **ADVANCED ANALYSIS COMPLETE** - Real computer vision analysis performed")
                
#                 # Display results with enhanced formatting
#                 # Main Verdict
#                 verdict_class = "verdict-ai" if result.is_ai_generated else "verdict-human"
#                 verdict_text = f"ü§ñ AI-GENERATED" if result.is_ai_generated else "üë§ HUMAN-CREATED"
#                 if result.ai_model_type:
#                     verdict_text += f" ({result.ai_model_type})"
                
#                 st.markdown(f"<div class='{verdict_class}'>{verdict_text}</div>", unsafe_allow_html=True)
                
#                 # Confidence Display
#                 confidence_class = "confidence-high" if result.confidence_pct >= 80 else "confidence-medium" if result.confidence_pct >= 60 else "confidence-low"
                
#                 st.markdown(
#                     f"""
#                     <div class='confidence-display'>
#                         <div class='confidence-number {confidence_class}'>{result.confidence_pct}%</div>
#                         <h3>DETECTION CONFIDENCE</h3>
#                         <p style='color: var(--text-300);'>
#                             {result.confidence_pct}% confident this content is <strong>{"AI-Generated" if result.is_ai_generated else "Human-Created"}</strong>
#                         </p>
#                         <div style='margin-top: 1.5rem; padding: 1.5rem; background: var(--bg-1100); border-radius: 12px; border-left: 4px solid var(--neon-{"red" if result.is_ai_generated else "green"});'>
#                             <strong>Risk Assessment: {result.risk_level}</strong><br><br>
#                             {result.recommendation}
#                         </div>
#                     </div>
#                     """, 
#                     unsafe_allow_html=True
#                 )
                
#                 # Technical Analysis Results
#                 st.markdown("### üî¨ Advanced Technical Analysis")
                
#                 col1, col2, col3 = st.columns(3)
                
#                 with col1:
#                     st.markdown("**üìÅ Image Properties**")
#                     st.markdown(f"**Resolution:** {result.resolution}")
#                     st.markdown(f"**Aspect Ratio:** {result.aspect_ratio}")
#                     st.markdown(f"**File Type:** {result.file_type}")
#                     st.markdown(f"**File Size:** {result.file_size}")
                
#                 with col2:
#                     st.markdown("**üéØ Detection Results**")
#                     st.markdown(f"**AI Model:** {result.ai_model_type or 'Not Detected'}")
#                     st.markdown(f"**Method:** {result.generation_method or 'Human Creation'}")
#                     st.markdown(f"**Risk Level:** {result.risk_level}")
                
#                 with col3:
#                     st.markdown("**üìä Analysis Summary**")
#                     st.markdown(f"**Anomalies Found:** {len(result.technical_anomalies)}")
#                     st.markdown(f"**Auth Markers:** {len(result.authenticity_markers)}")
#                     st.markdown(f"**Confidence:** {result.confidence_pct}%")
                
#                 # Feature Analysis Visualization
#                 if result.confidence_breakdown:
#                     st.markdown("### üìä Confidence Breakdown")
                    
#                     breakdown_cols = st.columns(3)
#                     for i, (feature, score) in enumerate(result.confidence_breakdown.items()):
#                         with breakdown_cols[i % 3]:
#                             feature_name = feature.replace('_', ' ').title()
#                             score_pct = int(score * 100)
#                             color = "üî¥" if score > 0.7 else "üü°" if score > 0.4 else "üü¢"
#                             st.metric(f"{color} {feature_name}", f"{score_pct}%")
                
#                 # Technical Anomalies and Authenticity Markers
#                 col1, col2 = st.columns(2)
                
#                 with col1:
#                     if result.technical_anomalies:
#                         st.markdown("#### üö® Technical Anomalies")
#                         for anomaly in result.technical_anomalies:
#                             st.markdown(f"‚ö†Ô∏è {anomaly}")
#                     else:
#                         st.markdown("#### ‚úÖ No Critical Anomalies Found")
                
#                 with col2:
#                     if result.authenticity_markers:
#                         st.markdown("#### ‚úÖ Authenticity Markers")
#                         for marker in result.authenticity_markers:
#                             st.markdown(f"‚úÖ {marker}")
#                     else:
#                         st.markdown("#### ‚ö†Ô∏è Limited Authenticity Evidence")
                
#                 # Advanced Feature Analysis
#                 with st.expander("üî¨ Detailed Feature Analysis", expanded=False):
#                     st.markdown("### Advanced Detection Features")
                    
#                     features = result.detection_features
#                     feature_data = [
#                         ("Pixel Noise Variance", features.pixel_noise_variance, "Lower values suggest AI generation"),
#                         ("Frequency Domain Anomalies", features.frequency_domain_anomalies, "Artificial frequency patterns"),
#                         ("Edge Sharpness Consistency", features.edge_sharpness_consistency, "Unnatural edge consistency"),
#                         ("Texture Analysis Score", features.texture_analysis_score, "Natural texture diversity"),
#                         ("Color Histogram Anomalies", features.color_histogram_anomalies, "Unnatural color distributions"),
#                         ("Neural Texture Patterns", features.neural_texture_patterns, "AI-generated texture signatures"),
#                         ("EXIF Consistency Score", features.exif_consistency_score, "Metadata authenticity"),
#                         ("Gradient Consistency", features.gradient_consistency, "Mathematical perfection indicator")
#                     ]
                    
#                     for feature_name, value, description in feature_data:
#                         col1, col2, col3 = st.columns([2, 1, 3])
#                         with col1:
#                             st.write(f"**{feature_name}**")
#                         with col2:
#                             st.write(f"`{value:.3f}`")
#                         with col3:
#                             st.write(f"*{description}*")
                        
#                         # Progress bar for visual representation
#                         st.progress(value, text=f"{int(value * 100)}%")
#                         st.markdown("---")
                
#             except Exception as e:
#                 st.error(f"‚ùå Analysis failed: {str(e)}")
#                 st.error("This could be due to an unsupported image format or corrupted file.")

#     st.markdown("</div>", unsafe_allow_html=True)

# # Enhanced Footer
# st.markdown("---")
# st.markdown(
#     """
#     <div style='text-align: center; color: var(--text-400); padding: 2rem;'>
#         <h3 style='color: var(--neon-cyan); margin-bottom: 1rem;'>üî¨ TRUTHLENS PRO V4.0</h3>
#         <p><strong>Real Computer Vision AI Detection</strong></p>
#         <p>Utilizing advanced pixel analysis, frequency domain processing, texture analysis, and metadata forensics</p>
#         <p style='font-size: 0.9rem; margin-top: 1.5rem; color: var(--text-500);'>
#             Results represent sophisticated probabilistic analysis using real computer vision techniques.<br>
#             For critical applications, always verify through multiple independent methods and sources.
#         </p>
#     </div>
#     """,
#     unsafe_allow_html=True
# )
#         anomalies.append("Absence of natural sensor noise patterns")












# # Enhanced main application with video support and legal-grade analysis
# def main():
#     """Enhanced main application function"""
    
#     # Header with legal theme
#     st.markdown("<div class='main-header'>‚öñÔ∏è TRUTHLENS PRO</div>", unsafe_allow_html=True)
#     st.markdown("<div class='main-subtitle'>Legal-Grade AI Detection ‚Ä¢ Video Analysis ‚Ä¢ Court-Admissible Evidence</div>", unsafe_allow_html=True)
#     st.markdown("<div class='version-badge'><span class='badge'>‚öñÔ∏è LEGAL EDITION V5.0 | COURT-READY ANALYSIS</span></div>", unsafe_allow_html=True)

#     # Legal-grade certification badge
#     st.markdown(
#         """
#         <div class='legal-grade-badge'>
#             üèõÔ∏è LEGAL-GRADE CERTIFIED<br>
#             <small>Court-Admissible Analysis Standards</small>
#         </div>
#         """, 
#         unsafe_allow_html=True
#     )

#     with st.container():
#         st.markdown("<div class='truthlens-panel'>", unsafe_allow_html=True)

#         tabs = st.tabs([
#             "üé¨ Video Analysis", 
#             "üì∏ Image Analysis", 
#             "üîó URL Detection", 
#             "‚öñÔ∏è Legal Report", 
#             "üî¨ Detection Science",
#             "‚öôÔ∏è Expert Settings"
#         ])

#         # Video Analysis Tab
#         with tabs[0]:
#             st.markdown("### üé¨ Advanced Video AI Detection")
#             st.markdown(
#                 """
#                 <div class='video-analysis-section'>
#                 <h4>üîç Video Analysis Capabilities</h4>
#                 <ul>
#                 <li><strong>Platform Support:</strong> YouTube, TikTok, Instagram, Facebook, Twitter/X, Vimeo, and 50+ platforms</li>
#                 <li><strong>Deepfake Detection:</strong> Facial morphing, lip-sync analysis, temporal consistency</li>
#                 <li><strong>AI Video Detection:</strong> Stable Video Diffusion, Runway ML, Pika Labs detection</li>
#                 <li><strong>Legal-Grade Analysis:</strong> Court-admissible evidence generation</li>
#                 </ul>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
            
#             video_option = st.radio(
#                 "Choose video source:",
#                 ["üìÅ Upload Video File", "üîó Video URL from Any Platform"],
#                 key="video_source"
#             )
            
#             if video_option == "üîó Video URL from Any Platform":
#                 col1, col2 = st.columns([5, 1])
#             elif 'analyze_uploaded_video' in st.session_state and st.session_state.analyze_uploaded_video and uploaded_video:
#             with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
#                 tmp_file.write(uploaded_video.read())
#                 source_data = tmp_file.name
#                 source_url = uploaded_video.name
#                 is_video = True
                
#         elif 'analyze_image_url' in st.session_state and st.session_state.analyze_image_url and image_url:
#             with st.spinner("üì• Downloading image from URL..."):
#                 image_data = download_image_from_url(image_url)
#                 if image_data:
#                     source_data = image_data
#                     source_url = image_url
#                     is_video = False
                    
#         elif 'analyze_image_upload' in st.session_state and st.session_state.analyze_image_upload and uploaded_image:
#             source_data = uploaded_image.read()
#             source_url = uploaded_image.name
#             is_video = False
            
#         elif 'analyze_universal' in st.session_state and st.session_state.analyze_universal and universal_url:
#             with st.spinner("üîç Analyzing URL content..."):
#                 # Try video first, then image
#                 temp_video = download_video_from_url(universal_url)
#                 if temp_video:
#                     source_data = temp_video
#                     source_url = universal_url
#                     is_video = True
#                 else:
#                     image_data = download_image_from_url(universal_url)
#                     if image_data:
#                         source_data = image_data
#                         source_url = universal_url
#                         is_video = False

#         # Perform Analysis
#         if source_data:
#             st.divider()
            
#             try:
#                 if is_video:
#                     # Video Analysis Pipeline
#                     with st.spinner("üé¨ Performing advanced video analysis..."):
#                         progress_bar = st.progress(0)
#                         status_text = st.empty()
                        
#                         status_text.text("üìπ Extracting video frames...")
#                         progress_bar.progress(15)
                        
#                         status_text.text("üîç Analyzing temporal consistency...")
#                         progress_bar.progress(30)
                        
#                         status_text.text("üë§ Detecting deepfake indicators...")
#                         progress_bar.progress(50)
                        
#                         status_text.text("üé≠ Analyzing facial morphing patterns...")
#                         progress_bar.progress(70)
                        
#                         status_text.text("‚öñÔ∏è Generating legal-grade assessment...")
#                         progress_bar.progress(90)
                        
#                         # Perform video analysis
#                         video_features, frame_analyses = comprehensive_video_analysis(source_data, source_url)
                        
#                         # Get best frame analysis for classification
#                         if frame_analyses:
#                             combined_features = frame_analyses[0]  # Use first frame as representative
#                         else:
#                             # Create default features if frame analysis fails
#                             combined_features = AdvancedDetectionFeatures(
#                                 pixel_noise_variance=0.5, frequency_domain_anomalies=0.5, edge_sharpness_consistency=0.5,
#                                 compression_artifacts=0.5, texture_analysis_score=0.5, color_histogram_anomalies=0.5,
#                                 gradient_consistency=0.5, local_binary_patterns=0.5, neural_texture_patterns=0.5,
#                                 upsampling_artifacts=0.5, attention_map_irregularities=0.5, latent_space_signatures=0.5,
#                                 exif_consistency_score=0.5, timestamp_plausibility=0.5, color_profile_analysis=0.5,
#                                 file_entropy_analysis=0.5, statistical_significance=0.5, cross_validation_score=0.5,
#                                 reproducibility_index=0.5, false_positive_probability=0.5
#                             )
                        
#                         url_analysis = analyze_url_patterns(source_url)
#                         is_ai, confidence, risk_level, legal_features = legal_grade_classification(
#                             combined_features, video_features, url_analysis
#                         )
                        
#                         progress_bar.progress(100)
#                         time.sleep(0.5)
#                         progress_bar.empty()
#                         status_text.empty()
                        
#                         st.success("‚úÖ **VIDEO ANALYSIS COMPLETE** - Legal-grade video detection performed")
                        
#                         # Display video-specific results
#                         display_video_analysis_results(is_ai, confidence, risk_level, video_features, legal_features, source_url)
                        
#                 else:
#                     # Image Analysis Pipeline
#                     with st.spinner("üì∏ Performing legal-grade image analysis..."):
#                         progress_bar = st.progress(0)
#                         status_text = st.empty()
                        
#                         status_text.text("üîç Analyzing pixel patterns...")
#                         progress_bar.progress(20)
                        
#                         status_text.text("üé® Processing color characteristics...")
#                         progress_bar.progress(40)
                        
#                         status_text.text("üìä Performing frequency analysis...")
#                         progress_bar.progress(60)
                        
#                         status_text.text("üìã Extracting metadata...")
#                         progress_bar.progress(80)
                        
#                         status_text.text("‚öñÔ∏è Generating legal assessment...")
#                         progress_bar.progress(100)
                        
#                         # Perform image analysis
#                         features = comprehensive_ai_detection(source_data, source_url)
#                         url_analysis = analyze_url_patterns(source_url)
#                         is_ai, confidence, risk_level, legal_features = legal_grade_classification(features, None, url_analysis)
                        
#                         time.sleep(0.5)
#                         progress_bar.empty()
#                         status_text.empty()
                        
#                         st.success("‚úÖ **IMAGE ANALYSIS COMPLETE** - Legal-grade analysis performed")
                        
#                         # Display image-specific results
#                         display_image_analysis_results(is_ai, confidence, risk_level, features, legal_features, source_url)
                
#                 # Clean up temporary files
#                 if is_video and os.path.exists(source_data):
#                     try:
#                         os.unlink(source_data)
#                     except:
#                         pass
                        
#             except Exception as e:
#                 st.error(f"‚ùå Analysis failed: {str(e)}")
#                 st.error("This could indicate a corrupted file, unsupported format, or processing error.")

#         st.markdown("</div>", unsafe_allow_html=True)

# def display_video_analysis_results(is_ai: bool, confidence: int, risk_level: str, 
#                                  video_features: VideoAnalysisFeatures, 
#                                  legal_features: LegalGradeFeatures, 
#                                  source_url: str):
#     """Display comprehensive video analysis results"""
    
#     # Main Verdict
#     verdict_class = "verdict-ai" if is_ai else "verdict-human"
#     verdict_text = "ü§ñ AI-GENERATED VIDEO" if is_ai else "üë§ HUMAN-CREATED VIDEO"
    
#     st.markdown(f"<div class='{verdict_class}'>{verdict_text}</div>", unsafe_allow_html=True)
    
#     # Confidence and Legal Assessment
#     confidence_class = "confidence-high" if confidence >= 85 else "confidence-medium" if confidence >= 65 else "confidence-low"
    
#     st.markdown(
#         f"""
#         <div class='confidence-display'>
#             <div class='confidence-number {confidence_class}'>{confidence}%</div>
#             <h3>DETECTION CONFIDENCE</h3>
#             <p style='color: var(--text-300);'>
#                 Legal Certainty Level: <strong>{risk_level}</strong><br>
#                 Evidence Quality: <strong>{legal_features.evidence_quality_rating}</strong>
#             </p>
#             <div style='margin-top: 1.5rem; padding: 1.5rem; background: var(--bg-1100); border-radius: 12px; border-left: 4px solid var(--{"legal-gold" if legal_features.court_ready_analysis else "neon-red"});'>
#                 <strong>Court Admissibility: {"READY" if legal_features.court_ready_analysis else "NOT READY"}</strong><br><br>
#                 Admissibility Score: {legal_features.admissibility_score:.2f}/1.0<br>
#                 Expert Witness Confidence: {legal_features.expert_witness_confidence:.2f}/1.0
#             </div>
#         </div>
#         """, 
#         unsafe_allow_html=True
#     )
    
#     # Video-Specific Analysis
#     st.markdown("### üé¨ Video Analysis Results")
    
#     col1, col2, col3 = st.columns(3)
    
#     with col1:
#         st.markdown("**üé≠ Deepfake Detection**")
#         deepfake_score = int(video_features.deepfake_indicators * 100)
#         st.metric("Deepfake Probability", f"{deepfake_score}%")
        
#         facial_morph_score = int(video_features.facial_morphing_detection * 100)
#         st.metric("Facial Morphing", f"{facial_morph_score}%")
        
#         lip_sync_score = int((1 - video_features.lip_sync_consistency) * 100)
#         st.metric("Lip-Sync Anomalies", f"{lip_sync_score}%")
    
#     with col2:
#         st.markdown("**‚è±Ô∏è Temporal Analysis**")
#         temporal_score = int(video_features.temporal_consistency_score * 100)
#         st.metric("Temporal Consistency", f"{temporal_score}%")
        
#         motion_anom_score = int(video_features.motion_vector_anomalies * 100)
#         st.metric("Motion Anomalies", f"{motion_anom_score}%")
        
#         frame_interp_score = int(video_features.frame_interpolation_artifacts * 100)
#         st.metric("Frame Interpolation", f"{frame_interp_score}%")
    
#     with col3:
#         st.markdown("**üîß Technical Analysis**")
#         compression_score = int(video_features.compression_pattern_analysis * 100)
#         st.metric("Compression Anomalies", f"{compression_score}%")
        
#         timestamp_score = int(video_features.generation_timestamp_analysis * 100)
#         st.metric("Timestamp Suspicion", f"{timestamp_score}%")
    
#     # Legal-Grade Assessment
#     st.markdown("### ‚öñÔ∏è Legal-Grade Assessment")
    
#     col1, col2 = st.columns(2)
    
#     with col1:
#         st.markdown("**üìã Evidence Quality Metrics**")
#         st.write(f"**Chain of Custody:** {legal_features.chain_of_custody_score:.3f}")
#         st.write(f"**Metadata Integrity:** {legal_features.metadata_integrity_score:.3f}")
#         st.write(f"**Source Authenticity:** {legal_features.source_authenticity_score:.3f}")
#         st.write(f"**Tampering Detection:** {legal_features.tampering_detection_score:.3f}")
        
#     with col2:
#         st.markdown("**üìä Statistical Analysis**")
#         st.write(f"**Expert Confidence:** {legal_features.expert_witness_confidence:.3f}")
#         st.write(f"**Admissibility Score:** {legal_features.admissibility_score:.3f}")
#         st.write(f"**Court Ready:** {'Yes' if legal_features.court_ready_analysis else 'No'}")
#         st.write(f"**Evidence Grade:** {legal_features.evidence_quality_rating}")

# def display_image_analysis_results(is_ai: bool, confidence: int, risk_level: str, 
#                                  features: AdvancedDetectionFeatures, 
#                                  legal_features: LegalGradeFeatures, 
#                                  source_url: str):
#     """Display comprehensive image analysis results"""
    
#     # Main Verdict
#     verdict_class = "verdict-ai" if is_ai else "verdict-human"
#     verdict_text = "ü§ñ AI-GENERATED IMAGE" if is_ai else "üë§ HUMAN-CREATED IMAGE"
    
#     st.markdown(f"<div class='{verdict_class}'>{verdict_text}</div>", unsafe_allow_html=True)
    
#     # Confidence Display
#     confidence_class = "confidence-high" if confidence >= 85 else "confidence-medium" if confidence >= 65 else "confidence-low"
    
#     st.markdown(
#         f"""
#         <div class='confidence-display'>
#             <div class='confidence-number {confidence_class}'>{confidence}%</div>
#             <h3>DETECTION CONFIDENCE</h3>
#             <p style='color: var(--text-300);'>
#                 Legal Certainty: <strong>{risk_level}</strong><br>
#                 Evidence Quality: <strong>{legal_features.evidence_quality_rating}</strong>
#             </p>
#         </div>
#         """, 
#         unsafe_allow_html=True
#     )
    
#     # Enhanced Technical Analysis
#     st.markdown("### üî¨ Enhanced Technical Analysis")
    
#     col1, col2, col3 = st.columns(3)
    
#     with col1:
#         st.markdown("**üìä Pixel Analysis**")
#         noise_score = int(features.pixel_noise_variance * 100)
#         st.metric("Noise Variance", f"{noise_score}%")
        
#         freq_score = int(features.frequency_domain_anomalies * 100)
#         st.metric("Frequency Anomalies", f"{freq_score}%")
        
#         edge_score = int(features.edge_sharpness_consistency * 100)
#         st.metric("Edge Consistency", f"{edge_score}%")
    
#     with col2:
#         st.markdown("**üé® Color Analysis**")
#         color_score = int(features.color_histogram_anomalies * 100)
#         st.metric("Color Anomalies", f"{color_score}%")
        
#         texture_score = int(features.texture_analysis_score * 100)
#         st.metric("Texture Complexity", f"{texture_score}%")
        
#         profile_score = int(features.color_profile_analysis * 100)
#         st.metric("Color Profile", f"{profile_score}%")
    
#     with col3:
#         st.markdown("**ü§ñ AI Signatures**")
#         neural_score = int(features.neural_texture_patterns * 100)
#         st.metric("Neural Patterns", f"{neural_score}%")
        
#         upsampling_score = int(features.upsampling_artifacts * 100)
#         st.metric("Upsampling Artifacts", f"{upsampling_score}%")
        
#         latent_score = int(features.latent_space_signatures * 100)
#         st.metric("Latent Signatures", f"{latent_score}%")
    
#     # Legal-Grade Statistics
#     st.markdown("### ‚öñÔ∏è Legal-Grade Statistics")
    
#     col1, col2 = st.columns(2)
    
#     with col1:
#         st.markdown("**üìà Statistical Validation**")
#         st.write(f"**Statistical Significance:** {features.statistical_significance:.3f}")
#         st.write(f"**Cross-Validation Score:** {features.cross_validation_score:.3f}")
#         st.write(f"**Reproducibility Index:** {features.reproducibility_index:.3f}")
#         st.write(f"**False Positive Probability:** {features.false_positive_probability:.3f}")
        
#     with col2:
#         st.markdown("**‚öñÔ∏è Legal Assessment**")
#         st.write(f"**Evidence Quality:** {legal_features.evidence_quality_rating}")
#         st.write(f"**Legal Certainty:** {legal_features.legal_certainty_level}")
#         st.write(f"**Admissibility Score:** {legal_features.admissibility_score:.3f}")
#         st.write(f"**Court Ready:** {'Yes' if legal_features.court_ready_analysis else 'No'}")

# def download_image_from_url(url: str) -> Optional[bytes]:
#     """Download image from URL with comprehensive error handling"""
#     try:
#         headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#         }
#         response = requests.get(url, headers=headers, timeout=15, stream=True)
#         response.raise_for_status()
        
#         # Check content type
#         content_type = response.headers.get('content-type', '').lower()
#         if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'gif', 'webp', 'bmp']):
#             st.error("‚ùå URL does not point to a valid image")
#             return None
        
#         # Check file size
#         content_length = response.headers.get('content-length')
#         if content_length and int(content_length) > 50 * 1024 * 1024:  # 50MB limit
#             st.error("‚ùå Image file too large (>50MB)")
#             return None
            
#         return response.content
        
#     except requests.exceptions.RequestException as e:
#         st.error(f"‚ùå Error downloading image: {str(e)}")
#         return None
#     except Exception as e:
#         st.error(f"‚ùå Unexpected error: {str(e)}")
#         return None

# # Enhanced Footer
# def display_footer():
#     """Display enhanced legal-grade footer"""
#     st.markdown("---")
#     st.markdown(
#         """
#         <div style='text-align: center; color: var(--text-400); padding: 2rem;'>
#             <h3 style='color: var(--legal-gold); margin-bottom: 1rem;'>‚öñÔ∏è TRUTHLENS PRO LEGAL EDITION</h3>
#             <p><strong>Court-Admissible AI Detection System</strong></p>
#             <p>Advanced computer vision, statistical validation, and legal-grade evidence generation</p>
#             <p style='font-size: 0.9rem; margin-top: 1.5rem; color: var(--text-500);'>
#                 <strong>DISCLAIMER:</strong> Results represent sophisticated probabilistic analysis using peer-reviewed techniques.<br>
#                 For legal proceedings, consult with qualified experts and follow proper chain of custody procedures.<br>
#                 This tool provides technical analysis to support, not replace, professional forensic examination.
#             </p>
#             <p style='font-size: 0.8rem; color: var(--text-600);'>
#                 Detection algorithms based on published research in computer vision, digital forensics, and AI detection.<br>
#                 Statistical methods follow accepted standards for scientific evidence in legal proceedings.
#             </p>
#         </div>
#         """,
#         unsafe_allow_html=True
#     )

# # Run the enhanced application
# if __name__ == "__main__":
#     try:
#         main()
#         display_footer()
#     except Exception as e:
#         st.error(f"Application error: {str(e)}")
#         st.error("Please refresh the page and try again.")
#  col1:
#                     video_url = st.text_input(
#                         "",
#                         placeholder="https://youtube.com/watch?v=... or any video URL",
#                         label_visibility="collapsed",
#                         key="video_url_input"
#                     )
#                     st.caption("‚úÖ Supports: YouTube, TikTok, Instagram, Facebook, Twitter, Vimeo, Dailymotion, and 50+ platforms")
                
#                 with col2:
#                     analyze_video_url = st.button("üé¨ ANALYZE", type="primary", key="analyze_video_url")
                    
#             else:
#                 uploaded_video = st.file_uploader(
#                     "Upload video file",
#                     type=["mp4", "avi", "mov", "mkv", "webm", "flv", "wmv"],
#                     key="video_uploader"
#                 )
#                 if uploaded_video:
#                     col1, col2, col3 = st.columns([2, 2, 2])
#                     with col2:
#                         analyze_uploaded_video = st.button("üî¨ DEEP ANALYZE", type="primary", key="analyze_uploaded")

#         # Image Analysis Tab  
#         with tabs[1]:
#             st.markdown("### üì∏ Enhanced Image AI Detection")
            
#             image_option = st.radio(
#                 "Choose image source:",
#                 ["üìÅ Upload Image File", "üîó Image URL"],
#                 key="image_source"
#             )
            
#             if image_option == "üîó Image URL":
#                 col1, col2 = st.columns([5, 1])
#                 with col1:
#                     image_url = st.text_input(
#                         "",
#                         placeholder="https://example.com/image.jpg",
#                         label_visibility="collapsed",
#                         key="image_url_input"
#                     )
#                 with col2:
#                     analyze_image_url = st.button("üîç ANALYZE", type="primary", key="analyze_image_url")
#             else:
#                 uploaded_image = st.file_uploader(
#                     "Upload image file",
#                     type=["jpg", "jpeg", "png", "webp", "bmp", "gif", "tiff"],
#                     key="image_uploader"
#                 )
#                 if uploaded_image:
#                     col1, col2, col3 = st.columns([2, 2, 2])
#                     with col2:
#                         analyze_uploaded_image = st.button("üî¨ ANALYZE", type="primary", key="analyze_image_upload")

#         # URL Detection Tab
#         with tabs[2]:
#             st.markdown("### üîó Universal URL Detection")
#             st.markdown("Detect AI-generated content from any URL - images, videos, social media posts")
            
#             col1, col2 = st.columns([5, 1])
#             with col1:
#                 universal_url = st.text_input(
#                     "",
#                     placeholder="Any URL containing media content",
#                     label_visibility="collapsed",
#                     key="universal_url"
#                 )
#                 st.caption("üåê Auto-detects content type and applies appropriate analysis")
#             with col2:
#                 analyze_universal = st.button("üîç DETECT", type="primary", key="analyze_universal")

#         # Legal Report Tab
#         with tabs[3]:
#             st.markdown(
#                 """
#                 <div class='legal-report-section'>
#                 <h3>‚öñÔ∏è Legal-Grade Analysis Report</h3>
#                 <p>Generate court-admissible evidence reports with statistical confidence intervals,
#                 chain of custody analysis, and expert witness testimony preparation.</p>
#                 </div>
#                 """,
#                 unsafe_allow_html=True
#             )
            
#             if st.button("üìã Generate Legal Report Template", key="legal_template"):
#                 st.markdown("""
#                 ### üìã Legal Evidence Report Template
                
#                 **Case Information:**
#                 - Case ID: _______________
#                 - Date of Analysis: _______________
#                 - Analyst: _______________
#                 - Chain of Custody Reference: _______________
                
#                 **Technical Analysis Summary:**
#                 - Detection Confidence: ___%
#                 - Statistical Significance: _______________
#                 - False Positive Probability: ___%
#                 - Cross-Validation Score: _______________
                
#                 **Evidence Quality Rating:**
#                 - [ ] Beyond Reasonable Doubt (‚â•95% confidence)
#                 - [ ] Clear and Convincing (80-94% confidence)  
#                 - [ ] Preponderance of Evidence (65-79% confidence)
#                 - [ ] Insufficient Evidence (<65% confidence)
                
#                 **Court Admissibility Assessment:**
#                 - Daubert Standard Compliance: _______________
#                 - Peer Review Status: _______________
#                 - Error Rate Analysis: _______________
#                 - General Acceptance in Scientific Community: _______________
#                 """)

#         # Detection Science Tab
#         with tabs[4]:
#             st.markdown("### üî¨ Enhanced Detection Science")
            
#             col1, col2 = st.columns(2)
            
#             with col1:
#                 st.markdown("#### üìä Image Analysis Techniques")
#                 st.markdown("""
#                 **Pixel-Level Forensics:**
#                 - Multi-scale noise analysis with statistical validation
#                 - GLCM texture analysis with cross-correlation
#                 - Enhanced frequency domain processing (FFT/DCT)
#                 - Edge consistency analysis using multiple algorithms
                
#                 **Color Science Analysis:**
#                 - HSV/LAB color space anomaly detection
#                 - Histogram peak analysis and entropy calculation
#                 - Color distribution naturalness assessment
#                 - Chromatic aberration analysis
                
#                 **Compression Forensics:**
#                 - JPEG artifact detection and quantization analysis
#                 - File entropy and bit distribution analysis
#                 - Compression ratio consistency evaluation
#                 """)
                
#             with col2:
#                 st.markdown("#### üé¨ Video Analysis Techniques")
#                 st.markdown("""
#                 **Temporal Analysis:**
#                 - Optical flow consistency evaluation
#                 - Inter-frame correlation analysis
#                 - Motion vector anomaly detection
#                 - Frame interpolation artifact detection
                
#                 **Deepfake Detection:**
#                 - Facial landmark consistency analysis
#                 - Lip-sync temporal alignment verification
#                 - Blending artifact detection at face boundaries
#                 - Eye gaze and blink pattern analysis
                
#                 **AI Video Signatures:**
#                 - Stable Diffusion video artifacts
#                 - Runway ML generation patterns
#                 - Frame generation consistency analysis
#                 """)

#         # Expert Settings Tab
#         with tabs[5]:
#             st.markdown("### ‚öôÔ∏è Expert Analysis Settings")
            
#             col1, col2 = st.columns(2)
#             with col1:
#                 st.markdown("**üî¨ Analysis Parameters**")
#                 legal_mode = st.checkbox("Legal-Grade Mode", True, help="Enables court-admissible analysis standards")
#                 statistical_validation = st.checkbox("Statistical Validation", True, help="Includes confidence intervals and significance testing")
#                 deep_metadata_analysis = st.checkbox("Deep Metadata Analysis", True, help="Comprehensive EXIF and file structure analysis")
#                 cross_validation = st.checkbox("Cross-Validation", True, help="Multiple algorithm verification")
                
#                 detection_sensitivity = st.slider("Detection Sensitivity", 0.5, 1.0, 0.85, 0.05, key="expert_sensitivity")
                
#             with col2:
#                 st.markdown("**üìä Reporting Options**")
#                 include_technical_details = st.checkbox("Technical Details", True)
#                 include_statistical_analysis = st.checkbox("Statistical Analysis", True)
#                 include_expert_opinion = st.checkbox("Expert Opinion Summary", True)
#                 include_legal_assessment = st.checkbox("Legal Admissibility Assessment", True)
                
#                 confidence_threshold = st.slider("Court Admissibility Threshold", 0.75, 0.95, 0.85, 0.05, key="legal_threshold")

#         # Analysis Execution Logic
#         source_data = None
#         source_url = ""
#         is_video = False
        
#         # Handle different input types
#         if 'analyze_video_url' in st.session_state and st.session_state.analyze_video_url and video_url:
#             with st.spinner("üì• Downloading video from URL..."):
#                 temp_video_path = download_video_from_url(video_url)
#                 if temp_video_path:
#                     source_data = temp_video_path
#                     source_url = video_url
#                     is_video = True
                    
#         elif 'analyze_uploaded_video' in st.session_state and st.session_state.analyze_uploaded_video and uploaded_video:
#             withdef calculate_statistical_reliability(pixel_analysis: Dict, color_analysis: Dict) -> float:
#     """Calculate statistical reliability of the analysis"""
#     try:
#         # Combine multiple analysis results for statistical validation
#         measurements = []
#         measurements.extend(list(pixel_analysis.values()))
#         measurements.extend(list(color_analysis.values()))
        
#         # Remove any None values
#         measurements = [m for m in measurements if m is not None]
        
#         if len(measurements) < 3:
#             return 0.5
        
#         # Calculate statistical measures
#         mean_measurement = np.mean(measurements)
#         std_measurement = np.std(measurements)
        
#         # Higher reliability when measurements are consistent
#         coefficient_of_variation = std_measurement / (mean_measurement + 1e-6)
#         reliability = 1.0 - min(1.0, coefficient_of_variation)
        
#         return max(0, min(1, reliability))
        
#     except Exception:
#         return 0.5

# def calculate_false_positive_probability(pixel_analysis: Dict, color_analysis: Dict) -> float:
#     """Calculate probability of false positive detection"""
#     try:
#         # Count strong indicators vs weak indicators
#         strong_indicators = 0
#         weak_indicators = 0
#         total_indicators = 0
        
#         all_values = list(pixel_analysis.values()) + list(color_analysis.values())
#         all_values = [v for v in all_values if v is not None]
        
#         for value in all_values:
#             total_indicators += 1
#             if value > 0.8:
#                 strong_indicators += 1
#             elif value > 0.6:
#                 weak_indicators += 1
        
#         if total_indicators == 0:
#             return 0.5
        
#         # Lower false positive probability when more strong indicators present
#         strong_ratio = strong_indicators / total_indicators
#         weak_ratio = weak_indicators / total_indicators
        
#         false_positive_prob = 1.0 - (strong_ratio * 0.8 + weak_ratio * 0.4)
#         return max(0.05, min(0.95, false_positive_prob))
        
#     except Exception:
#         return 0.5

# # Enhanced URL pattern analysis for social media platforms
# def analyze_url_patterns(url: str) -> Dict[str, float]:
#     """Enhanced URL pattern analysis supporting all major platforms"""
    
#     if not url:
#         return {'ai_probability': 0.5, 'source_confidence': 0.5}
    
#     url_lower = url.lower()
    
#     # AI generation platforms and tools
#     ai_platforms = {
#         'midjourney.com': 0.95, 'cdn.midjourney.com': 0.95, 'discord.com/attachments': 0.7,
#         'dalle': 0.9, 'dall-e': 0.9, 'openai.com/dalle': 0.9,
#         'stability.ai': 0.85, 'stable-diffusion': 0.85, 'stablediffusion': 0.85,
#         'runway.ml': 0.9, 'runwayml.com': 0.9, 'gen-2': 0.85,
#         'leonardo.ai': 0.8, 'firefly.adobe.com': 0.7, 'synthesia.io': 0.95,
#         'deepfake': 0.95, 'faceswap': 0.9, 'deepfacelab': 0.9,
#         'artbreeder': 0.8, 'thisxdoesnotexist': 0.95, 'generated.photos': 0.9,
#         'huggingface.co/spaces': 0.7, 'gradio.app': 0.6, 'replicate.com': 0.6
#     }
    
#     # Authentic/trusted sources
#     trusted_sources = {
#         'reuters.com': 0.95, 'apnews.com': 0.95, 'bbc.com': 0.9, 'cnn.com': 0.85,
#         'nytimes.com': 0.9, 'washingtonpost.com': 0.9, 'theguardian.com': 0.85,
#         'bloomberg.com': 0.85, 'wsj.com': 0.9, 'npr.org': 0.85,
#         'youtube.com': 0.7, 'vimeo.com': 0.75, 'twitch.tv': 0.7,
#         'instagram.com': 0.6, 'facebook.com': 0.6, 'twitter.com': 0.65, 'x.com': 0.65,
#         'tiktok.com': 0.5, 'snapchat.com': 0.6, 'linkedin.com': 0.7,
#         'flickr.com': 0.8, '500px.com': 0.8, 'behance.net': 0.7, 'dribbble.com': 0.7,
#         'shutterstock.com': 0.85, 'getty': 0.9, 'unsplash.com': 0.75, 'pexels.com': 0.75
#     }
    
#     # Check for AI platforms
#     ai_score = 0.0
#     for platform, score in ai_platforms.items():
#         if platform in url_lower:
#             ai_score = max(ai_score, score)
#             break
    
#     # Check for trusted sources
#     trust_score = 0.0
#     for source, score in trusted_sources.items():
#         if source in url_lower:
#             trust_score = max(trust_score, score)
#             break
    
#     # Additional suspicious patterns
#     suspicious_patterns = [
#         r'[0-9a-f]{32,}',  # Long hex strings
#         r'temp\d+', r'cache\d+', r'generated\d+',
#         r'ai[-_]generated', r'synthetic[-_]media'
#     ]
    
#     for pattern in suspicious_patterns:
#         if re.search(pattern, url_lower):
#             ai_score += 0.3
#             break
    
#     # Final calculation
#     if trust_score > 0:
#         final_ai_prob = max(0, ai_score - trust_score * 0.8)
#         source_confidence = trust_score
#     else:
#         final_ai_prob = ai_score
#         source_confidence = 0.5
    
#     return {
#         'ai_probability': min(0.95, final_ai_prob),
#         'source_confidence': source_confidence
#     }

# def analyze_compression_patterns(image_data: bytes, file_path: str) -> Dict[str, float]:
#     """Enhanced compression pattern analysis"""
#     results = {}
    
#     try:
#         # File entropy analysis
#         byte_counts = np.bincount(list(image_data), minlength=256)
#         entropy = stats.entropy(byte_counts + 1)  # Add 1 to avoid log(0)
#         results['file_entropy'] = min(1.0, entropy / 8.0)
        
#         # JPEG compression analysis
#         try:
#             with Image.open(BytesIO(image_data)) as img:
#                 # Check for JPEG compression artifacts
#                 if img.format == 'JPEG':
#                     # Convert to array for analysis
#                     img_array = np.array(img)
#                     if len(img_array.shape) == 3:
#                         # Analyze 8x8 block patterns typical in JPEG
#                         gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
                        
#                         # Check for blockiness
#                         blocks_h = gray.reshape(gray.shape[0]//8, 8, -1, 8).mean(axis=(1,3))
#                         blocks_v = gray.reshape(-1, 8, gray.shape[1]//8, 8).mean(axis=(1,3))
                        
#                         block_variance_h = np.var(blocks_h)
#                         block_variance_v = np.var(blocks_v)
                        
#                         # Natural images have more block variance
#                         block_naturalness = min(1.0, (block_variance_h + block_variance_v) / 2000.0)
#                         results['compression_naturalness'] = block_naturalness
#                     else:
#                         results['compression_naturalness'] = 0.5
#                 else:
#                     results['compression_naturalness'] = 0.7  # PNG/other formats
                    
#         except Exception:
#             results['compression_naturalness'] = 0.5
            
#     except Exception as e:
#         results = {'file_entropy': 0.5, 'compression_naturalness': 0.5}
    
#     return results

# def extract_and_analyze_metadata(image_path: Union[str, bytes]) -> Dict[str, float]:
#     """Enhanced metadata analysis for legal-grade detection"""
    
#     results = {
#         'exif_consistency': 0.5,
#         'creation_plausibility': 0.5,
#         'camera_signature': 0.5,
#         'software_analysis': 0.5,
#         'gps_consistency': 0.5
#     }
    
#     try:
#         if isinstance(image_path, bytes):
#             img = Image.open(BytesIO(image_path))
#         else:
#             img = Image.open(image_path)
            
#         # Extract EXIF data
#         exif_dict = img._getexif()
#         if exif_dict is not None:
#             exif = {}
#             for k, v in exif_dict.items():
#                 if k in ExifTags.TAGS:
#                     exif[ExifTags.TAGS[k]] = v
            
#             # Enhanced camera analysis
#             camera_make = str(exif.get('Make', '')).lower()
#             camera_model = str(exif.get('Model', '')).lower()
#             software = str(exif.get('Software', '')).lower()
            
#             # Comprehensive AI software detection
#             ai_software_signatures = [
#                 'midjourney', 'dall-e', 'dalle', 'stable diffusion', 'sd',
#                 'runway', 'synthesia', 'deepfake', 'faceswap', 'artbreeder',
#                 'generated', 'artificial', 'ai', 'neural', 'diffusion',
#                 'gpt', 'clip', 'vqgan', 'stylegan', 'biggan'
#             ]
            
#             # Check software field
#             software_ai_score = 0.0
#             for signature in ai_software_signatures:
#                 if signature in software:
#                     software_ai_score = 0.9
#                     break
            
#             results['software_analysis'] = 1.0 - software_ai_score
            
#             # Enhanced camera signature analysis
#             legitimate_camera_brands = [
#                 'canon', 'nikon', 'sony', 'fujifilm', 'panasonic', 'olympus',
#                 'leica', 'pentax', 'hasselblad', 'mamiya', 'phase one'
#             ]
            
#             smartphone_brands = [
#                 'apple', 'iphone', 'samsung', 'galaxy', 'pixel', 'google',
#                 'oneplus', 'huawei', 'xiaomi', 'lg', 'motorola', 'nokia'
#             ]
            
#             camera_score = 0.3  # Default suspicious
            
#             # Check for legitimate camera brands
#             for brand in legitimate_camera_brands:
#                 if brand in camera_make or brand in camera_model:
#                     camera_score = 0.9
#                     break
            
#             # Check for smartphones
#             if camera_score < 0.9:
#                 for brand in smartphone_brands:
#                     if brand in camera_make or brand in camera_model:
#                         camera_score = 0.7
#                         break
            
#             results['camera_signature'] = camera_score
            
#             # Date/time analysis
#             datetime_original = exif.get('DateTimeOriginal')
#             datetime_digitized = exif.get('DateTimeDigitized')
            
#             if datetime_original and datetime_digitized:
#                 try:
#                     dt_orig = datetime.strptime(datetime_original, '%Y:%m:%d %H:%M:%S')
#                     dt_dig = datetime.strptime(datetime_digitized, '%Y:%m:%d %H:%M:%S')
                    
#                     # Check if dates are reasonable
#                     now = datetime.now()
#                     if dt_orig <= now and dt_dig <= now:
#                         time_diff = abs((dt_orig - dt_dig).total_seconds())
#                         # Reasonable if times are close but not identical
#                         if time_diff < 3600:  # Within 1 hour
#                             results['creation_plausibility'] = 0.8
#                         else:
#                             results['creation_plausibility'] = 0.6
#                     else:
#                         results['creation_plausibility'] = 0.2  # Future dates suspicious
#                 except:
#                     results['creation_plausibility'] = 0.4
            
#             # GPS consistency analysis
#             gps_info = exif.get('GPSInfo')
#             if gps_info:
#                 # Presence of GPS data suggests legitimate capture
#                 results['gps_consistency'] = 0.8
#             else:
#                 results['gps_consistency'] = 0.5  # Neutral - not all photos have GPS
                
#             # Overall EXIF consistency
#             exif_fields = len(exif)
#             if exif_fields > 20:  # Rich EXIF suggests real camera
#                 results['exif_consistency'] = 0.9
#             elif exif_fields > 10:
#                 results['exif_consistency'] = 0.7
#             elif exif_fields > 5:
#                 results['exif_consistency'] = 0.5
#             else:
#                 results['exif_consistency'] = 0.3
                
#         else:
#             # No EXIF data - suspicious for photographs, normal for graphics
#             results['exif_consistency'] = 0.3
    
#     except Exception:
#         # Error reading EXIF - moderately suspicious
#         results['exif_consistency'] = 0.3
    
#     return results

# # Enhanced classification with legal-grade precision
# def legal_grade_classification(features: AdvancedDetectionFeatures, 
#                              video_features: Optional[VideoAnalysisFeatures] = None,
#                              url_analysis: Dict = None) -> Tuple[bool, int, str, LegalGradeFeatures]:
#     """Legal-grade classification with court admissibility standards"""
    
#     if url_analysis is None:
#         url_analysis = {'ai_probability': 0.5, 'source_confidence': 0.5}
    
#     try:
#         # Enhanced weighted scoring for legal precision
#         weights = {
#             'pixel_noise_variance': -0.18,        # Stronger weight for key indicators
#             'frequency_domain_anomalies': 0.15,   
#             'edge_sharpness_consistency': 0.16,   
#             'compression_artifacts': 0.10,
#             'texture_analysis_score': -0.14,      
#             'color_histogram_anomalies': 0.12,
#             'gradient_consistency': 0.11,
#             'neural_texture_patterns': 0.17,      # High importance
#             'upsampling_artifacts': 0.13,
#             'attention_map_irregularities': 0.10,
#             'latent_space_signatures': 0.14,
#             'exif_consistency_score': -0.16,      # Strong indicator
#             'timestamp_plausibility': -0.10,
#             'color_profile_analysis': 0.08,
#             'file_entropy_analysis': 0.07,
#             'statistical_significance': 0.12,     # Legal-grade features
#             'cross_validation_score': 0.10,
#             'reproducibility_index': 0.08
#         }
        
#         # Calculate base AI score
#         ai_score = 0.0
#         feature_dict = features.__dict__
        
#         for feature_name, weight in weights.items():
#             if feature_name in feature_dict and feature_dict[feature_name] is not None:
#                 ai_score += feature_dict[feature_name] * weight
        
#         # Add URL analysis with higher weight for legal cases
#         ai_score += url_analysis.get('ai_probability', 0.5) * 0.30
        
#         # Video analysis integration
#         if video_features is not None:
#             video_score = (
#                 video_features.deepfake_indicators * 0.25 +
#                 video_features.facial_morphing_detection * 0.20 +
#                 video_features.temporal_consistency_score * -0.15 +  # Negative: good consistency = human
#                 video_features.frame_interpolation_artifacts * 0.18
#             )
#             ai_score += video_score * 0.35  # Video evidence weighs heavily
        
#         # Apply sigmoid transformation with adjusted sensitivity for legal standards
#         ai_probability = 1 / (1 + np.exp(-ai_score * 6))  # More sensitive for legal precision
        
#         # Legal-grade classification thresholds
#         if ai_probability >= 0.90:
#             is_ai = True
#             confidence = int(90 + ai_probability * 8)  # 90-98%
#             risk_level = "BEYOND REASONABLE DOUBT"
#         elif ai_probability >= 0.80:
#             is_ai = True
#             confidence = int(80 + ai_probability * 15)  # 80-95%
#             risk_level = "CLEAR AND CONVINCING"
#         elif ai_probability >= 0.65:
#             is_ai = True
#             confidence = int(65 + ai_probability * 20)  # 65-85%
#             risk_level = "PREPONDERANCE OF EVIDENCE"
#         elif ai_probability <= 0.15:
#             is_ai = False
#             confidence = int(85 + (1 - ai_probability) * 13)  # 85-98%
#             risk_level = "BEYOND REASONABLE DOUBT"
#         elif ai_probability <= 0.25:
#             is_ai = False
#             confidence = int(75 + (1 - ai_probability) * 20)  # 75-95%
#             risk_level = "CLEAR AND CONVINCING"
#         elif ai_probability <= 0.40:
#             is_ai = False
#             confidence = int(60 + (1 - ai_probability) * 25)  # 60-85%
#             risk_level = "PREPONDERANCE OF EVIDENCE"
#         else:
#             # Uncertain range - not suitable for legal proceedings
#             is_ai = ai_probability > 0.5
#             confidence = int(50 + abs(ai_probability - 0.5) * 30)
#             risk_level = "INSUFFICIENT EVIDENCE"
        
#         # Generate legal-grade features
#         legal_features = generate_legal_grade_analysis(
#             video_features if video_features else VideoAnalysisFeatures(
#                 temporal_consistency_score=0.5, motion_vector_anomalies=0.5,
#                 frame_transition_artifacts=0.5, optical_flow_irregularities=0.5,
#                 compression_pattern_analysis=0.5, facial_morphing_detection=0.5,
#                 lip_sync_consistency=0.5, deepfake_indicators=0.5,
#                 generation_timestamp_analysis=0.5, frame_interpolation_artifacts=0.5
#             ),
#             [features],  # Frame features
#             url_analysis.get('source_confidence', 0.5)
#         )
        
#         return is_ai, min(98, confidence), risk_level, legal_features
        
#     except Exception as e:
#         st.error(f"Error in legal-grade classification: {str(e)}")
#         # Return conservative default for legal safety
#         default_legal = LegalGradeFeatures(
#             chain_of_custody_score=0.5, forensic_hash_verification=0.5,
#             metadata_integrity_score=0.5, source_authenticity_score=0.5,
#             tampering_detection_score=0.5, expert_witness_confidence=0.5,
#             admissibility_score=0.5, evidence_quality_rating="INSUFFICIENT",
#             legal_certainty_level="INSUFFICIENT EVIDENCE", court_ready_analysis=False
#         )
#         return True, 50, "INSUFFICIENT EVIDENCE", default_legal
#         #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# import hashlib
# import os
# import time
# import re
# import numpy as np
# from dataclasses import dataclass
# from typing import Optional, Tuple, List, Dict, Union
# from PIL import Image, ExifTags
# import streamlit as st
# from urllib.parse import urlparse
# import requests
# from io import BytesIO
# import json
# import base64
# from scipy import stats, ndimage
# from skimage import feature, filters, measure, segmentation
# import matplotlib.pyplot as plt
# import warnings
# import tempfile
# import subprocess
# from datetime import datetime, timezone
# import yt_dlp
# import imageio
# from scipy.fft import fft2, fftshift
# from scipy.spatial.distance import cosine
# import sqlite3
# from concurrent.futures import ThreadPoolExecutor, as_completed

# # Suppress warnings for cleaner output
# warnings.filterwarnings('ignore')

# # -----------------------------
# # Page setup
# # -----------------------------

# st.set_page_config(
#     page_title="Truthlens Pro ‚Äî Legal-Grade AI Detection",
#     page_icon="‚öñÔ∏è",
#     layout="wide",
# )

# # Enhanced CSS with legal theme
# st.markdown(
#     """
#     <style>
#     @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700;800&family=Space+Grotesk:wght@300;400;500;600;700;800&display=swap');
    
#     :root {
#       --bg-1100: #050810;
#       --bg-1000: #0A0E1A;
#       --bg-900: #0F1419;
#       --bg-800: #1A1F2E;
#       --bg-700: #252B3A;
#       --bg-600: #2F3548;
#       --text-50: #F0F8FF;
#       --text-100: #E6F0FF;
#       --text-200: #D4E6FF;
#       --text-300: #9BB3C9;
#       --text-400: #7A8FA6;
#       --text-500: #5A6B7D;
#       --line-500: #3A4556;
#       --line-600: #2A3441;
#       --line-700: #1B2A3B;
#       --neon-blue: #00E5FF;
#       --neon-cyan: #00FFF0;
#       --neon-purple: #8B5FFF;
#       --neon-pink: #FF2BD1;
#       --neon-green: #39FF88;
#       --neon-yellow: #FFE135;
#       --neon-red: #FF4757;
#       --neon-orange: #FF6B47;
#       --legal-gold: #FFD700;
#       --legal-silver: #C0C0C0;
#       --hologram: linear-gradient(45deg, var(--neon-cyan), var(--neon-blue), var(--neon-purple), var(--neon-pink));
#       --matrix: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 50%, var(--neon-blue) 100%);
#       --legal: linear-gradient(135deg, var(--legal-gold) 0%, var(--legal-silver) 100%);
#       --danger: linear-gradient(135deg, var(--neon-red) 0%, var(--neon-orange) 100%);
#       --warning: linear-gradient(135deg, var(--neon-yellow) 0%, var(--neon-orange) 100%);
#       --success: linear-gradient(135deg, var(--neon-green) 0%, var(--neon-cyan) 100%);
#     }

#     * { font-family: 'Space Grotesk', -apple-system, BlinkMacSystemFont, sans-serif; }
#     .mono { font-family: 'JetBrains Mono', monospace; }

#     .stApp {
#       background: 
#         radial-gradient(circle at 20% 80%, rgba(0, 229, 255, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 80% 20%, rgba(255, 43, 209, 0.1) 0%, transparent 50%),
#         radial-gradient(circle at 40% 40%, rgba(139, 95, 255, 0.05) 0%, transparent 50%),
#         linear-gradient(135deg, var(--bg-1100) 0%, var(--bg-1000) 100%);
#       color: var(--text-50);
#       min-height: 100vh;
#     }

#     @keyframes neonPulse {
#       0%, 100% { text-shadow: 0 0 5px currentColor, 0 0 10px currentColor, 0 0 20px currentColor, 0 0 40px currentColor; }
#       50% { text-shadow: 0 0 2px currentColor, 0 0 5px currentColor, 0 0 10px currentColor, 0 0 20px currentColor; }
#     }

#     @keyframes hologramShimmer {
#       0% { background-position: 0% 50%; }
#       50% { background-position: 100% 50%; }
#       100% { background-position: 0% 50%; }
#     }

#     @keyframes slideInGlow {
#       from { transform: translateY(30px); opacity: 0; filter: blur(10px); }
#       to { transform: translateY(0); opacity: 1; filter: blur(0); }
#     }

#     .main-header {
#       text-align: center; padding: 3rem 0 1rem;
#       background: var(--legal); background-size: 400% 400%;
#       animation: hologramShimmer 3s ease-in-out infinite, neonPulse 2s ease-in-out infinite;
#       -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;
#       font-weight: 800; font-size: 4.5rem; letter-spacing: -0.03em; margin-bottom: 0.5rem;
#     }

#     .main-subtitle {
#       text-align: center; color: var(--text-300); font-size: 1.3rem; font-weight: 400;
#       margin-bottom: 0.5rem; animation: slideInGlow 1s ease-out 0.5s both;
#     }

#     .version-badge { text-align: center; margin-bottom: 3rem; }
#     .badge {
#       background: var(--legal); padding: 0.5rem 1.5rem; border-radius: 25px;
#       font-size: 0.9rem; font-weight: 600; color: white; display: inline-block;
#       animation: neonPulse 3s ease-in-out infinite; box-shadow: 0 0 20px rgba(255, 215, 0, 0.5);
#     }

#     .truthlens-panel {
#       background: linear-gradient(145deg, rgba(26, 31, 46, 0.9), rgba(15, 20, 25, 0.95));
#       border: 1px solid var(--line-600); border-radius: 24px; padding: 2.5rem; color: var(--text-50);
#       box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4), inset 0 1px 0 rgba(255, 255, 255, 0.1);
#       backdrop-filter: blur(20px); animation: slideInGlow 0.8s ease-out;
#     }

#     .legal-grade-badge {
#       background: var(--legal);
#       padding: 0.75rem 1.5rem;
#       border-radius: 15px;
#       font-weight: 700;
#       color: #000;
#       text-align: center;
#       margin: 1rem 0;
#       box-shadow: 0 0 20px rgba(255, 215, 0, 0.3);
#       animation: neonPulse 2s ease-in-out infinite;
#     }

#     .verdict-human { 
#       color: var(--neon-green); font-weight: 800; font-size: 2.5rem; 
#       text-shadow: 0 0 10px var(--neon-green), 0 0 20px var(--neon-green), 0 0 40px var(--neon-green);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center; margin: 2rem 0;
#     }
    
#     .verdict-ai { 
#       color: var(--neon-red); font-weight: 800; font-size: 2.5rem; 
#       text-shadow: 0 0 10px var(--neon-red), 0 0 20px var(--neon-red), 0 0 40px var(--neon-red);
#       animation: slideInGlow 1s ease-out, neonPulse 3s ease-in-out infinite 1s;
#       text-align: center; margin: 2rem 0;
#     }

#     .confidence-display {
#       text-align: center; margin: 2rem 0; padding: 2rem;
#       background: linear-gradient(145deg, var(--bg-1000), var(--bg-900));
#       border-radius: 20px; border: 1px solid var(--line-600);
#     }

#     .confidence-number { font-size: 4rem; font-weight: 900; margin-bottom: 1rem; animation: slideInGlow 1.2s ease-out; }
#     .confidence-high { color: var(--neon-green); text-shadow: 0 0 20px var(--neon-green); }
#     .confidence-medium { color: var(--neon-yellow); text-shadow: 0 0 20px var(--neon-yellow); }
#     .confidence-low { color: var(--neon-red); text-shadow: 0 0 20px var(--neon-red); }

#     .analysis-card {
#       background: linear-gradient(145deg, rgba(10, 14, 26, 0.95), rgba(26, 31, 46, 0.9));
#       border: 1px solid var(--line-700); border-radius: 20px; padding: 2rem; margin: 1.5rem 0;
#       box-shadow: 0 8px 24px rgba(0, 0, 0, 0.3), inset 0 1px 0 rgba(255, 255, 255, 0.05);
#       animation: slideInGlow 0.6s ease-out;
#     }

#     .video-analysis-section {
#       background: linear-gradient(145deg, rgba(139, 95, 255, 0.1), rgba(255, 43, 209, 0.1));
#       border: 2px solid var(--neon-purple);
#       border-radius: 20px;
#       padding: 2rem;
#       margin: 2rem 0;
#       box-shadow: 0 0 30px rgba(139, 95, 255, 0.3);
#     }

#     .legal-report-section {
#       background: linear-gradient(145deg, rgba(255, 215, 0, 0.1), rgba(192, 192, 192, 0.1));
#       border: 2px solid var(--legal-gold);
#       border-radius: 20px;
#       padding: 2rem;
#       margin: 2rem 0;
#       box-shadow: 0 0 30px rgba(255, 215, 0, 0.3);
#     }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )

# # -----------------------------
# # Enhanced AI Detection System
# # -----------------------------

# @dataclass
# class VideoAnalysisFeatures:
#     """Video-specific AI detection features"""
#     temporal_consistency_score: float
#     motion_vector_anomalies: float
#     frame_transition_artifacts: float
#     optical_flow_irregularities: float
#     compression_pattern_analysis: float
#     facial_morphing_detection: float
#     lip_sync_consistency: float
#     deepfake_indicators: float
#     generation_timestamp_analysis: float
#     frame_interpolation_artifacts: float

# @dataclass
# class LegalGradeFeatures:
#     """Legal-grade analysis features for court admissibility"""
#     chain_of_custody_score: float
#     forensic_hash_verification: float
#     metadata_integrity_score: float
#     source_authenticity_score: float
#     tampering_detection_score: float
#     expert_witness_confidence: float
#     admissibility_score: float
#     evidence_quality_rating: str
#     legal_certainty_level: str
#     court_ready_analysis: bool

# @dataclass
# class AdvancedDetectionFeatures:
#     """Enhanced AI detection features with legal-grade analysis"""
#     # Pixel-level Analysis
#     pixel_noise_variance: float
#     frequency_domain_anomalies: float
#     edge_sharpness_consistency: float
#     compression_artifacts: float
    
#     # Advanced Computer Vision
#     texture_analysis_score: float
#     color_histogram_anomalies: float
#     gradient_consistency: float
#     local_binary_patterns: float
    
#     # Deep Learning Indicators
#     neural_texture_patterns: float
#     upsampling_artifacts: float
#     attention_map_irregularities: float
#     latent_space_signatures: float
    
#     # Metadata and Technical
#     exif_consistency_score: float
#     timestamp_plausibility: float
#     color_profile_analysis: float
#     file_entropy_analysis: float
    
#     # Legal-Grade Enhancements
#     statistical_significance: float
#     cross_validation_score: float
#     reproducibility_index: float
#     false_positive_probability: float

# class LegalGradeVideoAnalyzer:
#     """Legal-grade video analysis for court admissibility"""
    
#     def __init__(self):
#         self.deepfake_signatures = self._load_deepfake_signatures()
#         self.temporal_analyzers = self._initialize_temporal_analyzers()
#         self.legal_thresholds = self._load_legal_thresholds()
        
#     def _load_deepfake_signatures(self) -> Dict:
#         """Load known deepfake generation signatures"""
#         return {
#             'faceswap': {
#                 'face_boundary_artifacts': 0.85,
#                 'temporal_flicker': 0.7,
#                 'eye_gaze_consistency': 0.6
#             },
#             'deepfacelab': {
#                 'blending_artifacts': 0.8,
#                 'resolution_inconsistency': 0.75,
#                 'compression_patterns': 0.65
#             },
#             'first_order_motion': {
#                 'keypoint_drift': 0.9,
#                 'motion_amplification': 0.8,
#                 'background_consistency': 0.7
#             },
#             'wav2lip': {
#                 'lip_sync_artifacts': 0.95,
#                 'facial_texture_mismatch': 0.85,
#                 'temporal_discontinuity': 0.75
#             }
#         }
    
#     def _initialize_temporal_analyzers(self) -> Dict:
#         """Initialize temporal analysis components"""
#         return {
#             'optical_flow': cv2.FarnebackOpticalFlow_create(),
#             'face_detector': cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'),
#             'motion_detector': cv2.createBackgroundSubtractorMOG2()
#         }
    
#     def _load_legal_thresholds(self) -> Dict:
#         """Load legal admissibility thresholds"""
#         return {
#             'high_confidence': 0.95,
#             'court_admissible': 0.90,
#             'expert_testimony': 0.85,
#             'preliminary_evidence': 0.75,
#             'insufficient_evidence': 0.60
#         }

# def download_video_from_url(url: str) -> Optional[str]:
#     """Enhanced video downloader supporting all major platforms"""
#     try:
#         # Configure yt-dlp for maximum compatibility
#         ydl_opts = {
#             'format': 'best[height<=720]',  # Limit to 720p for processing efficiency
#             'outtmpl': tempfile.mktemp(suffix='.%(ext)s'),
#             'quiet': True,
#             'no_warnings': True,
#             'extractaudio': False,
#             'writeinfojson': True,
#         }
        
#         with yt_dlp.YoutubeDL(ydl_opts) as ydl:
#             # Extract info first
#             info = ydl.extract_info(url, download=False)
            
#             # Check if it's a live stream (not downloadable)
#             if info.get('is_live', False):
#                 st.error("‚ùå Live streams cannot be analyzed")
#                 return None
            
#             # Download the video
#             ydl.download([url])
            
#             # Find the downloaded file
#             filename_template = ydl_opts['outtmpl']
#             # Replace template with actual values
#             filename = filename_template.replace('%(ext)s', info.get('ext', 'mp4'))
            
#             if os.path.exists(filename):
#                 return filename
#             else:
#                 # Try common extensions if exact match fails
#                 for ext in ['mp4', 'webm', 'mkv', 'avi']:
#                     test_filename = filename_template.replace('%(ext)s', ext)
#                     if os.path.exists(test_filename):
#                         return test_filename
                
#                 st.error("‚ùå Downloaded file not found")
#                 return None
                
#     except yt_dlp.utils.DownloadError as e:
#         st.error(f"‚ùå Download failed: {str(e)}")
#         return None
#     except Exception as e:
#         st.error(f"‚ùå Unexpected error downloading video: {str(e)}")
#         return None

# def extract_video_frames(video_path: str, max_frames: int = 30) -> List[np.ndarray]:
#     """Extract frames from video for analysis"""
#     try:
#         cap = cv2.VideoCapture(video_path)
        
#         if not cap.isOpened():
#             st.error("‚ùå Could not open video file")
#             return []
        
#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         fps = cap.get(cv2.CAP_PROP_FPS)
#         duration = total_frames / fps if fps > 0 else 0
        
#         # Calculate frame sampling interval
#         if total_frames <= max_frames:
#             frame_interval = 1
#         else:
#             frame_interval = total_frames // max_frames
        
#         frames = []
#         frame_count = 0
        
#         while len(frames) < max_frames and cap.isOpened():
#             ret, frame = cap.read()
#             if not ret:
#                 break
                
#             if frame_count % frame_interval == 0:
#                 # Convert BGR to RGB
#                 rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#                 frames.append(rgb_frame)
            
#             frame_count += 1
        
#         cap.release()
        
#         st.info(f"üìä Extracted {len(frames)} frames from video ({duration:.1f}s, {fps:.1f} FPS)")
#         return frames
        
#     except Exception as e:
#         st.error(f"‚ùå Error extracting frames: {str(e)}")
#         return []

# def analyze_temporal_consistency(frames: List[np.ndarray]) -> Dict[str, float]:
#     """Analyze temporal consistency between frames"""
#     if len(frames) < 2:
#         return {'temporal_consistency': 0.5, 'motion_smoothness': 0.5}
    
#     try:
#         consistency_scores = []
#         motion_scores = []
        
#         for i in range(1, len(frames)):
#             prev_frame = cv2.cvtColor(frames[i-1], cv2.COLOR_RGB2GRAY)
#             curr_frame = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)
            
#             # Calculate optical flow
#             flow = cv2.calcOpticalFlowPyrLK(
#                 prev_frame, curr_frame, 
#                 np.random.randint(0, min(prev_frame.shape), (100, 1, 2)).astype(np.float32),
#                 None
#             )[0]
            
#             if flow is not None and len(flow) > 0:
#                 # Calculate motion consistency
#                 motion_magnitude = np.linalg.norm(flow.reshape(-1, 2), axis=1)
#                 motion_consistency = 1.0 - np.std(motion_magnitude) / (np.mean(motion_magnitude) + 1e-6)
#                 motion_scores.append(max(0, min(1, motion_consistency)))
            
#             # Frame difference analysis
#             frame_diff = cv2.absdiff(prev_frame, curr_frame)
#             diff_mean = np.mean(frame_diff)
#             diff_std = np.std(frame_diff)
            
#             # AI-generated videos often have unnatural temporal transitions
#             consistency_score = 1.0 - min(1.0, diff_std / (diff_mean + 1e-6))
#             consistency_scores.append(max(0, consistency_score))
        
#         avg_consistency = np.mean(consistency_scores) if consistency_scores else 0.5
#         avg_motion_smoothness = np.mean(motion_scores) if motion_scores else 0.5
        
#         return {
#             'temporal_consistency': avg_consistency,
#             'motion_smoothness': avg_motion_smoothness,
#             'frame_transition_score': (avg_consistency + avg_motion_smoothness) / 2
#         }
        
#     except Exception as e:
#         st.error(f"Error in temporal analysis: {str(e)}")
#         return {'temporal_consistency': 0.5, 'motion_smoothness': 0.5, 'frame_transition_score': 0.5}

# def detect_deepfake_indicators(frames: List[np.ndarray]) -> Dict[str, float]:
#     """Detect deepfake-specific indicators in video frames"""
#     if not frames:
#         return {'deepfake_probability': 0.5}
    
#     try:
#         deepfake_scores = []
#         face_consistency_scores = []
        
#         # Initialize face detector
#         face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        
#         previous_face_features = None
        
#         for frame in frames:
#             gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            
#             # Detect faces
#             faces = face_cascade.detectMultiScale(gray_frame, 1.1, 4)
            
#             if len(faces) > 0:
#                 # Analyze the largest face
#                 face = max(faces, key=lambda x: x[2] * x[3])
#                 x, y, w, h = face
                
#                 face_region = gray_frame[y:y+h, x:x+w]
                
#                 if face_region.size > 0:
#                     # Extract features for consistency analysis
#                     try:
#                         # Calculate Local Binary Pattern for face texture
#                         lbp = feature.local_binary_pattern(face_region, 8, 1, method='uniform')
#                         face_texture_features = np.histogram(lbp.ravel(), bins=10)[0]
#                         face_texture_features = face_texture_features / (np.sum(face_texture_features) + 1e-6)
                        
#                         # Check consistency with previous frame
#                         if previous_face_features is not None:
#                             similarity = 1.0 - cosine(face_texture_features, previous_face_features)
#                             face_consistency_scores.append(max(0, min(1, similarity)))
                        
#                         previous_face_features = face_texture_features
                        
#                         # Analyze face region for artifacts
#                         # Check for unnatural smoothness (common in deepfakes)
#                         face_variance = np.var(face_region.astype(float))
#                         smoothness_score = 1.0 - min(1.0, face_variance / 1000.0)
                        
#                         # Check for edge artifacts around face boundary
#                         face_edges = cv2.Canny(face_region, 50, 150)
#                         edge_density = np.sum(face_edges > 0) / face_edges.size
#                         edge_artifact_score = abs(edge_density - 0.1) * 10  # Unnatural if too high or too low
                        
#                         # Combine scores
#                         frame_deepfake_score = (smoothness_score + min(1.0, edge_artifact_score)) / 2
#                         deepfake_scores.append(frame_deepfake_score)
                        
#                     except Exception as e:
#                         continue
        
#         # Calculate final deepfake probability
#         avg_deepfake_score = np.mean(deepfake_scores) if deepfake_scores else 0.5
#         avg_face_consistency = np.mean(face_consistency_scores) if face_consistency_scores else 0.5
        
#         # Inconsistent faces or high artifact scores suggest deepfake
#         deepfake_probability = (avg_deepfake_score + (1.0 - avg_face_consistency)) / 2
        
#         return {
#             'deepfake_probability': deepfake_probability,
#             'face_consistency_score': avg_face_consistency,
#             'artifact_detection_score': avg_deepfake_score,
#             'faces_detected': len(deepfake_scores)
#         }
        
#     except Exception as e:
#         st.error(f"Error in deepfake detection: {str(e)}")
#         return {'deepfake_probability': 0.5}

# def analyze_video_compression_patterns(video_path: str) -> Dict[str, float]:
#     """Analyze video compression patterns for AI generation indicators"""
#     try:
#         # Get video properties using ffprobe
#         cmd = [
#             'ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', 
#             '-show_streams', video_path
#         ]
        
#         try:
#             result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
#             video_info = json.loads(result.stdout)
#         except:
#             # Fallback to basic analysis if ffprobe fails
#             return {'compression_naturalness': 0.5, 'bitrate_consistency': 0.5}
        
#         # Extract video stream info
#         video_streams = [s for s in video_info.get('streams', []) if s.get('codec_type') == 'video']
        
#         if not video_streams:
#             return {'compression_naturalness': 0.5, 'bitrate_consistency': 0.5}
        
#         video_stream = video_streams[0]
        
#         # Analyze compression parameters
#         codec_name = video_stream.get('codec_name', 'unknown')
#         bit_rate = int(video_stream.get('bit_rate', 0)) if video_stream.get('bit_rate') else 0
#         width = int(video_stream.get('width', 0))
#         height = int(video_stream.get('height', 0))
        
#         # Calculate expected bitrate for natural video
#         pixel_count = width * height
#         expected_bitrate = pixel_count * 0.1  # Rough estimate
        
#         # Analyze bitrate consistency
#         if bit_rate > 0 and expected_bitrate > 0:
#             bitrate_ratio = bit_rate / expected_bitrate
#             bitrate_consistency = 1.0 - abs(bitrate_ratio - 1.0)
#             bitrate_consistency = max(0, min(1, bitrate_consistency))
#         else:
#             bitrate_consistency = 0.5
        
#         # Analyze codec appropriateness
#         natural_codecs = ['h264', 'h265', 'hevc', 'vp9', 'av1']
#         codec_naturalness = 0.8 if any(nc in codec_name.lower() for nc in natural_codecs) else 0.3
        
#         return {
#             'compression_naturalness': codec_naturalness,
#             'bitrate_consistency': bitrate_consistency,
#             'codec_analysis': codec_naturalness
#         }
        
#     except Exception as e:
#         st.error(f"Error in compression analysis: {str(e)}")
#         return {'compression_naturalness': 0.5, 'bitrate_consistency': 0.5}

# def comprehensive_video_analysis(video_path: str, source_url: str = "") -> Tuple[VideoAnalysisFeatures, List[AdvancedDetectionFeatures]]:
#     """Comprehensive video analysis combining frame and temporal analysis"""
    
#     # Extract frames for analysis
#     frames = extract_video_frames(video_path, max_frames=20)
    
#     if not frames:
#         # Return default values if frame extraction fails
#         default_video_features = VideoAnalysisFeatures(
#             temporal_consistency_score=0.5, motion_vector_anomalies=0.5,
#             frame_transition_artifacts=0.5, optical_flow_irregularities=0.5,
#             compression_pattern_analysis=0.5, facial_morphing_detection=0.5,
#             lip_sync_consistency=0.5, deepfake_indicators=0.5,
#             generation_timestamp_analysis=0.5, frame_interpolation_artifacts=0.5
#         )
#         return default_video_features, []
    
#     # Analyze individual frames
#     frame_analyses = []
#     for i, frame in enumerate(frames[:10]):  # Limit to 10 frames for performance
#         try:
#             frame_analysis = comprehensive_ai_detection(frame, source_url)
#             frame_analyses.append(frame_analysis)
#         except Exception as e:
#             st.warning(f"Frame {i} analysis failed: {str(e)}")
#             continue
    
#     # Temporal analysis
#     temporal_analysis = analyze_temporal_consistency(frames)
    
#     # Deepfake detection
#     deepfake_analysis = detect_deepfake_indicators(frames)
    
#     # Compression analysis
#     compression_analysis = analyze_video_compression_patterns(video_path)
    
#     # Combine analyses into VideoAnalysisFeatures
#     video_features = VideoAnalysisFeatures(
#         temporal_consistency_score=temporal_analysis.get('temporal_consistency', 0.5),
#         motion_vector_anomalies=1.0 - temporal_analysis.get('motion_smoothness', 0.5),
#         frame_transition_artifacts=1.0 - temporal_analysis.get('frame_transition_score', 0.5),
#         optical_flow_irregularities=deepfake_analysis.get('deepfake_probability', 0.5),
#         compression_pattern_analysis=1.0 - compression_analysis.get('compression_naturalness', 0.5),
#         facial_morphing_detection=deepfake_analysis.get('artifact_detection_score', 0.5),
#         lip_sync_consistency=deepfake_analysis.get('face_consistency_score', 0.5),
















# Truthlens-AI Detector ‚Äî Lightweight AI Image Detector (Streamlit)
# ---------------------------------------------------------------
# Usage
#   1) Install requirements:  
#        pip install -r requirements.txt
#      (or) minimal set: pip install streamlit pillow numpy opencv-python scikit-image piexif
#   2) Run:  
#        streamlit run truthlens_ai_detector.py
#   3) Open the local URL shown in terminal.
#
# Notes
#  - This app uses several classical forensic heuristics (ELA, frequency artifacts,
#    noise statistics, JPEG quantization analysis, and metadata checks) and blends
#    them into a single "AI-likelihood" score with explainability for each image.
#  - It does NOT rely on large deep models, so it runs locally without downloads.
#  - Heuristics can make mistakes. Treat the results as probabilistic signals, not
#    absolute ground truth.

import io
import os
import math
import json
import base64
from typing import Dict, List, Tuple, Optional

import numpy as np
from PIL import Image, ImageChops, ImageEnhance, PngImagePlugin

import streamlit as st

# Optional dependencies (app still works without them, but extra signals can help)
try:
    import cv2  # type: ignore
except Exception:
    cv2 = None

try:
    import piexif  # type: ignore
except Exception:
    piexif = None

try:
    from skimage import filters  # type: ignore
except Exception:
    filters = None

APP_NAME = "Truthlens-AI Detector"
VERSION = "1.0.0"

AI_KEYWORDS = [
    "ai", "artificial", "generative", "diffusion", "stable diffusion",
    "midjourney", "dall-e", "dalle", "firefly", "sdxl", "comfyui",
    "leonardo", "playground", "ideogram", "runway", "synth", "gen-"
]

# ----------------------------- Utility Functions ----------------------------- #

def pil_to_array(img: Image.Image) -> np.ndarray:
    if img.mode not in ("RGB", "RGBA"):
        img = img.convert("RGB")
    return np.array(img)


def array_to_pil(arr: np.ndarray) -> Image.Image:
    if arr.dtype != np.uint8:
        arr = np.clip(arr, 0, 255).astype(np.uint8)
    return Image.fromarray(arr)


def safe_read_metadata(img: Image.Image) -> Dict:
    """Extract EXIF/XMP/PNG text chunks where available. Best-effort only."""
    meta = {"exif": {}, "png_text": {}, "format": img.format}
    # PIL EXIF
    try:
        exif = getattr(img, "getexif", lambda: None)()
        if exif:
            exif_dict = {str(k): str(v) for k, v in exif.items()}
            meta["exif"] = exif_dict
    except Exception:
        pass

    # PNG tEXt/iTXt chunks
    try:
        if isinstance(img.info, dict):
            for k, v in img.info.items():
                if isinstance(v, (str, bytes)):
                    meta["png_text"][str(k)] = v.decode("utf-8", errors="ignore") if isinstance(v, bytes) else str(v)
    except Exception:
        pass

    # piexif for richer EXIF
    if piexif is not None and hasattr(img, "info") and "exif" in img.info:
        try:
            exif_bytes = img.info.get("exif")
            if exif_bytes:
                exif_dict = piexif.load(exif_bytes)
                meta["piexif"] = {k: {str(t): str(v) for t, v in vdict.items()} for k, vdict in exif_dict.items()}
        except Exception:
            pass

    return meta


def find_ai_strings(meta: Dict) -> List[Tuple[str, str]]:
    findings = []
    as_text = json.dumps(meta).lower()
    for key in AI_KEYWORDS:
        if key in as_text:
            findings.append(("metadata", key))
    return findings


# -------------------------- Heuristic Signal Extractors ---------------------- #

def compute_ela_signal(img: Image.Image, quality: int = 90) -> Tuple[float, Image.Image]:
    """Error Level Analysis: re-compress JPEG and measure residual exaggeration.
    Returns (signal_strength [0-1], visualization_image).
    """
    # Convert to JPEG in-memory
    if img.mode != "RGB":
        img_rgb = img.convert("RGB")
    else:
        img_rgb = img

    buf = io.BytesIO()
    img_rgb.save(buf, "JPEG", quality=quality)
    buf.seek(0)
    recompressed = Image.open(buf)

    # Difference
    ela = ImageChops.difference(img_rgb, recompressed)
    # Enhance for visibility
    enhancer = ImageEnhance.Brightness(ela)
    ela_vis = enhancer.enhance(20.0)

    # Signal: mean residual normalized
    arr = np.asarray(ela).astype(np.float32)
    score = float(np.mean(arr) / 255.0)
    score = float(np.clip(score * 5.0, 0.0, 1.0))  # scale up; cap to [0,1]

    return score, ela_vis


def compute_frequency_signal(img: Image.Image) -> Tuple[float, Image.Image]:
    """Magnitude spectrum proportion in high frequencies. Heuristic for diffusion artifacts."""
    arr = pil_to_array(img)
    if arr.ndim == 3 and arr.shape[2] == 4:
        arr = arr[:, :, :3]
    gray = np.mean(arr, axis=2).astype(np.float32)

    # FFT
    f = np.fft.fft2(gray)
    fshift = np.fft.fftshift(f)
    mag = np.abs(fshift)
    mag_log = np.log1p(mag)

    h, w = gray.shape
    cy, cx = h // 2, w // 2
    r = min(cy, cx)

    # High frequency: outside inner circle
    Y, X = np.ogrid[:h, :w]
    dist = np.sqrt((Y - cy) ** 2 + (X - cx) ** 2)
    inner = dist <= (0.15 * r)

    total = np.sum(mag)
    high = np.sum(mag[~inner])
    ratio = float(high / (total + 1e-8))

    # Normalize: natural photos tend to have more low-freq; AI may increase high-freq textures
    score = float(np.clip((ratio - 0.92) / 0.08, 0.0, 1.0))  # heuristic mapping

    # Visualization of spectrum
    spec = (mag_log / np.max(mag_log + 1e-6) * 255.0).astype(np.uint8)
    spec_img = Image.fromarray(spec)

    return score, spec_img


def compute_noise_signal(img: Image.Image) -> float:
    """Low sensor noise can hint at synthesis. Use Laplacian variance; invert to score AI."""
    if cv2 is None:
        return 0.0
    arr = pil_to_array(img)
    if arr.ndim == 3 and arr.shape[2] == 4:
        arr = arr[:, :, :3]
    gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)
    lap = cv2.Laplacian(gray, cv2.CV_64F)
    var = float(lap.var())
    # Map variance to [0,1] AI-likelihood (lower var -> higher score)
    score = float(np.clip((150.0 - var) / 150.0, 0.0, 1.0))
    return score


def compute_jpeg_quant_anomaly(img: Image.Image) -> float:
    """Check JPEG quantization tables for anomalies."""
    info = getattr(img, "info", {}) or {}
    qtables = info.get("quantization")
    if qtables is None:
        # If not JPEG, or tables missing, return neutral 0.5 so it neither helps nor hurts much
        return 0.5

    # Heuristic: typical camera Q tables follow smooth monotonic increases.
    # Measure monotonicity; higher disorder -> higher AI-likelihood.
    if isinstance(qtables, dict):
        tables = list(qtables.values())
    else:
        tables = qtables

    def disorder(tbl: List[int]) -> float:
        diffs = np.diff(np.array(tbl, dtype=np.float32))
        neg_steps = np.sum(diffs < 0)
        return float(np.clip(neg_steps / max(1, len(diffs)), 0.0, 1.0))

    scores = [disorder(t) for t in tables if isinstance(t, (list, tuple)) and len(t) > 8]
    if not scores:
        return 0.5
    return float(np.mean(scores))


def compute_edge_consistency(img: Image.Image) -> float:
    """Edge-map entropy using Sobel. AI faces can show overly uniform edges; map to score."""
    if filters is None:
        return 0.0
    arr = pil_to_array(img)
    if arr.ndim == 3 and arr.shape[2] == 4:
        arr = arr[:, :, :3]
    gray = np.mean(arr, axis=2).astype(np.float32) / 255.0
    sx = filters.sobel_h(gray)
    sy = filters.sobel_v(gray)
    mag = np.hypot(sx, sy)
    # Entropy of edge magnitude histogram
    hist, _ = np.histogram(mag, bins=64, range=(0.0, float(mag.max()) + 1e-6), density=True)
    hist = hist + 1e-12
    ent = -np.sum(hist * np.log2(hist))  # in [0, ~6]
    # Map entropy to score: very low or very high entropy could be suspicious; use a U-shape
    score = float(np.clip(1.0 - math.exp(-((ent - 3.0) ** 2) / 1.5), 0.0, 1.0))
    return score


# ---------------------------- Scoring & Fusion ------------------------------- #

def sigmoid(x: float) -> float:
    return 1.0 / (1.0 + math.exp(-x))


def fuse_scores(signals: Dict[str, float], weights: Dict[str, float], bias: float = -0.5) -> float:
    x = bias
    for k, v in signals.items():
        w = float(weights.get(k, 0.0))
        x += w * float(v)
    return float(sigmoid(x))  # return probability in [0,1]


DEFAULT_WEIGHTS = {
    "ela": 1.2,
    "freq": 1.0,
    "noise": 0.7,
    "jpeg": 0.5,
    "edge": 0.6,
    "meta": 1.8,
}
DEFAULT_BIAS = -0.6  # lower prior AI probability


# ------------------------------ Streamlit UI -------------------------------- #

st.set_page_config(page_title=f"{APP_NAME}", page_icon="üïµÔ∏è‚Äç‚ôÄÔ∏è", layout="wide")
st.title(f"{APP_NAME}")
st.caption("Lightweight, explainable heuristics for spotting AI-generated images. v" + VERSION)

with st.sidebar:
    st.header("Detection Settings")
    q = st.slider("ELA recompress quality", 60, 99, 90)
    th = st.slider("Decision threshold (AI if ‚â•)", 0.1, 0.9, 0.6, step=0.05)

    st.subheader("Signal Weights")
    w_ela = st.slider("ELA", 0.0, 3.0, float(DEFAULT_WEIGHTS["ela"]))
    w_freq = st.slider("Frequency", 0.0, 3.0, float(DEFAULT_WEIGHTS["freq"]))
    w_noise = st.slider("Noise", 0.0, 3.0, float(DEFAULT_WEIGHTS["noise"]))
    w_jpeg = st.slider("JPEG Quant", 0.0, 3.0, float(DEFAULT_WEIGHTS["jpeg"]))
    w_edge = st.slider("Edge Consistency", 0.0, 3.0, float(DEFAULT_WEIGHTS["edge"]))
    w_meta = st.slider("Metadata Flags", 0.0, 3.0, float(DEFAULT_WEIGHTS["meta"]))
    bias = st.slider("Bias (lower = stricter)", -2.0, 2.0, float(DEFAULT_BIAS), step=0.1)

    weights = {"ela": w_ela, "freq": w_freq, "noise": w_noise, "jpeg": w_jpeg, "edge": w_edge, "meta": w_meta}

st.markdown(
    """
    **Upload one or more images**. For each image you'll get an AI-likelihood score (0‚Äì100%),
    a verdict, and per-signal explanations with visualizations.
    """
)

files = st.file_uploader("Drop images here", type=["jpg", "jpeg", "png", "webp", "bmp", "tiff"], accept_multiple_files=True)

results: List[Dict] = []

if files:
    for f in files:
        try:
            img = Image.open(f).convert("RGB")
        except Exception as e:
            st.error(f"Failed to open {f.name}: {e}")
            continue

        col0, col1 = st.columns([2, 1])
        with col0:
            st.image(img, caption=f.name, use_container_width=True)

        # Compute signals
        ela_score, ela_vis = compute_ela_signal(img, quality=q)
        freq_score, spec_img = compute_frequency_signal(img)
        noise_score = compute_noise_signal(img)
        jpeg_score = compute_jpeg_quant_anomaly(img)
        edge_score = compute_edge_consistency(img)

        meta = safe_read_metadata(img)
        meta_hits = find_ai_strings(meta)
        meta_score = 1.0 if len(meta_hits) > 0 else 0.0

        signals = {
            "ela": ela_score,
            "freq": freq_score,
            "noise": noise_score,
            "jpeg": jpeg_score,
            "edge": edge_score,
            "meta": meta_score,
        }

        prob = fuse_scores(signals, weights, bias=bias)
        verdict = "Likely AI-generated" if prob >= th else "Likely camera photo"

        with col1:
            st.metric("AI-likelihood", f"{prob*100:.1f}%")
            st.markdown(f"**Verdict:** {verdict}")
            with st.expander("Signal breakdown"):
                st.write({k: round(v, 3) for k, v in signals.items()})
                if meta_hits:
                    st.info("Metadata flags: " + ", ".join(sorted(set([k for _, k in meta_hits]))))
                else:
                    st.caption("No explicit AI tool strings found in metadata.")

        c1, c2 = st.columns(2)
        with c1:
            st.image(ela_vis, caption="ELA visualization (bright areas = higher residual)", use_container_width=True)
        with c2:
            st.image(spec_img, caption="FFT magnitude spectrum (more outer energy can be suspicious)", use_container_width=True)

        results.append({
            "file": f.name,
            "probability": float(prob),
            "verdict": verdict,
            **{f"sig_{k}": float(v) for k, v in signals.items()},
            "meta_flagged": bool(len(meta_hits) > 0),
        })

if results:
    st.divider()
    st.subheader("Batch Results")
    st.dataframe(results, use_container_width=True)

    # Download JSON/CSV
    colA, colB = st.columns(2)
    with colA:
        json_bytes = json.dumps(results, indent=2).encode("utf-8")
        st.download_button("Download JSON report", json_bytes, file_name="truthlens_results.json", mime="application/json")
    with colB:
        # Simple CSV
        headers = list(results[0].keys())
        lines = [",".join(headers)]
        for r in results:
            row = [str(r.get(h, "")) for h in headers]
            lines.append(",".join([s.replace(",", " ") for s in row]))
        csv_bytes = ("\n".join(lines)).encode("utf-8")
        st.download_button("Download CSV", csv_bytes, file_name="truthlens_results.csv", mime="text/csv")

st.divider()
st.markdown(
    """
    **How it works (quick overview):**

    - *ELA (Error Level Analysis):* recompresses to JPEG and measures residuals; synthetic images can retain uneven residual maps.
    - *Frequency spectrum:* diffusion images often have relatively stronger high-frequency content.
    - *Noise statistics:* AI images may lack natural sensor noise; very low Laplacian variance can be suspicious.
    - *JPEG quantization:* unusual or non-monotonic quant tables can indicate software rendering.
    - *Edge consistency:* highly uniform or highly chaotic edge distributions may be artifacts of synthesis.
    - *Metadata flags:* explicit mentions like "Stable Diffusion" or "Midjourney" in EXIF/PNG text.

    The app fuses these signals into a single probability via a logistic function. You can tune
    weights and thresholds in the sidebar. Always corroborate with context.
    """
)



